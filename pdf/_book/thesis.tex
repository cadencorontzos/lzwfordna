% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See https://www.reed.edu/cis/help/LaTeX/index.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: https://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
% Thanks, @Xyv
\usepackage{calc}
% End of CII addition
\usepackage{rotating}

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Optimizing Lempel Ziv Welch for DNA Compression}
\author{Caden Corontzos}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2023}
\division{Mathematics and Natural Sciences}
\advisor{Eitan Frachtenberg}
\institution{Reed College}
\degree{Bachelor of Arts}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
% End of CII addition

%%% Remember to use the correct department!
\department{Computer Science}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

% From {rticles}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
% As noted by @mirh [2] is needed instead of [3] for 2.12
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
I want to thank a few people.
}

\Dedication{
You can have a dedication here if you wish.
}

\Preface{
This is an example of a thesis setup to use the reed thesis document class
(for LaTeX) and the R bookdown package, in general.
}

\Abstract{
The Lempel Ziv Welch compression algorithm is a lossless data compression algorithm used for numerous applications, including the Unix file compression utility \texttt{compress} and the GIF image format. Storing, reading, and transferring enormous amounts of data is often an issue in the biological field, especially when concerning DNA. This thesis explores the application of Lempel Ziv Welch to the compression of DNA. A variety of different optimization of the original LZW algorithm are explore included palatalizing, multiple dictionaries, and some other cool thing here broh.
}

	\usepackage{setspace}\onehalfspacing
\usepackage[edges]{forest}
\usepackage{tikz}
	\usepackage{forest}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
% End of CII addition
%%
%% End Preamble
%%
%
\begin{document}

% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter
  \begin{acknowledgements}
    I want to thank a few people.
  \end{acknowledgements}
  \begin{preface}
    This is an example of a thesis setup to use the reed thesis document class
    (for LaTeX) and the R bookdown package, in general.
  \end{preface}
\chapter*{List of Abbreviations}
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
                \textbf{EOF} & End of file \\
                \textbf{LZW} & Lempel Ziv Welch \\
                \textbf{RLE} & Run Length Encoding \\
            \end{tabular}
\end{table}
  \hypersetup{linkcolor=black}
  \setcounter{secnumdepth}{2}
  \setcounter{tocdepth}{2}
  \tableofcontents

  \listoftables

  \listoffigures
  \begin{abstract}
    The Lempel Ziv Welch compression algorithm is a lossless data compression algorithm used for numerous applications, including the Unix file compression utility \texttt{compress} and the GIF image format. Storing, reading, and transferring enormous amounts of data is often an issue in the biological field, especially when concerning DNA. This thesis explores the application of Lempel Ziv Welch to the compression of DNA. A variety of different optimization of the original LZW algorithm are explore included palatalizing, multiple dictionaries, and some other cool thing here broh.
  \end{abstract}
  \begin{dedication}
    You can have a dedication here if you wish.
  \end{dedication}
\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{introduction}{%
\chapter*{Introduction}\label{introduction}}
\addcontentsline{toc}{chapter}{Introduction}

When dealing with DNA, it

\hypertarget{backround-and-motivations}{%
\chapter{Backround and Motivations}\label{backround-and-motivations}}

This thesis deals with some high-level topics and uses language specific to compression research. This chapter tries to give brief summaries and examples of the relevant topics to be discussed so readers of all experience levels can put our results into context.

\hypertarget{what-is-information}{%
\section{What is information?}\label{what-is-information}}

Suppose you had an idea that you wanted to share with another person. Humans have many ways to communicate information; you could send a text message, you could tell them with words, you could tell them with sign language. But regardless of the medium, you have some idea that you want to get across. Does it matter if the other person gets your message exactly? If someone asks you ``Where library'', despite the lack of prepositions, you still understand what they mean. So did that person convey any less information than a person who asks ``Where is the library''?
Clearly, information is fundamental to how humans interact and how they understand the world, but defining it proves difficult. For our purposes, lets assumed that infomation is data with some sort of significance that makes it worth perserving and conveying.

Information on computers can take many forms, such as text, audio, and video. This information can travel through many channels including the internet, wires, screens, etc. To maximize the amount of information that can be transmitted through a channel with a limited capacity, we need to encode the information in a way which minimizes its size, while also preserving its essential features. This process is called compression.

\hypertarget{compression-metrics}{%
\section{Compression Metrics}\label{compression-metrics}}

\hypertarget{compression-ratio}{%
\subsection{Compression Ratio}\label{compression-ratio}}

Compression Ratio is the measure of size reduction achieved by a compression algorithm. It is typically expressed as a ratio of the size of the uncompressed data's original size (\(OS\)) to the size of the compressed size (\(CS\)).

\[CR = \frac{OS}{CS}\]

So a higher compression ratio means a more effective compression algorithm, and means that we were able to store more information in less space, allowing for easier storage and transfer.

\hypertarget{run-time}{%
\subsection{Run Time}\label{run-time}}

The run time is also an important part of evaluating the effectiveness of a compression algorithm. Run time is typically defined as the length of time a program takes to complete a task. Sometimes, if time is constrained, you may care less about saving space. For example, if you have the option of two compression algorithms, one with a compression ratio of 2.0, and another with a compression ratio of 2.15 but takes twice as long as the other, you may opt for a lower compression ratio to save time.

\hypertarget{memory-usage}{%
\subsection{Memory Usage}\label{memory-usage}}

Memory usage is closely tied with runtime when it comes to compression algorithms. Memory generally refers to storage where information that programs track as they are running is stored. So to reduce our run time and make a more effective compression algorithm, we want to be saving only the most important data that our algorithm needs in order to reduce our memory usage. Throughout this thesis, we will assume that most of memory usage is encapsulated in our measurement of run time.

\hypertarget{lossless-vs.-lossy-compression}{%
\section{Lossless vs.~Lossy Compression}\label{lossless-vs.-lossy-compression}}

\hypertarget{lossy}{%
\subsection{Lossy}\label{lossy}}

Lossy compression is based on the idea that not all information is vital. For instance, when saving a picture on your computer, your computer may save it in the .jpeg format to save space. Jpegs lose some of the information in the original picture and produce an overall lower quality picture, but the general information in the picture is preserved. Another example is MP3 audio files. MP3 compression discards some of the information and sound quality in exchange for a file that takes up less space, which is often favorable for small devices like MP3 players and cellphones.

\hypertarget{lossless}{%
\subsection{Lossless}\label{lossless}}

Lossless compression is the compression of data with the goal of preserving all the information in the data so that it can be reproduced perfectly on decompression. As a result, lossless compression algorithms usually don't compress as well as their lossy counterparts. Lossless algorithms are important for use cases in which data needs to be wholly recovered, like scientific data, archiving (e.g a .zip folder), and high end audio recording. Examples of lossless compression algorithms are Huffman Encoding and Lempel-Ziv-Welch, which is the focus of this thesis.

\hypertarget{classic-compression-algorithms}{%
\section{Classic Compression Algorithms}\label{classic-compression-algorithms}}

\hypertarget{run-length-encoding}{%
\subsection{Run Length Encoding}\label{run-length-encoding}}

Run Length Encoding (RLE) is one of the simplest and most intuitive forms of compression. We can take advantage of redundant runs of characters in a sequence by just giving the number of times each character appears.
Suppose you want to send the following message

\[AAGCTTTTTTTTGGGGGCCCT\]

Even if this message did mean something, we can get the information across without repeating ourselves. When writing a grocery list, you don't write ``egg egg egg egg'', you say ``4 eggs''. RLE uses this same strategy.

\[2A1G1C8T5G3C1T\]

We could compress this even further if we omit the 1 on characters that only appear once. Although not as sophisticated as other methods, RLE is effective when used on texts that have a lot of repeating characters.

\hypertarget{huffman}{%
\subsection{Huffman}\label{huffman}}

Huffman Encoding is a strategy that assigns variable length code to certain symbols in the data. The goal is to assign short codes to frequently appearing symbols and longer codes to less frequent symbols.

Suppose we have a message ``ACAGGATGGC''. We can calculate the frequency of each letter by counting the number of times each letter shows up and dividing by the total number of letters

Then, we can use the frequencies to build a tree, which will assign short codes for frequent letters and longer code for less frequent letters. The more frequent character occur higher up in the tree, giving them a shorter length. The less frequent characters occur farther down on the tree. If you follow the branches in Figure \ref{fig:reedlogo} down to a letter, it will tell you the code associated with that letter.
\begin{figure}[h]\centering


\tikzset{iv/.style={draw,fill=red!50,circle,minimum size=20pt,inner
sep=0pt,text=black},ev/.style={draw,fill=yellow,rectangle,minimum
size=20pt,inner sep=0pt,text=black}}
\begin{forest}
for tree={where n children={0}{ev}{iv},l+=8mm,
if n=1{edge label={node [midway, left] {0} } }{edge label={node [midway, right] {1} } },}
[
 [G]  
 [
  [A]
  [
    [C]
    [T]
  ]
 ] 
] 
\end{forest}
\caption{Example Huffman tree}
\label{fig:huffman}
\end{figure}
So \(G=0\), \(A = 10\), \(T=111\) and \(C=110\). Notice that none of the encodings are prefixes of one another, which makes it unambiguous in decoding.

So our message would be encoded to \(1011010001011100110\).

\hypertarget{arithmetic}{%
\subsection{Arithmetic}\label{arithmetic}}

Arithmetic encoding is another lossless compression algorithm that uses probability to assign codes to symbols in the message. Unlike Huffman, arithmetic encoding assigns a single code to the whole message, rather than separate codes for each symbol.

Here is a simple example. Say we want to encode a string of characters ``ACGT''. Arithmetic Encoding also requires the encoder and decoder know the probabilities of each of the characters that could possibly be in the message. Let's say the probability of each symbol in the message are
\begin{itemize}
\tightlist
\item
  P(A) = 1/10
\item
  P(C) = 2/10
\item
  P(G) = 4/10
\item
  P(T) = 3/10
\end{itemize}
We want to represent the message as a fractional number between 0 and 1. We will divide the interval {[}0,1{]} into sub intervals using the probabilities of each character in the message. That way, each symbol is represented by the sub-interval that corresponds to its probability.

Since `A' comes first, we divide {[}0,1{]} into {[}0.0,0.1). Since `C' is next, we go from {[}0.0,0.1{]} to {[}0.01, 0.03). Then since `G is next, we go from {[}0.01, 0.03) to {[}0.016, 0.02). Finally, since 'T' is last, we go from {[}0.016, 0.02) to {[}0.0188, 0.02).

So any number in the interval can be used to represent our message.

Arithmetic encoding can have a better compression ratio that Huffman in some cases, but the computation time is often not worth the payoff.

\textbf{TODO}: Add source

\hypertarget{lempel-ziv-welch}{%
\subsection{Lempel-Ziv-Welch}\label{lempel-ziv-welch}}

Lempel-Ziv-Welch is another lossless compression algorithm. When compressing, LZW builds a dictionary of codewords, where codewords represent strings previously seen in the message. As it compresses the message, the dictionary grows. The compression algorithm leaves behind the codewords and some of the original characters, allowing the decompression algorithm to build up the same dictionary as it decompresses the message.

Here is a simple example. We may be sending messages with the characters `A', `C', `T', `G', so we can start by assigning those strings codewords. So our dictionary will start as \{`A' = 0, `C' = 1, `T' = 2, `G' = 3\}. Say we want to send the message

\[\text{AAGGAATCC}\]

When we compress, we start at the beginning of the message and scan through. We ask ourselves, ``Is''A'' in our dictionary?''

\[\textbf{A} \text{AGGAATCC}\]

Since we started with ``A'' in our dictionary, we can move on. We then add on the next character in the sequence and ask ``Is''AA'' in our dictionary?''

\[\textbf{AA} \text{GGAATCC}\]

We have not seen ``AA'' before, so we should add it to our dictionary. So now our dictionary looks like this \{`A' = 0, `C' = 1, `T' = 2, `G' = 3, `AA' = 4\}. Next time we see ``AA'', we know it is associated with the codeword 4. To indicate this, in our resulting string we will output the code for ``A'', the part of the string we've seen before, and the character ``A''.

So the encoded string will look something like

\[\text{0A....}\]
\begin{table}
\begin{tabular}{ | c | c | p{.4\textwidth} | c | }
\hline
\textbf{Step} & \textbf{Input String} & \textbf{Dictionary State} & \textbf{Encoded String} \\
\hline
1 & \textbf{A}AGGAATCC & {A: 0, G: 1, T: 2, C: 3} & - \\
\hline
2 & \textbf{AA}GGAATCC & {A: 0, G: 1, T: 2, C: 3} & 0A \\
\hline
3 & AA\textbf{G}GAATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4} & 0A  \\
\hline
4 & AA\textbf{GG}AATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4} & 0A1G   \\
\hline
5 & AAGG\textbf{A}ATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5} & 0A1G \\
\hline
6 & AAGG\textbf{AA}TCC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6} & 0A1G  \\
\hline
7 & AAGG\textbf{AAT}CC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6} & 0A1G4T  \\
\hline
8 & AAGGAAT\textbf{C}C & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6, AAT: 7} & 0A1G4T \\
\hline
9 & AAGGAAT\textbf{CC} & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6, AAT: 7} & 0A1G4T3C \\
\hline
10 & AAGGAATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6, AAT: 7, CC: 8} & 0A1G4T3C \\
\hline
\end{tabular}
\caption{ An example of LZW ran on the input "AAGGAATCC"}
\end{table}
When we are decoding, we start with the same dictionary. We see ``0A'' and know that that means ``Take the string that has codeword 0, add on the character `A' and add that new string to the dictionary. We would add that to the dictionary and assign it to our next available codeword, 4. As you can see, while decoding, we are able to build up the same dictionary as was used for encoding, as long as we use the same starting dictionary.

This method has several convenient properties:
\begin{itemize}
\tightlist
\item
  When we send this encoding to someone else, we don't need to send a ``codebook'' (our dictionary). They are able to build it up themselves as they decode.
\item
  you only need to go over the data once to encode and decode. This means that the run time of the algorithm should roughly increase linearly with the length of the input.
\item
  As runs get longer, we will start to see more and more repeating patterns, and replacing them with codewords will become more and more effective.
\end{itemize}
To decode, we can simply start with the same dictionary. We see codewords followed by one character, so we decode that codeword and add the character to the end. Since the decoder continues to look for codewords, we need some special character at the end of our encoding to let the decoder know when the message is done.

Now that we have laid out how the algorithm works, we can get more specific for our use case. For us, the input is a file on the computer, and the output is also a file (hopefully a smaller one). We read all the characters in the file, encode them, and put them into a new file. Then, when we want to decode, we read the encoded file and output the decoded characters. Again, LZW is lossless, so the original file and the decoded file should be identical.

Here is some example pseudo code on what this algorithm would look like.
\begin{verbatim}
LZWEncode(input):

    Dictionary dictionary; // where we store our string => codeword mappings
    dictionary.inititalize; // initialzie the dictionary with single characters

    codeword; // the unique numbers we assign strings
    output; // where we output the encoded characters

    currentBlock = first character of input;
    for every nextCharacter in the input:
        
        // this function returns the longest string we've already seen
        // starting at our current place in the input
        nextLongestRun = findLongestInDict();

        if currentCharacter + nextLongestRun.length > input.length:
            break;
            
        // output the code of the next longest run and next character
        code = dictionary.lookup(nextLongestRun);
        nextCharacter = input[nextLongestRun + 1]
        output(code);
        output(nextCharacter);

        
        dictionary.add(currentBlock + nextCharacter, map it to codeword);

        codeword = codeword + 1;
        input = input + nextLongestRun + 1;


    output(special end of file character);
\end{verbatim}
The decoding is much simpler. The only real difference is that we are now mapping codewords to strings, since the encoded string contains codewords.
\begin{verbatim}
LZWDecode(input):

    Dictionary dictionary; // where we store our codeword => string mappings
    dictionary.inititalize; // initialzie the dictionary with single characters

    codeword; // the unique numbers we assign strings
    result; // where we output the encoded characters

    while we don't see the end of file character:

        codewordFound = input.readCodeword()
        nextCharacter = input.readCharacter()

        sequence = dictionary.lookup(codewordFound) + nextCharacter
        result.output(sequence)

        dictionary.add(sequence = codeword)
        codeword = codeword + 1;
\end{verbatim}
This is the basic strategy we will start with for our LZW algorithm. In the next chapter, we will go over parts of the algorithm in depth in C++. Here is a quick summary of terms repeated throughout the next two chapters.
\begin{itemize}
\tightlist
\item
  \texttt{dictionary}: a key-value system. Like a real dictionary holds words and their corresponding definitions, our dictionary holds codewords and their corresponding strings of characters. In C++, this is called a \texttt{map} and uses a hash table, but the concept is the same.
\item
  \texttt{codeword}: a number used to take the place of a string in our encoding.
\item
  \texttt{run}: the next run of characters in our input that are already in our dictionary. So if we are encoding ``ACTG'', and ``A'', ``AC'', and ``ACT'' are in the dictionary but ``ACTG'' is not, we have a run of 3.
\item
  \texttt{EOF}: end of file, the special character that we need to output at the end of the encoding.
\end{itemize}
\hypertarget{related-work}{%
\section{Related work}\label{related-work}}

The idea of compressing DNA is not novel, nor is the idea of using LZW for this purpose. DNA compression is a significant research area, in the intersection of bioinformatics, computer science, and mathematics.

There has been several attempts to optimize LZW by computer science researchers. One paper made use of multiple indexed dictionaries in order to speed up the compression process (Keerthy, 2019). The concept is simple: rather than a single large dictionary, have multiple dictionaries, one for each possible string length That way, the dictionaries grow more slowly and accesses are faster. This paper also used Genomic data to gather their metrics and compared their algorithm to other popular DNA compression techniques, which makes it particularly relevant for this thesis.

Another paper used simple parallelization techniques to improve compression speed (Pani, Mishra, \& Mishra, 2012). Rather than compressing the whole file linearly, the researches broke the file into portions and compressed them with LZW in parallel, which greatly increased the compression speed at the cost of a reduced compression ratio.

Yet another paper made use of Chinese Remainder Theorem to augment Lempel-Ziv-Welch (Ibrahim \& Gbolagade, 2020). They saw great reduction in compression time without compromising compression ratio, although these results could not be verified. The details of their implementation were not clear from the paper. We tried multiple different methods of utilizing CRT given the pseudocode in their paper, but we could not get anything that looked like it may improve compression time. We reached out to the authors of the paper, but we were not able to further our progress on this method and thus the it is not used in this thesis.

DNA-specific compression algorithms have also been around for a while. These papers do not focus on LZW, but they do consider some similar methods.

One of the first papers exploring this was published in 1994 (Grumbach \& Tahi, 1994). It proposes an algorithm called \texttt{biocompress2}, expanding on a previous paper by the same author. They focus on encoding palindromes in DNA sequences, which doesn't do much to help the compression ratio. However, this paper has been cited by many following papers sparking interest in DNA compression, and the collection of sequences that it uses for algorithm comparison is used in this thesis.

Chen et al.~proposed an algorithm called GenCompress, which uses approximate matching (Chen, Kwong, \& Li, 2001). It matches sequnces to sequences already seen, and maps those sequences using various edits to turn one sequence into another. They are able to achieve a great compression ratio with this method, although their technique is computationally expensive.

In 2007, Minh Cao et al.~published a paper detailing another algorithm, XM, which uses statistical methods to try and predict the next character while encoding and decoding (Cao, Dix, Allison, \& Mears, 2007). This method was found to outperform both Biocompress2 and GenCompress in terms of compression ratio.

As a whole, these papers give us some guidance in terms of where to aim our research. Most of them boast great compression ratios, but their methods can be very computationally intensive in some cases, and thus, slow. We will aim to use previous research on LZW to make a very fast implementation for DNA sequences, then try and use characteristics of the sequences to improve compression ratio. Our hope isn't necessarily to create the best compression ratio out of all these methods, but to make a fast LZW implmentation with a respectable compression ratio. If we are able to make the algorithm very fast, it may be preferable to these other algorithms in some cases if the file is very large.

\hypertarget{optimizing-lzw-approach}{%
\chapter{Optimizing LZW: Approach}\label{optimizing-lzw-approach}}

To restate the goal of this thesis, we seek to optimize LZW for use in compression of DNA. I chose to write in C++. A majority of the work in this thesis involved rewriting, refactoring, and reconfiguring code to improve performance. The various methods we used for this process are discussed throughout the chapter.

While we may not end up creating the best DNA compressor available, the objective is to explore the boundaries of LZW and to tailor it as best we can for the task of DNA compression. As you will see, the algorithm has limitations.

Our Strategy was as follows:
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement a basic version of LZW in C++.
\item
  Optimize the algorithm. Make it as fast as possible, and specifically focus on compressing DNA
\item
  Once the algorithm is fast, try to entropy encode (Huffman, arithmetic encoding, etc) the compressed files to improve compression ratio
\end{enumerate}
\hypertarget{corpora}{%
\section{Corpora}\label{corpora}}

Most compression papers make use of a corpus, which is a collection of files to run a compression algorithm on in order to evaluate performance and to compare the performance of different algorithms to one another.

In the world of DNA compression, there are several academic papers on the subject. One of the first and most popular of the papers was published in 1994, and the selection of DNA sequences used in the paper have become an informal corpus for the subject of DNA compression, cited by more than thirty publications (Grumbach \& Tahi, 1994).
\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:corpus1filesfig}Corpus 1}\tabularnewline
\toprule()
Name & bytes \\
\midrule()
\endfirsthead
\toprule()
Name & bytes \\
\midrule()
\endhead
chmpxx & 121024 \\
chntxx & 155844 \\
hehcmv & 229354 \\
humdyst & 38770 \\
humghcs & 66495 \\
humhbb & 73308 \\
humhdab & 58864 \\
humprtb & 56737 \\
mpomtcg & 186609 \\
mtpacga & 100314 \\
vaccg & 191737 \\
\bottomrule()
\end{longtable}
Another, newer paper aimed to create a corpus specifically for compressing DNA (Pratas \& Pinho, 2018). They put together a corpus of DNA sequences for this purpose, as summarized below. Since the papers publishing, it has been cited by several DNA compression papers.
\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:corpus2filesfig}Corpus 2}\tabularnewline
\toprule()
Name & bytes \\
\midrule()
\endfirsthead
\toprule()
Name & bytes \\
\midrule()
\endhead
AeCa & 1591049 \\
AgPh & 43970 \\
BuEb & 18940 \\
DaRe & 62565020 \\
DrMe & 32181429 \\
EnIn & 26403087 \\
EsCo & 4641652 \\
GaGa & 148532294 \\
HaHi & 3890005 \\
HePy & 1667825 \\
HoSa & 189752667 \\
OrSa & 43262523 \\
PlFa & 8986712 \\
ScPo & 10652155 \\
YeMi & 73689 \\
\bottomrule()
\end{longtable}
The dataset in Table \ref{tab:corpus2filesfig} is publicly available at this \url{https://tinyurl.com/DNAcorpus}.

\hypertarget{evaluating-performance}{%
\section{Evaluating Performance}\label{evaluating-performance}}

Evaluating performance of a program is difficult. There is a notion of theoretical run time, but on an actual computer there are many processes running in the background, so it can be hard to get a consistent reading on performance.

To attempt to counteract this, we ran the function on the same file multiple times, and took the median of the compression and decompression times for all the runs. Also, for any graphs or tables in this thesis, all versions of the algorithm for a particular table were run on the same computer. We also did our best to mitigate any other programs running on the computer at the time of data collection to prevent interference.

Most graphs in this section will refer to throughput and average compression time. Throughput is defined as the number of bytes that the program processes per second. The higher the throughput, the more efficient the algorithm. The average compression time is taken as a harmonic average, where the times are weighted by the size of the file.

\hypertarget{a-starting-point}{%
\section{A Starting Point}\label{a-starting-point}}

As stated previously, we thought it was best to get a working implementation of LZW in C++ on regular text files, , and then optimize it for DNA. We want to try various techniques tried by researches in the field, but it is important to have a fast baseline from which we can compare and improve upon. If the initial implementation is inefficient, it makes us harder to tell if the different techniques we have are affecting performance.

\hypertarget{growing-codewords-and-bit-output}{%
\subsection{Growing Codewords and Bit Output}\label{growing-codewords-and-bit-output}}

When reading files on the computer, most characters are stored as bytes, which are made up of 8 bits. For instance \texttt{01000001} stands for the letter `A' in ASCII encoding. Numbers in binary are simpler to display, so \texttt{00000001} is 1, \texttt{00000010} is 2, and so on.

But if we are translating numbers to binary, we don't need all of the bits in a byte. In binary, \texttt{1} is the same as \texttt{01} is the same as \texttt{00000000000001}. So when we are outputting codewords for LZW, we don't necessarily need to output a whole byte. We can have growing codewords.

As the number of codewords grows, the number of bits needed to represent it also grows. So if we are on codeword 8, we need 4 bits since 8 is \texttt{1000}. As our dictionary grows, we can grow the number of bits needed to display a codeword and save a lot of space in our compressed document.

So we needed a method of outputting bits one by one, and reading in bits one by one. This is not something that is supported in C++ on its own. We were able to create this functionality by defining a class.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// BitInput: Read a single bit at a time from an input stream.}
\CommentTok{// Before reading any bits, ensure input stream still has valid input}
\KeywordTok{class}\NormalTok{ BitInput }\OperatorTok{\{}
 \KeywordTok{public}\OperatorTok{:}
  \CommentTok{// Construct with an input stream}
\NormalTok{  BitInput}\OperatorTok{(}\AttributeTok{const} \DataTypeTok{char}\OperatorTok{*}\NormalTok{ input}\OperatorTok{);}

\NormalTok{  BitInput}\OperatorTok{(}\AttributeTok{const}\NormalTok{ BitInput}\OperatorTok{\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}
\NormalTok{  BitInput}\OperatorTok{(}\NormalTok{BitInput}\OperatorTok{\&\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}

  \CommentTok{// Read a single bit (or trailing zero)}
  \CommentTok{// Allowed to crash or throw an exception if past end{-}of{-}file.}
  \DataTypeTok{bool}\NormalTok{ input\_bit}\OperatorTok{();}

  \DataTypeTok{int}\NormalTok{ read\_n\_bits}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ n}\OperatorTok{);}
\OperatorTok{\}}

\CommentTok{// BitOutput: Write a single bit at a time to an output stream}
\CommentTok{// Make sure all bits are written out when exiting scope}
\KeywordTok{class}\NormalTok{ BitOutput }\OperatorTok{\{}
 \KeywordTok{public}\OperatorTok{:}
  \CommentTok{// Construct with an input stream}
\NormalTok{  BitOutput}\OperatorTok{(}\BuiltInTok{std::}\NormalTok{ostream}\OperatorTok{\&}\NormalTok{ os}\OperatorTok{);}

  \CommentTok{// Flushes out any remaining bits and trailing zeros, if any:}
  \OperatorTok{\textasciitilde{}}\NormalTok{BitOutput}\OperatorTok{();}

\NormalTok{  BitOutput}\OperatorTok{(}\AttributeTok{const}\NormalTok{ BitOutput}\OperatorTok{\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}
\NormalTok{  BitOutput}\OperatorTok{(}\NormalTok{BitOutput}\OperatorTok{\&\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}

  \CommentTok{// Output a single bit (buffered)}
  \DataTypeTok{void}\NormalTok{ output\_bit}\OperatorTok{(}\DataTypeTok{bool}\NormalTok{ bit}\OperatorTok{);}

  \DataTypeTok{void}\NormalTok{ output\_n\_bits}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ bits}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ n}\OperatorTok{);}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
So when we are encoding and need to output a codeword, we can \texttt{output\_n\_bits}, where \texttt{n} is the number of bits needed to display our greatest codeword. When decoding, we can just \texttt{read\_n\_bits}.

\hypertarget{getting-eof-to-work}{%
\subsection{Getting EOF to work}\label{getting-eof-to-work}}

One of the very early issues with the implementation was how to denote the end of a file. The early implementation would work for some files, but for others the very last part of the file would be lost after encoding and then decoding.

In theoretical implementations of LZW, computer scientists tend to denote the end of a message with a special character, one that isn't seen anywhere else in the file. In this initial implementation, that wasn't possible because we wanted to be able to compress any file with any characters.

The solution was to reserve a codeword to mark the end of the file. So we start with a starting dictionary containing all ASCII characters.
\begin{Shaded}
\begin{Highlighting}[]
    \BuiltInTok{std::}\NormalTok{unordered\_map}\OperatorTok{\textless{}}\BuiltInTok{std::}\NormalTok{string}\OperatorTok{,} \DataTypeTok{int}\OperatorTok{\textgreater{}}\NormalTok{ dictionary}\OperatorTok{;}
    \ControlFlowTok{for} \OperatorTok{(}\DataTypeTok{int}\NormalTok{ i }\OperatorTok{=} \DecValTok{0}\OperatorTok{;}\NormalTok{ i }\OperatorTok{\textless{}} \DecValTok{256}\OperatorTok{;} \OperatorTok{++}\NormalTok{i}\OperatorTok{)\{}
        \BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{str1}\OperatorTok{(}\DecValTok{1}\OperatorTok{,} \DataTypeTok{char}\OperatorTok{(}\NormalTok{i}\OperatorTok{));}
\NormalTok{        dictionary}\OperatorTok{[}\NormalTok{str1}\OperatorTok{]} \OperatorTok{=}\NormalTok{ i}\OperatorTok{;}
    \OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
As stated in Section \ref{lempel-ziv-welch}, we will need to reserve a codeword to output when we are done encoding the file so that the decoder knows where the end is. So the algorithm goes along reading a file. It builds up a current string character by character, adding the character to the string and checking if it has seen that sequence before. Once it find the end of file, we stop and output the EOF codeword.

The problem was, what about what is left over? Suppose we are reading a file, and the file ends with ``ACCT''. If ``A'' is in the dictionary, we see if ``AC'' is in the dictionary, and so on. This leaves us with three possible cases when we reached the end of the file
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``ACC'' was in the dictionary but ``ACCT'' was not. This means we can output the codeword for ``ACC'', follow it by the character ``T'', and we are done. This is the ideal scenario, because nothing is left over when we output the EOF codeword
\item
  ``ACCT'' was in the dictionary: This means we have one more codeword to output, but since we reached the end of the file, we never got to output it.
\item
  ``AC'' was in the dictionary, but ``ACC'' was not: in this case, we would output the codeword for ``AC'' output the character ``C'', and then start looping again starting at ``T''. But we reach the end of the file, so we output EOF before outputting T.
\end{enumerate}
We solved this issue by adding 2 extra bits after the EOF codeword. These bits denote the case that occurred
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// after we\textquotesingle{}ve encoded, we either have }
\CommentTok{// no current block (case 0)}
\CommentTok{// we have a current block that is a single character (case 1)}
\CommentTok{// otherwise we have a current block \textgreater{} 1 byte (default)}
\ControlFlowTok{switch} \OperatorTok{(}\NormalTok{currentBlock}\OperatorTok{.}\NormalTok{length}\OperatorTok{())\{}
\ControlFlowTok{case} \DecValTok{0}\OperatorTok{:}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{false}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{false}\OperatorTok{);}
    \ControlFlowTok{break}\OperatorTok{;}
\ControlFlowTok{case} \DecValTok{1}\OperatorTok{:}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{false}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{true}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_n\_bits}\OperatorTok{((}\DataTypeTok{int}\OperatorTok{)}\NormalTok{ currentBlock}\OperatorTok{[}\DecValTok{0}\OperatorTok{],}\NormalTok{ CHAR\_BIT}\OperatorTok{);}
    \ControlFlowTok{break}\OperatorTok{;}
\ControlFlowTok{default}\OperatorTok{:}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{true}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{true}\OperatorTok{);}

    \DataTypeTok{int}\NormalTok{ code }\OperatorTok{=}\NormalTok{ dictionary}\OperatorTok{[}\NormalTok{currentBlock}\OperatorTok{];}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_n\_bits}\OperatorTok{(}\NormalTok{code}\OperatorTok{,}\NormalTok{ codeword\_size}\OperatorTok{);}
    \ControlFlowTok{break}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
So when the decoder is reading and encounters the EOF codeword, it can look at the next two bits to see if anything is left over.

At this point, there was a working implementation that was able to compress and decompress files. Here is the performance of this version on the two corpora.

\hypertarget{using-constants}{%
\subsection{Using Constants}\label{using-constants}}

The early version of the code was not clean. There were hard coded variables, unspecified integer types, and generally messy naming conventions that made the code difficult to read and debug.

The next major step in the code was to start using constants for everything, including
\begin{itemize}
\tightlist
\item
  \texttt{STARTING\_CODEWORD}: What codeword we should start at
\item
  \texttt{EOF\_CODEWORD}: What we should output when we reach end of file
\item
  \texttt{STARTING\_DICT\_SIZE}: At this stage, we had a starting dict size of 256 to hold all possible bytes, but later we will specialize for DNA
\end{itemize}
It also made sense to start using a specific type for codewords. At this stage, we opted for a 32 bit unsigned integer.

There are several tools at a developers disposal when looking to debug and optimize code. One tool used for this thesis was \texttt{callgrind} which is a tool of \texttt{valgrind} a profiling tool. Profiling tools are used to look at how your code works, where the bottlenecks are, and what can be changed/improved for the performance of your code.

Callgrind in particular is a profiling tool which associates assembly instructions to lines of code, indicating to the programmer which lines take a lot of instructions and which take less. For those unfamiliar, assembly instructions are what code is turned into so that it can be ran on your computer's processor. In general, more instructions means that code takes longer to run.

The callgrind output drew attention to one particular part of the code. A C++ \texttt{unordered\_map} uses iterators, basically pointers into the dictionary. If an entry is not present in the dictionary, the \texttt{find()} function will return a iterator to the end of the dictionary.

The check for this in our algorithm looked like this.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// if we\textquotesingle{}ve already seen the sequence, keep going}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ dictionary}\OperatorTok{.}\NormalTok{end}\OperatorTok{())\{}
\NormalTok{    currentBlock }\OperatorTok{=}\NormalTok{ currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
Here is the callgrind output for that line.
\begin{verbatim}
105,030,135 ( 0.18%)          if (dictionary.find(currentBlock + next_character) != dictionary.end()){
13,537,450,317 (22.83%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char) (3,890,005x)
11,653,779,430 (19.65%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::find(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (3,890,005x)
2,108,383,120 ( 3.56%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string() (3,890,005x)
956,941,242 ( 1.61%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::end() (3,890,005x)
241,180,314 ( 0.41%)  => /usr/include/c++/9/bits/hashtable_policy.h:bool std::__detail::operator!=<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true>(std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&, std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&) (3,890,005x)
\end{verbatim}
As shown, this line is taking a significant amount of instructions, and it needs to pull the end() of the dictionary each time it is ran. If we use cend() instead and save that iterator in a variable called end, we can save a significant amount of instructions.
\begin{verbatim}
89,470,115 ( 0.61%)          if (dictionary.find(currentBlock + next_character) != end ){
3,353,009,053 (22.78%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char) (3,890,005x)
2,833,786,025 (19.26%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::find(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (3,890,005x)
420,120,704 ( 2.85%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string() (3,890,005x)
50,570,065 ( 0.34%)  => /usr/include/c++/9/bits/hashtable_policy.h:bool std::__detail::operator!=<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true>(std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&, std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&) (3,890,005x)
\end{verbatim}
Of course, there are several issues with this method. It is difficult to associate instructions with a single line of code. Some lines are interdependent, and assembly often behaves differently than the code that produces it. Another thing is that compliers are very advanced, and sometimes small optimizations like this are done by the compiler automatically.

Despite these issues, this change was still worth making, if not to save time then for sake of clarity and readability of the code. Also, despite the inaccuracy of callgrind, like many profiling tools, its job is not necessarily to provide exact measurements of code performance, but to give indications to trouble spots which can be improved.

Here are the runs after this optimization.

\hypertarget{extraneous-string-concatenations}{%
\subsection{Extraneous String Concatenations}\label{extraneous-string-concatenations}}

The LZW algorithm is built on iteration: we go through each character, adding it to our current block. If we've seen that current block before, we keep going. If not, we add that block to the dictionary and start over.

Another thing that I noticed from the callgrind output was that a lot of time/instructions are being spent on string concatenation. In general, string concatenation in most language, including C++, have a lot of overhead. A lot of implementations involve creating a new string every time you concatenate two existing string, which can have a significant performance penalty.

In the version of the algorithm at the time, every time we have already seen a sequence, we have to concatenate a character. I noticed that I was doing this concatenation multiple times without needing to.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// we concatenate the strings here}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ end }\OperatorTok{)\{}
    \CommentTok{// and here}
\NormalTok{    currentBlock }\OperatorTok{=}\NormalTok{ currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\OperatorTok{\}}
\ControlFlowTok{else}\OperatorTok{\{}

    \CommentTok{// other code here ommitted}


    \CommentTok{// and here! }
\NormalTok{    dictionary}\OperatorTok{[}\NormalTok{currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{]} \OperatorTok{=}\NormalTok{ codeword}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
If I just save \texttt{currentBlock\ +\ next\_character} into a new variable, that will save me from doing the concatenation 2 more times.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// save concatenation here}
\BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{string\_seen\_plus\_new\_char }\OperatorTok{=}\NormalTok{ current\_string\_seen }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ end }\OperatorTok{)\{}
\NormalTok{    current\_string\_seen }\OperatorTok{=}\NormalTok{ string\_seen\_plus\_new\_char}\OperatorTok{;}
\OperatorTok{\}}
\ControlFlowTok{else}\OperatorTok{\{}

\CommentTok{// other code omitted here}

\NormalTok{    dictionary}\OperatorTok{[}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{]} \OperatorTok{=}\NormalTok{ codeword}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
Here are the statistics on the version of the algorithm after this change.

\hypertarget{dictionary-lookups}{%
\subsection{Dictionary Lookups}\label{dictionary-lookups}}

Dictionary lookups can be expensive, especially with the standard library. We learned from \texttt{callgrind} that along with string operations, our program spent a lot of time doing these lookups.

We looked for ways to reduce the volume of lookups. At the time, the way the algorithm worked was that it looked up the current string and the next character in the dictionary. If that string is in the dictionary, we keep going. If not, we output the codeword for the current string.

But, the current string on this iteration is the current string from the last iteration, plus one character. So when we were on the previous iteration of the loop, we could save that lookup and prevent a second lookup.
\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{while}\OperatorTok{(}\NormalTok{next\_character }\OperatorTok{!=}\NormalTok{ EOF}\OperatorTok{)\{}

    \CommentTok{// code omitted}


    \CommentTok{// if we\textquotesingle{}ve already seen the sequence, keep going}
    \BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{string\_seen\_plus\_new\_char }\OperatorTok{=}\NormalTok{ current\_string\_seen }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
    \CommentTok{// save this iterator\textasciigrave{}}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ not\_in\_dictionary }\OperatorTok{)\{}
\NormalTok{        current\_string\_seen }\OperatorTok{=}\NormalTok{ string\_seen\_plus\_new\_char}\OperatorTok{;}
    \OperatorTok{\}}
    \ControlFlowTok{else}\OperatorTok{\{}

        \CommentTok{// shouldn\textquotesingle{}t look up again}
        \DataTypeTok{int}\NormalTok{ code }\OperatorTok{=}\NormalTok{ dictionary}\OperatorTok{[}\NormalTok{current\_string\_seen}\OperatorTok{];}


        \CommentTok{// code omitted}
    \OperatorTok{\}}
\NormalTok{    next\_character }\OperatorTok{=}\NormalTok{ input}\OperatorTok{.}\NormalTok{get}\OperatorTok{();}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
We can save that lookup, like so.
\begin{Shaded}
\begin{Highlighting}[]

\ControlFlowTok{while}\OperatorTok{(}\NormalTok{next\_character }\OperatorTok{!=}\NormalTok{ EOF}\OperatorTok{)\{}

    \CommentTok{// code ommitted}

    \CommentTok{// if we\textquotesingle{}ve already seen the sequence, keep going}
    \BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{string\_seen\_plus\_new\_char }\OperatorTok{=}\NormalTok{ current\_string\_seen }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\NormalTok{    codeword\_seen\_now }\OperatorTok{=}\NormalTok{ dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{);}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{codeword\_seen\_now }\OperatorTok{!=}\NormalTok{ not\_in\_dictionary }\OperatorTok{)\{}
\NormalTok{        current\_string\_seen }\OperatorTok{=}\NormalTok{ string\_seen\_plus\_new\_char}\OperatorTok{;}
\NormalTok{        codeword\_seen\_previously }\OperatorTok{=}\NormalTok{ codeword\_seen\_now}\OperatorTok{;} \CommentTok{// save codeword here}
    \OperatorTok{\}}
    \ControlFlowTok{else}\OperatorTok{\{}

        \CommentTok{// on the next iteration, we use it here}
        \DataTypeTok{int}\NormalTok{ code }\OperatorTok{=}\NormalTok{ codeword\_seen\_previously}\OperatorTok{{-}\textgreater{}}\NormalTok{second}\OperatorTok{;}

        \CommentTok{// code omitted}

    \OperatorTok{\}}
\NormalTok{    next\_character }\OperatorTok{=}\NormalTok{ input}\OperatorTok{.}\NormalTok{get}\OperatorTok{();}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{using-const-char}{%
\subsection{Using Const Char *}\label{using-const-char}}

The algorithm works by reading through the entire file, so we know that at some point, we will need to see every byte of the entire file.

When reading a byte stream of the file, the file may not be in memory the way we want it. The \texttt{ifstream} class in C++ also has many extraneous feature that we don't need. If we map the file directly into memory using \texttt{mmap} and pass around a pointer to that data, it will simplify and speed up the scanning process. Also, using a \texttt{char*} opens the possibility to getting rid of \texttt{std::string} entirely, which means way less overhead and decreased compression time.

\hypertarget{comparison}{%
\subsection{Comparison}\label{comparison}}

Taking metrics of the algorithm at each of the stages listed in this chapter, we can make a graph showing the improvements in performance.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/comparecorporaoptimizingfig-1.pdf}
\caption{\label{fig:comparecorporaoptimizingfig}Comparison of the performance of the different milestones}
\end{figure}
Figure \ref{fig:comparecorporaoptimizingfig} shows the performance of the algorithm at the different milestones mentioned in this section. All graphs use the files from both corpora unless stated otherwise. On the left side of the figure you see that throughput increases for the program with each optimization. On the right, observe that mean compression time decreases with each optimization.

\hypertarget{trying-different-dictionaries}{%
\section{Trying Different Dictionaries}\label{trying-different-dictionaries}}

A lot of the stress of the LZW algorithm is on the dictionary. We are constantly looking strings up and placing others. Because of the reliance on this data structure, we know that the dictionary accesses and lookups are a bottleneck, so improvements in those areas could greatly increase the efficiency of our program.

So another step towards an efficient LZW seemed to be to abstract out the C++ \texttt{std::unordered\_map} and have multiple different dictionary implementations to try and experiment with in our attempt to optimize LZW for DNA compression.

\hypertarget{direct-map}{%
\subsection{Direct Map}\label{direct-map}}

In our analysis of the two corpora, we found some interesting statistics in the redundancy of the data.
\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{rrrr}
\toprule
AVERAGE RUN LENGTH & MAXIMUM RUN LENGTH & MEDIAN RUN LENGTH & SD RUN LENGTH\\
\midrule
\cellcolor{gray!6}{6.135398} & \cellcolor{gray!6}{17} & \cellcolor{gray!6}{6} & \cellcolor{gray!6}{1.237217}\\
\bottomrule
\end{tabular}}
\end{table}
\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{rrrr}
\toprule
AVERAGE RUN LENGTH & MAXIMUM RUN LENGTH & MEDIAN RUN LENGTH & SD RUN LENGTH\\
\midrule
\cellcolor{gray!6}{10.96825} & \cellcolor{gray!6}{190} & \cellcolor{gray!6}{11} & \cellcolor{gray!6}{2.494305}\\
\bottomrule
\end{tabular}}
\end{table}
Tables \ref{tab:runstatsfigcp1} and \ref{tab:runstatsfigcp2} show stats on the run lengths of the ``runs'' of data, where a run is a string added to the dictionary during a run of the LZW algorithm, and a histogram of runs from both corpora can be seen in Figure \ref{fig:allrunshistfig}.
\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{rrrrr}
\toprule
X & AVERAGE RUN LENGTH & MAXIMUM RUN LENGTH & MEDIAN RUN LENGTH & SD RUN LENGTH\\
\midrule
\cellcolor{gray!6}{1} & \cellcolor{gray!6}{10.95318} & \cellcolor{gray!6}{190} & \cellcolor{gray!6}{11} & \cellcolor{gray!6}{2.505904}\\
\bottomrule
\end{tabular}}
\end{table}
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/allrunshistfig-1.pdf}
\caption{\label{fig:allrunshistfig}A histogram showing the lengths of runs for both copora.}
\end{figure}
Given this data, it is clear that it would be advantageous to speed up the dictionary for smaller run sizes, since most of the runs are below size 15. As stated before, there have been research papers about the possibility of using multiple indexed dictionaries for LZW, including Keerthy (Keerthy, 2019).
To achieve a similar effect to multiple indexed dictionaries, we opted for a unique approach. Rather than use a hashmap, we can map the strings directly into memory. Since all of the strings only contain four characters (`A', `C', `T', and `G'), we can represent the characters with two bits. So for a length \texttt{n} string, we can represent it with \texttt{2n} bits.

So we can create an indexed dictionary directly in memory for all strings below a certain length. We can use the \texttt{2n} bit representation of the string
to index into an array of codewords.

For each string size 1 to n, we have an array with enough slots for every possible string. For example, for strings of length 3, we have an array of size \(4^3\), since there are \(4^3\) possible strings. In each of those \(4^3\) slots, we have space for a codeword. All strings of length 3 can be represented by 6 bits, and since 6 bits can represent \(2^6=4^3\) values, we can use the bit representation to index into the dictionary. If the codeword at that place in the dictionary is 0, we have never seen it before. If it is non-zero, we have found the codeword for that string. For all strings greater than \texttt{n}, we can just use a hashmap on top to handle those.

As seen in Figure \ref{fig:directmaplength10statsfig}, the both the average compression time and throughput are greater for a direct map dictionary with a max string length of 15 as opposed to a max string length of 10. This makes sense, because any string over the max requires an entry in a hashmap, which takes much more time than a dictionary access. Using a string length of over 15 is difficult because the amount of memory required increases exponentially.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/directmaplenthcompfig-1.pdf}
\caption{\label{fig:directmaplenthcompfig}Comparing different max lengths for Direct Map}
\end{figure}
It's also worth noting that since we are allowing strings of all lengths, the compression ratio has not changed.

\hypertarget{multiple-indexed-dictionaries}{%
\subsection{Multiple Indexed Dictionaries}\label{multiple-indexed-dictionaries}}

Similar to the Direct Mapped approach, we use dictionaries for each string size up to a certain size \texttt{n} , and for all strings of length greater than \texttt{n}, we use a regular dictionary. As with the direct map dictionary, we need to specify a max string length. We collected metrics for different choices of max string length. As seen in Figure \ref{fig:multdictlengthcompfig}, the throughput tends to increase and the average compression time tends to increase as we increase the number of indexed dictionaries. This result was not necessarily one we expected, but it does make sense that there is a certain amount of overhead that is required for a hashmap. The hash function takes about the same amount of time no matter the number of elements in the map, and resizing is rare. So adding more dictionaries only adds more overhead, which tends to slightly decrease efficiency.

Compression ratio still remains the same as strings of all lengths are accommodated.
This logic is supported by Figure \ref{fig:multvsonedictfig}, which shoes one \texttt{std::unordered\_map} compared the multiple indexed model.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/multdictlengthcompfig-1.pdf}
\caption{\label{fig:multdictlengthcompfig}Comparing different max lengths for Direct Map}
\end{figure}
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/multvsonedictfig-1.pdf}
\caption{\label{fig:multvsonedictfig}Comparing one Dict to Mult Dict}
\end{figure}
\hypertarget{comparison-1}{%
\subsection{Comparison}\label{comparison-1}}

Figure \ref{fig:dictionarytechniquecompfig} shows a comparison of all three techniques: Standard Dictionary, Multiple Standard Dictionaries, and Direct Mapped Dictionary. As shown, the Direct Map technique greatly increases throughput and thus decreases average compression time. Given these results, we decided to shift our focus onto the Direct Map and try to optimize this scheme as much as possible.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/dictionarytechniquecompfig-1.pdf}
\caption{\label{fig:dictionarytechniquecompfig}Comparing the three types of dictionaries. The Direct Map has a max length of 15 and the Mult Dict has a max length of 10.}
\end{figure}
\hypertarget{optimizing-direct-map-even-more}{%
\section{Optimizing Direct Map Even more}\label{optimizing-direct-map-even-more}}

Given that the Direct Map dictionary showed great performance improvements, we decided to narrow in on the scheme to see if there were ways to improve it even further.

\hypertarget{finding-the-longest-runs}{%
\subsection{Finding the Longest Runs}\label{finding-the-longest-runs}}

Our dictionary data structures all have a \texttt{get\_longest\_in\_dict} function. This function does the boring work of iterating through the input from the start, checking if each substring is in the dictionary.

Given the statistics of our corpora, we know that this process can be faster. Since most runs are above 6-7, we waste a lot of time by starting from the bottom.

Another strategy would be to start from the maximum string length of the dictionary, so 15. We can check if the next string of the max length is in the dictionary. If it is, we need to check strings longer than the max, so we can iterate up. If it isn't, we need to check strings shorter, so we can either iterate down . Here is some pseudocode that mimics this proposed algorithm.
\begin{verbatim}
find_longest(input_string){

    // calculate the index of the next 15 chars
    // where index = converting each char to two bits
    index_of_next_15_chars = calculate_index(input_string[0:15]);

    // look up our string
    lookup = dictionary[index_of_next_15_chars];

    if(lookup is in dictionary){
        loop_up();
    }
    else {
        loop_down();
    }
    
}
\end{verbatim}
Calculating the index, however, takes time. While we are looping up or down, we could just use the index of the next 15 characters as a starting point. If we are looping up, we can add on the next character's two bit representation as we loop. For instance, suppose our string has an index of \texttt{00101100}. If the next character is `A', we can simply tack the two bit representation for `A' to the end of our index, yielding \texttt{00101100\textbar{}00}.

Similarly, we can chop off two bits at a time while looping down. We can name this process looping ``on the fly'', since we are constructing our index on the fly rather than recalculating it every time. So our modified pseudocode would look like the code below:
\begin{verbatim}
find_longest(input_string){

    // calculate the index of the next 15 chars
    // where index = converting each char to two bits
    index_of_next_15_chars = calculate_index(input_string[0:15]);

    // look up our string
    lookup = dictionary[index_of_next_15_chars];

    if(lookup is in dictionary){
        loop_up_on_fly(index);
    }
    else {
        loop_down_on_fly(index);
    }
    
}
\end{verbatim}
Of course, there are theoretically quicker ways of iterating than looping up or down. We could use binary search.

Binary search is a searching technique for sorted lists, but we can use it in this scenario as well. Suppose we have a string of characters, and we are searching for the longest string already in the dictionary. The algorithm works by repeatedly dividing the search interval in half, comparing the middle element of the subarray to the target value, and then deciding whether to continue the search on the lower half or upper half of the subarray.

If the middle element is equal to the target value, the search ends and the position of the element is returned. If the middle element is greater than the target value, the search continues on the lower half of the subarray. Conversely, if the middle element is less than the target value, the search continues on the upper half of the subarray.

Binary search has a time complexity of O(log n), which makes it a very efficient algorithm for searching large sorted arrays. In our case, we search for where the string of length n is in the dictionary, but the string of length n+1 is not.
\begin{verbatim}
find_longest(input_string){

    // code omitted...

    if(lookup is in dictionary){
        loop_up_on_fly(index);
    }
    else {
        loop_down_binary_search(index);
    }
    
}
\end{verbatim}
Of course, we could also calculate the indexes for binary search on the fly. So we now have 5 different schemes of finding the longest run: looping up or down, looping up or down on the fly, binary search, binary search on the fly, and looping up from 0 like we were doing before. Figure \ref{fig:findlongestfrommaxfig} shows a comparison of these methods.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestfrommaxfig-1.pdf}
\caption{\label{fig:findlongestfrommaxfig}Comparing the different ways of finding the longest run}
\end{figure}
We see that calculating the index on the fly does improve performance. However, we can also see that none of the other strategies are better than just looping up from zero. This could be because most of the runs are very short, which we can see in \ref{fig:allrunshistfig}. This means that if we start looking from runs around length 10, we should see a performance improvement.

\hypertarget{finding-the-longest-from-the-average}{%
\subsection{Finding the Longest From The Average}\label{finding-the-longest-from-the-average}}

As we can see in Figure \ref{fig:findlongestfromavg}, looping or doing binary search from the average run length, which we approximate at 7, increases throughput and decreases mean compression time relative to looping up from zero.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestfromavg-1.pdf}
\caption{\label{fig:findlongestfromavg}Comparing the different ways of finding the longest run}
\end{figure}
As we predicted, this greatly improves performance. We are capitalizing on the fact that most runs are short. There is the occasional run that is very long, but from the run statistics we saw that the standard deviation for both corpora was pretty small. So on the edge cases in which there are very long runs, we could be wasting a lot of time.

\hypertarget{not-allowing-strings-over-max}{%
\subsection{Not Allowing strings over max}\label{not-allowing-strings-over-max}}

One way to avoid the edge case where we encounter a very long run is to just not allow strings in the dictionary longer than the max string length, in this case, 15. This means we won't have to deal with the overhead of the hashmap on top of our dictionary, but we will take a hit in compression ratio. Figure \ref{fig:findlongestwmax} summarizes the results of the different methods with the max length rule enforced. There isn't too much of a performance change, which makes sense because long runs are very rare.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestwmax-1.pdf}
\caption{\label{fig:findlongestwmax}Comparing the different ways of finding the longest run starting at average with strings over max not accepted}
\end{figure}
Of course, not allowing string over max length does mean that our compression ratio will change. Up until this point, all the different versions of the Direct Mapped Dictionary had the same compression ratio. Table \ref{tab:maxenforcedstats} shows the affect of enforcing the max string length on the total compression ratio over both corpora. As we can see, it is minimal, which makes sense because long runs are very rare.
\begin{table}

\caption{\label{tab:maxenforcedstats}Compression Ratio change from disallowing long strings}
\centering
\begin{tabular}[t]{rr}
\toprule
cr\_before & cr\_after\\
\midrule
2.909175 & 2.907686\\
\bottomrule
\end{tabular}
\end{table}
\hypertarget{using-pext}{%
\subsection{\texorpdfstring{Using \texttt{pext}}{Using pext}}\label{using-pext}}

One potential bottleneck of finding the longest run is converting a run of characters into an index. We can try to do it on the fly as we loop up or down, but we could also use machine instructions.

Our strategy is to use \texttt{pext}, which extracts bits in parallel, meaning at the same time. The \texttt{pext} instruction is a recent addition to the x86 istruction set, so it has not been widely used yet as optimization technique.

We give \texttt{pext} a string of characters, say `ACTG', and a bit mask, and it will extract those bits from our string. Figure \ref{fig:pext} details this process for a string of length 4.

It theoretically does this in one machine instruction, which could be much more efficient than looping over all the characters.
\begin{figure}[h]\centering


\usetikzlibrary{chains,decorations.pathreplacing}
 \begin{tikzpicture}[
node distance=0pt,
 start chain = A going right,
    X/.style = {rectangle, draw,% styles of nodes in string (chain)
                minimum width=2ex, minimum height=3ex,
                outer sep=0pt, on chain},
    B/.style = {decorate,
                decoration={brace, amplitude=5pt,
                pre=moveto,pre length=1pt,post=moveto,post length=1pt,
                raise=1mm,
                            #1}, % for mirroring of brace, if necessary
                thick},
    B/.default=mirror, % by default braces are mirrored
                        ]
\foreach \i in {0,1,0,0,0,0,0,1,
                0,1,0,0,0,0,1,1,
                0,1,0,0,0,1,1,1,
                0,1,0,1,0,1,0,0, }% <-- content of nodes
    \node[X] {\i};
\draw[B] ( A-6.south west) -- node[below=2mm] {Index of A} ( A-7.south east);
\draw[B] (A-14.south west) -- node[below=2mm] {Index of C} (A-15.south east);
\draw[B] (A-22.south west) -- node[below=2mm] {Index of G} (A-23.south east);
\draw[B] (A-30.south west) -- node[below=2mm] {Index of T} (A-31.south east);
\node (B1) [inner sep=1pt,above=of A-1.north west] {$\downarrow$};
\node (B2) [inner sep=1pt,above=of A-9.north west] {$\downarrow$};
\draw[B=](B1.north) -- node[above=2mm] {A in binary}(B2.north);
\node (B3) [inner sep=1pt,above=of A-9.north west] {$\downarrow$};
\node (B4) [inner sep=1pt,above=of A-17.north west] {$\downarrow$};
\draw[B=](B3.north) -- node[above=2mm] {C in binary}(B4.north);
\node (B5) [inner sep=1pt,above=of A-17.north west] {$\downarrow$};
\node (B6) [inner sep=1pt,above=of A-25.north west] {$\downarrow$};
\draw[B=](B5.north) -- node[above=2mm] {G in binary}(B6.north);
\node (B7) [inner sep=1pt,above=of A-25.north west] {$\downarrow$};
\node (B8) [inner sep=1pt,above=of A-33.north west] {$\downarrow$};
\draw[B=](B7.north) -- node[above=2mm] {T in binary}(B8.north);
    \end{tikzpicture}
\caption{How `pext` extracts bits}

\label{fig:pext}
\end{figure}
We can apply this technique to our \texttt{find\_longest} function: we can extract the index of a string using pext very quickly, then use that index rather than recalculating it every time. Figure \ref{fig:findlongestwpext} hows the results of this application for both looping and binary search. In both cases, using pext to extract the index of the string increases throughput and decreases average compression time.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestwpext-1.pdf}
\caption{\label{fig:findlongestwpext}Comparing the different ways of finding the longest run with pext}
\end{figure}
\hypertarget{returning-to-compression-ratio}{%
\section{Returning to Compression Ratio}\label{returning-to-compression-ratio}}

After spending a lot of time debugging, we returned our attenting to the compression ratio. Most of our optimizations didn't have much of an effect on compression ratio, with the exception of disallowing strings over a certain length. It is also worth noting that the Direct Map dictionary and the Std dictionary implementations will have different compression ratios, as the Direct map has a max codeword size of 16 bits while the Std dict has a max codeword size of 32 bits. Std dict also allows longer strings.

\hypertarget{a-point-of-comparison}{%
\subsection{A point of Comparison}\label{a-point-of-comparison}}

It's worth noting that for sequences of DNA, if the bases are encoded in ASCII, there is a simple and effecient algorithm to compress the file with a 4.0 compression ratio every time. Since there are only 4 bases, we can represent each base with 2 bits. This is a simple conversion, and it requires no dictionaries or codewords.

So the takeaway is, if a DNA compression algorithm can't compress with a compression ratio of higher than 4.0 (2 bits per base), than this simple algorithm would be perferable every time.

We implemented this method using pext, and the results are detailed in the next chapter.

\hypertarget{entropy-encoding}{%
\subsection{Entropy Encoding}\label{entropy-encoding}}

Our initial idea was that we would compress the DNA with LZW, then use a entropy encoder like arithmetic encoding or Huffman to further compress the data. This was an oversight, as the algorithm as we have described it thus far does not lend itself well to entropy encoding.

Our algorithm outputs codewords and two bits representing the next character. For a codeword size of 16 bits, this means each loop of our algorithm outputs 18 bits. So any repretitiveness or reuse will not be detectable by an entropy encoder which works on data of 8, 16, or 32 bit chunks.

So maybe we break the compressed file into two seperate files; a file of codewords and a file of the two bit representations of the following characters. Now we can compress these two files, right?

The issue is that entropy encoders rely on repeating numbers of a high frequency. We can cut down the number of bits a certain entry takes to make it more compressible, and make less common entries take more bits (see Figure \ref{fig:huffman}). DNA neucleotides show up with roughly the same frequency, so having 2 bits per character is hard to beat. So the file with all the charaters in it can't be compressed further.

What about the codewords? Well, a key realization is that any codeword will only show up 4 times in a single run of the program. Say we add the codeword \texttt{1234="ACT"} to our dictionary. When will we output this codeword ? Well, we will output it when we see ``ACT'' followed by another character. Since there are only 4 characters, once you see those 4 other strings (``ACTA'', ``ACTG'', ``ACTC'', and ``ACTT''), we will never output it again because we are looking for the longest run possible.

There are two cases in which we will output a codeword more than 4 times. If we run out of codewords and we have never encoded ``ACTG'', any time ``ACTG'' comes up, we will output \texttt{1234G}. This case is hard to predict as we have no way of controlling what strings are left when we run out of codewords. The other case is for strings that are the max length. These long runs may show up multiple times, and since we won't add any strings longer than the max to our dictionary, it will never be overwritten. However, these runs are very rare, or else the compression ratio wouldn't be an issue.

So on average, every codeword shows up a maximum of 4 times. This means that for long strands of DNA, we have output nearly all of the codewords with relatively even frequency. In other words, entropy encoding will not shorten the length of the codewords.

\hypertarget{a-new-approach}{%
\subsection{A New Approach}\label{a-new-approach}}

Given that we are not able to achieve a compression ratio over 4.0 with the current LZW, we need to alter our approach. The issue with our dataset is that long runs are rare. Every once in a while, you may replace a run of 15 with a codeword, but most of the time you are replacing a run less than 8. Eight characters can be represented by 16 bits, so any run under 8 that is replaced loses bits

The fact that we are not getting a compression ratio over 4.0 led us to think of a new twist on the algorithm specifically tailored for this situation. The scheme uses three streams of data: characters, codewords, and indicator bits. The algorithm works as follows.

Compression:
\begin{itemize}
\tightlist
\item
  if the next longest run is less than 8 characters, we add it to the dictionary, but rather than output the codeword, we just output the 2 bit representation of the characters. We output a 0 to the indicator stream to indicate this choice.
\item
  if the next longest run is equal to or greater than 8, we output a codeword to the codeword stream and the next character to the character stream. We also output a 1 to the indicator stream to indicate a codeword was output.
\end{itemize}
Decompression: We start by reading an indicator bit
\begin{itemize}
\tightlist
\item
  if the bit is 1, we read a codeword from the codeword stream and the next character from the character stream. We add this to the dictionary like the old algorithm.
\item
  if the bit is 0, we read the next 8 characters. We then use \texttt{find\_longest} to find the longest run and add it to the dictionary. We can then put the 8 characters into the output stream and move on
\end{itemize}
This is, in essence, a greedy algorithm. We have the choice of outputting a codeword or 8 characters every time, and we choose the choice which results in the least number of bits output.

The other advantage to this algorithm is that it works well for entropy encoding. The indicator bits are mostly 0, since most runs are less than 8. The codeword stream is also compressible, since none of the codewords for strings less than length 8 are output. The performance of this new scheme is assessed in the next chapter.

\hypertarget{comparison-to-other-tools}{%
\chapter{Comparison to other tools}\label{comparison-to-other-tools}}

Now that we have implemented several versions of LZW, we want to compare them to each other to see which would be preferable in different situations. We also want to compare the performance of these different implementations to the 4 to 1 direct translation and other professional compression tools.

\hypertarget{comparison-of-our-implementations}{%
\section{Comparison of our Implementations}\label{comparison-of-our-implementations}}

\hypertarget{compression-algorithms-in-literature}{%
\section{Compression Algorithms in Literature}\label{compression-algorithms-in-literature}}

As discussed in the related work section of chapter 1, there have been several other compression algorithms proposed and tested in the field tailored for DNA. Not all of these are publicly available, thus we can only compare to the numbers that they reported.

Note that I wasn't able to get compression times for these algorithms, only the compression ratios.
\begin{table}[!h]
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}{lrrr}
\toprule
Sequence & BioCompress2 & GenCompress & XM\\
\midrule
\cellcolor{gray!6}{CHMPXX} & \cellcolor{gray!6}{1.6848} & \cellcolor{gray!6}{1.6730} & \cellcolor{gray!6}{1.6577}\\
CHNTXX & 1.6172 & 1.6146 & 1.6068\\
\cellcolor{gray!6}{HEHCMVCG} & \cellcolor{gray!6}{1.8480} & \cellcolor{gray!6}{1.8470} & \cellcolor{gray!6}{1.8426}\\
HUMDYSTROP & 1.9262 & 1.9231 & 1.9031\\
\cellcolor{gray!6}{HUMGHCSA} & \cellcolor{gray!6}{1.3074} & \cellcolor{gray!6}{1.0969} & \cellcolor{gray!6}{0.9828}\\
HUMHBB & 1.8800 & 1.8204 & 1.7513\\
\addlinespace
\cellcolor{gray!6}{HUMHDAB} & \cellcolor{gray!6}{1.8770} & \cellcolor{gray!6}{1.8192} & \cellcolor{gray!6}{1.6671}\\
HUMHPRTB & 1.9066 & 1.8466 & 1.7361\\
\cellcolor{gray!6}{MPOMTCG} & \cellcolor{gray!6}{1.9378} & \cellcolor{gray!6}{1.9058} & \cellcolor{gray!6}{1.8768}\\
MTPACG & 1.8752 & 1.8624 & 1.8447\\
\cellcolor{gray!6}{VACCG} & \cellcolor{gray!6}{1.7614} & \cellcolor{gray!6}{1.7614} & \cellcolor{gray!6}{1.7649}\\
\bottomrule
\end{tabular}}
\end{table}
\hypertarget{comparison-to-other-professional-tools}{%
\section{Comparison to Other Professional Tools}\label{comparison-to-other-professional-tools}}

\hypertarget{conclusion}{%
\chapter*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{chapter}{Conclusion}

If we don't want Conclusion to have a chapter number next to it, we can add the \texttt{\{-\}} attribute.

\textbf{More info}

And here's some other random info: the first paragraph after a chapter title or section head \emph{shouldn't be} indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there.

\appendix

\hypertarget{the-first-appendix}{%
\chapter{The First Appendix}\label{the-first-appendix}}

This first appendix includes all of the R chunks of code that were hidden throughout the document (using the \texttt{include\ =\ FALSE} chunk tag) to help with readibility and/or setup.

\textbf{In the main Rmd file}
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# This chunk ensures that the thesisdown package is}
\CommentTok{\# installed and loaded. This thesisdown package includes}
\CommentTok{\# the template files for the thesis.}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(remotes)) \{}
  \ControlFlowTok{if}\NormalTok{ (params}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Install needed packages for \{thesisdown\}}\StringTok{\textasciigrave{}}\NormalTok{) \{}
    \FunctionTok{install.packages}\NormalTok{(}\StringTok{"remotes"}\NormalTok{, }\AttributeTok{repos =} \StringTok{"https://cran.rstudio.com"}\NormalTok{)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{stop}\NormalTok{(}
      \FunctionTok{paste}\NormalTok{(}\StringTok{\textquotesingle{}You need to run install.packages("remotes")",}
\StringTok{            "first in the Console.\textquotesingle{}}\NormalTok{)}
\NormalTok{    )}
\NormalTok{  \}}
\NormalTok{\}}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(thesisdown)) \{}
  \ControlFlowTok{if}\NormalTok{ (params}\SpecialCharTok{$}\StringTok{\textasciigrave{}}\AttributeTok{Install needed packages for \{thesisdown\}}\StringTok{\textasciigrave{}}\NormalTok{) \{}
\NormalTok{    remotes}\SpecialCharTok{::}\FunctionTok{install\_github}\NormalTok{(}\StringTok{"ismayc/thesisdown"}\NormalTok{)}
\NormalTok{  \} }\ControlFlowTok{else}\NormalTok{ \{}
    \FunctionTok{stop}\NormalTok{(}
      \FunctionTok{paste}\NormalTok{(}
        \StringTok{"You need to run"}\NormalTok{,}
        \StringTok{\textquotesingle{}remotes::install\_github("ismayc/thesisdown")\textquotesingle{}}\NormalTok{,}
        \StringTok{"first in the Console."}
\NormalTok{      )}
\NormalTok{    )}
\NormalTok{  \}}
\NormalTok{\}}
\FunctionTok{library}\NormalTok{(thesisdown)}
\ControlFlowTok{if}\NormalTok{ (}\SpecialCharTok{!}\FunctionTok{require}\NormalTok{(DiagrammeR)) \{}
  \FunctionTok{install.packages}\NormalTok{(}\StringTok{"DiagrammeR"}\NormalTok{)}
\NormalTok{\}}
\FunctionTok{library}\NormalTok{(DiagrammeR)}
\CommentTok{\# Set how wide the R output will go}
\FunctionTok{options}\NormalTok{(}\AttributeTok{width =} \DecValTok{70}\NormalTok{)}
\end{Highlighting}
\end{Shaded}
\textbf{In Chapter \ref{ref-labels}:}

\hypertarget{the-second-appendix-for-fun}{%
\chapter{The Second Appendix, for Fun}\label{the-second-appendix-for-fun}}

\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\noindent

\setlength{\parindent}{-0.20in}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-CaoXM}{}}%
Cao, D., Dix, T., Allison, L., \& Mears, C. (2007). A simple statistical algorithm for biological sequence compression. In \emph{In Proceedings of the Conference on Data Compression} (pp. 43--52). http://doi.org/\href{https://doi.org/10.1109/DCC.2007.7}{10.1109/DCC.2007.7}

\leavevmode\vadjust pre{\hypertarget{ref-Chen2001ACA}{}}%
Chen, X., Kwong, S. T. W., \& Li, M. (2001). A compression algorithm for DNA sequences. \emph{IEEE Engineering in Medicine and Biology Magazine}, \emph{20}, 61--66.

\leavevmode\vadjust pre{\hypertarget{ref-grumbach}{}}%
Grumbach, S., \& Tahi, F. (1994). {A New Challenge for Compression Algorithms: Genetic Sequences}. \emph{{Information Processing and Management}}, \emph{30}. Retrieved from \url{https://hal.inria.fr/inria-00180949}

\leavevmode\vadjust pre{\hypertarget{ref-ibrahimgbolagade}{}}%
Ibrahim, M., \& Gbolagade, K. (2020). Enhancing computational time of lempel-ziv-welch-based text compression with chinese remainder theorem. \emph{Journal of Computer Science and Its Application}, \emph{27}. http://doi.org/\href{https://doi.org/10.4314/jcsia.v27i1.9}{10.4314/jcsia.v27i1.9}

\leavevmode\vadjust pre{\hypertarget{ref-KeerthyMID}{}}%
Keerthy, P. (2019). Genomic sequence data compression using lempel-ziv-welch algorithm with indexed multiple dictionary. \emph{International Journal of Engineering and Advanced Technology}.

\leavevmode\vadjust pre{\hypertarget{ref-panialok}{}}%
Pani, A., Mishra, M., \& Mishra, T. (2012). Parallel lempel-ziv-welch (PLZW) technique for data compression. \emph{International Journal of Computer Science and Information Technology}, \emph{3}, 4038--4040.

\leavevmode\vadjust pre{\hypertarget{ref-prataspinho}{}}%
Pratas, D., \& Pinho, A. (2018). A DNA sequence corpus for compression benchmark. In (pp. 208--215). http://doi.org/\href{https://doi.org/10.1007/978-3-319-98702-6_25}{10.1007/978-3-319-98702-6\_25}

\end{CSLReferences}

% Index?

\end{document}
