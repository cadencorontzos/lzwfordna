```{r includepackages2, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if (!require(remotes)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("remotes", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("remotes")',
        "first in the Console."
      )
    )
  }
}
if (!require(dplyr)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("dplyr", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("dplyr")',
        "first in the Console."
      )
    )
  }
}
if (!require(ggplot2)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("ggplot2", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("ggplot2")',
        "first in the Console."
      )
    )
  }
}
if (!require(bookdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("bookdown", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("bookdown")',
        "first in the Console."
      )
    )
  }
}

if (!require(patchwork)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("patchwork", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("patchwork")',
        "first in the Console."
      )
    )
  }
}

if (!require(thesisdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    remotes::install_github("ismayc/thesisdown")
  } else {
    stop(
      paste(
        "You need to run",
        'remotes::install_github("ismayc/thesisdown")',
        "first in the Console."
      )
    )
  }
}
library(thesisdown)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(patchwork)
```


```{r harmonicmeanfunc, echo = F}
harmonic_mean <- function(corpus){
  num_files <-length(unique(corpus$File.Name))
  means <- corpus %>% 
    mutate(
      comp_recip = 1/max(Compression.Time,1),
      decomp_recip = 1/max(Decompression.Time,1)
    ) %>% 
    summarize(
      harmonic_mean_comp= num_files/sum(comp_recip),
      harmonic_mean_decomp= num_files/sum(decomp_recip)
    )
  return(means)
}

```

```{r csvtotablefunc, echo = F}
csv_to_table <- function(filename, caption = "f"){
df <- read.csv(filename)
knitr::kable(df, "markdown", align= 'c', caption = caption)
}
```

```{r statscsvfunc, echo = F}
stats_csv <- function(filename, caption = "Foo"){
  df <- read.csv(filename)
  knitr::kable(df, format="latex", booktabs=TRUE, caption = caption) %>% 
  kable_styling(latex_options=c("scale_down", "hold_position", "striped"))
}
```

```{r comparecorporafunc, echo = F}
compare_coprora <- function(array, per_corpus = FALSE){
	df <- data.frame()
  ord <- 1
	for(corpus_pair in array){
		cp1 <- read.csv(corpus_pair[1])
		cp2 <- read.csv(corpus_pair[2])
				cbind(corpus = 1, cp1)
		cp1["corpus"] <- "Corpus 1"
		cp2["corpus"] <- "Corpus 2"
		comb <- bind_rows(cp1, cp2)

		means <- harmonic_mean(comb)

		comb<- comb %>% summarize(
		  comp_throughput = sum(Original.File.Size)/sum(Compression.Time),
		  hm_comp = means$harmonic_mean_comp,
		  description = corpus_pair[3],
		  order = ord,
		  corpus = corpus
		)
		ord<- ord+1
		df <- bind_rows(comb, df)
	}

  plot1<- df %>% 
    ggplot(aes(x = reorder(description, order), y = comp_throughput, fill = description))+
    geom_col(show.legend = F)+
    {if(per_corpus)facet_wrap(~corpus)}+
    labs(x="Milestone", y = "Throughput(bytes/ms)", title = "Throughput")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  plot2 <- df %>% 
    ggplot(aes(x = reorder(description, order), y = hm_comp, fill = description))+
    geom_col(show.legend = F)+
    {if(per_corpus)facet_wrap(~corpus)}+
    labs(x="Milestone", y = "Mean Compression Time (ms)", title = "Average Compression Time")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  plot1/plot2
}
```

```{r comparemaxstringlengthsfunc, echo = F}
compare_max_string_lengths <- function(array){
	df <- data.frame()
	for(corpus_pair in array){
		cp1 <- read.csv(corpus_pair[1])
		cp2 <- read.csv(corpus_pair[2])
		comb <- bind_rows(cp1, cp2)
		means <- harmonic_mean(comb)

		comb<- comb %>% summarize(
		  comp_throughput = sum(Original.File.Size)/sum(Compression.Time),
		  hm_comp = means$harmonic_mean_comp,
		  size = corpus_pair[3],
		)
		df <- bind_rows(comb, df)
	}
  plot1<- df %>% 
    ggplot(aes(x = size, y = comp_throughput, fill = size))+
    geom_col()+
    labs(x="Max String Length", y = "Throughput(bytes/ms)")
  plot2 <- df %>% 
    ggplot(aes(x = size, y = hm_comp, fill = size))+
    geom_col()+
    labs(x="Max String Length", y = "Mean Compression Time (ms)")	
  plot1/plot2
}

```

# Optimizing LZW: Approach

To restate the goal of this thesis, we seek to optimize LZW for use in compression of DNA. I chose to write in C++.

**TODO**: Elaborate here

## Supporting Research

There has been several attempts to optimize LZW by computer science researchers. One paper made use of multiple indexed dictionaries in order to speed up the compression process [@KeerthyMID]. The concept is simple, rather than a single large dictionary, have multiple dictionaries, one for each possible string size. That way, the dictionaries grow more slowly and accesses are faster. This paper also used Genomic data to gather their metrics and compared their algorithm to other popular DNA compression techniques, which makes it particularly relevant for this paper.

Another paper used simple parallelization techniques to improve compression speed [@panialok]. Rather than compressing the whole file linearly, the researches broke the file into portions and compressed them with LZW in parallel, which greatly increased the compression speed at the cost of a reduced compression ratio.

Yet another paper made use of Chinese Remainder Theorem to augment Lempel Ziv Welch [@ibrahimgbolagade]. They saw great reduction in compression time without comprimising compression ratio, although these results could not be verified. The details of their implementation were not clear from the paper. We tried multiple different methods of utilizing CRT given the pseudocode in their paper, but we could not get anything that looked like it may improve compression time. We reached out to the authors of the paper, but we were not able to further our progress on this method and thus the method is not used in this thesis.

## Corpora

Most compression papers make use of a Corpus, which is a collection of files to run a compression algorithm on in order to evaluate performance and to compare different algorithms to one another. 

In the world of DNA compression, there are several academic papers on the subject. One of the first and most popular of the papers was published in 1994, and the selection of DNA sequences used in the paper have become an informal corpus for the subject of DNA compression, cited by more than thirty publications [@grumbach].

```{r corpus1filesfig, warnings= F, messages = F, fig.cap = "Corpus 1", fig.height = 7, echo = FALSE} 
csv_to_table('data/corpus_1_summary.csv', "Corpus 1")
```

Another, newer paper aimed to create a corpus specifically for compressing DNA [@prataspinho]. They put together a corpus of DNA sequences for this purpose, as summarized below. Since the papers publishing, it has been cited by several DNA compression papers.

```{r corpus2filesfig, warnings= F, messages = F, fig.cap = "Corpus 2", fig.height = 7, echo = FALSE}
csv_to_table('data/corpus_2_summary.csv', "Corpus 2")
```
This particular dataset is publicly available at this [link](https://tinyurl.com/DNAcorpus).

## Evaluating Performance

Evaluating performance of a program is difficult. There is a notion of theoretical run time, but on an actual computer there are many processes running in the background, so it can be hard to get a consistent reading on performance.

To attempt to counteract this, we ran the function on the same file multiple times, and took the median of the compression and decompression times for all the runs.

## A Starting Point

As a starting point, we thought it was best to get a working implementation of LZW in C++ on regular text files, optimize it as much as we could, and then try variations from there, optimizing it for DNA. We want to try various techniques tried by researches in the field, but it is important to have a fast baseline from which we can compare and improve upon.


### Growing Codewords and Bit Output

When reading files on the computer, most characters are stored as bytes, which is made up of 8 bits. For instance `01000001` stands for the letter 'A' in ASCII encoding. Numbers are more simple to display, so `00000001` is 1, `00000010` is 2, and so on.

But if we are translating numbers to binary, we don't need all of the bits in a byte. In binary, `1` is the same as `01` is the same as `00000000000001`. So when we are outputting codewords for LZW, we don't necessarily need to output a whole byte. We can have growing codewords.

As the number of codewords grows, the number of bits needed to represent it also grows. So if we are on codeword 8, we need 4 bits since 8 is `1000`. As our dictionary grows, we can grow the number of bits needed to display a codeword and save a lot of space in our compressed document.

So we needed a method of outputting bits one by one, and reading in bits one by one. This is not something that is supported in C++ on its own. We were able to create this functionality by defining a class.

```c++
// BitInput: Read a single bit at a time from an input stream.
// Before reading any bits, ensure your input stream still has valid inputs.
class BitInput {
 public:
  // Construct with an input stream
  BitInput(const char* input);

  BitInput(const BitInput&) = default;
  BitInput(BitInput&&) = default;

  // bool eof();
  // Read a single bit (or trailing zero)
  // Allowed to crash or throw an exception if called past end-of-file.
  bool input_bit();

  int read_n_bits(int n);
}

// BitOutput: Write a single bit at a time to an output stream
// Make sure all bits are written out by the time the destructor is done.
class BitOutput {
 public:
  // Construct with an input stream
  BitOutput(std::ostream& os);

  // Flushes out any remaining output bits and trailing zeros, if any:
  ~BitOutput();

  BitOutput(const BitOutput&) = default;
  BitOutput(BitOutput&&) = default;

  // Output a single bit (buffered)
  void output_bit(bool bit);

  void output_n_bits(int bits, int n);
}

```
So when we are encoding and need to output a codeword, we can `output_n_bits`, where `n` is the number of bits needed to display our greatest codeword. When decoding, we can just `read_n_bits`.

### Getting EOF to work

One of the very early issues with the implementation was how to denote the end of a file. The early implementation would work for some files, but for others the very last part of the file would be lost after encoding and then decoding.

In theoretical implementations of LZW, computer scientists tend to denote the end of a message with a special character, one that isn't seen anywhere else in the file. In this initial implementation, that wasn't possible because we wanted to be able to compress any file with any characters.

The solution was to reserve a codeword to mark the end of the file. So we start with a starting dictionary containing all ASCII characters.

```c++
	std::unordered_map<std::string, int> dictionary;
   	for (int i = 0; i < 256; ++i){
		std::string str1(1, char(i));
		dictionary[str1] = i;
	}
```
Then use the code 256 to denote the end of file. So the algorithm goes along reading a file. It builds up a current string character by character, adding the character to the string and checking if it has seen that sequence before. Once it find the end of file, we stop and output the EOF codeword. 


The problem was, what about what is left over? Suppose we are reading a file, and the file ends with "ACCT". If "A" is in the dictionary, we see if "AC" is in the dictionary, and so on. This leaves us with three possible cases when we reached the end of the file

1. "ACC" was in the dictionary but "ACCT" was not. This means we can output the codeword for "ACC", follow it by the character "T", and we are done. This is the ideal scenario, because nothing is left over when we output the EOF codeword
2. "ACCT" was in the dictionary: This means we have one more codeword to output, but since we reached the end of the file, we never got to output it.
3. "AC" was in the dictionary, but "ACC" was not: in this case, we would output the codeword for "AC" output the character "C", and then start looping again starting at "T". But we reach the end of the file, so we output EOF before outputting T.

We solved this issue by adding 2 extra bits after the EOF codeword. These bits denote the case that occurred

```c++
    // after we've encoded, we either have 
    // no current block (case 0)
    // we have a current block that is a single character (case 1)
    // otherwise we have a current block > 1 byte (default)
    switch (currentBlock.length()){
    case 0:
        bit_output.output_bit(false);
        bit_output.output_bit(false);
        break;
    case 1:
        bit_output.output_bit(false);
        bit_output.output_bit(true);
        bit_output.output_n_bits((int) currentBlock[0], CHAR_BIT);
        break;
    default:
        bit_output.output_bit(true);
        bit_output.output_bit(true);

        int code = dictionary[currentBlock];
        bit_output.output_n_bits(code, codeword_size);
        break;
    }
```
So when the decoded is reading and encounters the EOF codeword, it can look at the next two bits to see if anything is left over.

At this point, there was a working implementation that was able to compress and decompress files. Here is the performance of this version on the two copora. 
<!---
9066eef
-->
```{r fixedeofstatsfig, warnings= F, messages = F, fig.cap = "Stats for the corpora after fixing EOF.", fig.height = 7,  echo = F}
stats_csv('data/stats/corpus_1_stats_fixedeof_9066eef.csv', caption = " Corpus 1 Stats after fixing EOF")
stats_csv('data/stats/corpus_2_stats_fixedeof_9066eef.csv', caption = "Corpus 2 stats after fixing EOF")
```
### Using Constants
<!--
2c3f782
-->
The early version of the code was not clean. There were hard coded variables, unspecified integer types, and generally messy naming conventions that made the code difficult to read and debug.

The next major step in the code was to start using constants for everything, including

- `STARTING_CODEWORD`: What codeword we should start at
- `EOF_CODEWORD`: What we should output when we reach end of file
- `STARTING_DICT_SIZE`: At this stage, we had a starting dict size of 256 to hold all possible bytes, but later we will specialize for DNA

It also made sense to start using a specific type for codewords. At this stage, we opted for a 32 bit integer.

There are several tools at a developers disposal when looking to debug and optimize code. One tool used for this thesis was `callgrind` which is a tool of `valgrind` a profiling tool. Profiling tools are used to look at how your code works, where the bottlenecks are, and what can be changed/improved for the performance of your code.

Callgrind in particular is a profiling tool which associates assembly instructions to lines of code, indicating to the programmer which lines take a lot of instructions and which take less. For those unfamiliar, assembly instructions are what code is turned into so that it can be ran on your computer's processor. In general, more instructions means that code takes longer to run.

The callgrind output drew attention to one particular part of the code. A C++ `unordered_map` uses iterators, basically pointers into the dictionary. If an entry is not present in the dictionary, the `find()` function will return a iterator to the end of the dictionary. 

The check for this in our algorithm looked like this.

```c++
        // if we've already seen the sequence, keep going
        // TODO: use cend() and save this iterator
        if (dictionary.find(currentBlock + next_character) != dictionary.end()){
            currentBlock = currentBlock + next_character;
        }
```

Here is the callgrind output for that line.

```
105,030,135 ( 0.18%)          if (dictionary.find(currentBlock + next_character) != dictionary.end()){
13,537,450,317 (22.83%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char) (3,890,005x)
11,653,779,430 (19.65%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::find(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (3,890,005x)
2,108,383,120 ( 3.56%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string() (3,890,005x)
956,941,242 ( 1.61%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::end() (3,890,005x)
241,180,314 ( 0.41%)  => /usr/include/c++/9/bits/hashtable_policy.h:bool std::__detail::operator!=<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true>(std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&, std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&) (3,890,005x)
```
As shown, this line is taking a significant amount of instructions, and it needs to pull the end() of the dictionary each time it is ran. If we use cend() instead and save that iterator in a variable called end, we can save a significant amount of instructions.

```
89,470,115 ( 0.61%)          if (dictionary.find(currentBlock + next_character) != end ){
3,353,009,053 (22.78%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char) (3,890,005x)
2,833,786,025 (19.26%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::find(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (3,890,005x)
420,120,704 ( 2.85%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string() (3,890,005x)
50,570,065 ( 0.34%)  => /usr/include/c++/9/bits/hashtable_policy.h:bool std::__detail::operator!=<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true>(std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&, std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&) (3,890,005x)
```

Of course, there are several issues with this method. It is difficult to associate instructions with a single line of code. Some lines are interdependent, and assembly often behaves differently than the code that produces it. Another thing is that compliers are very advanced, and sometimes small optimizations like this are done by the compiler automatically. 

Despite these issues, this change was still worth making, if not to save time then for sake of clarity and readability of the code. Also, despite the inaccuracy of callgrind, like many profiling tools, its job is not necessarily to provide exact measurements of code performance, but to give indications to trouble spots which can be improved. 

Here are the runs after this optimization.

```{r usingconstantsstatsfig, warnings= F, messages = F, fig.cap = "Stats for the corproa after switching some constants", fig.height = 7, echo = F}
stats_csv('data/stats/corpus_1_usingconstants_2c3f782.csv', caption = "Corpus 1 Stats after changing constants")
stats_csv('data/stats/corpus_2_usingconstants_2c3f782.csv', caption = "Corpus 2 Stats after changing constants")
```

### Extraneous String Concatenations

<!--
8963f64ed59983edc15ed255b894ee666f2e1176
-->

The LZW algorithm is build on iteration: we go through each character, adding it to our current block. If we've seen that current block before, we keep going. If not, we add that block to the dictionary and start over. 

Another thing that I noticed from the callgrind output was that a lot of time/instructions are being spent on string concatenation. In general, string concatenation in most language, including C++, have a lot of overhead. A lot of implementations involve creating a new string every time you concatenate two existing string, which can have a significant performance penalty.

In the version of the algorithm at the time, every time we have already seen a sequence, we have to concatenate a character. I noticed that I was doing this concatenation multiple times without needing to.

```c++
		// we concatenate the strings here
        if (dictionary.find(currentBlock + next_character) != end ){
			// and here
            currentBlock = currentBlock + next_character;
        }
        else{

            // other code here ommitted


			// and here! 
            dictionary[currentBlock + next_character] = codeword;
        }
```

If I just concatenate them and save the output into a new string, that will save me from doing the concatenation 2 more times.

```c++
	// save concatenation here
      std::string string_seen_plus_new_char = current_string_seen + next_character;
        if (dictionary.find(string_seen_plus_new_char) != end ){
            current_string_seen = string_seen_plus_new_char;
        }
        else{

	// other code omitted here

            dictionary[string_seen_plus_new_char] = codeword;
        }
```

Here are the statistics on the version of the algorithm after this change.

```{r stringconcatstatsfig, warnings= F, messages = F, fig.cap = "Stats after eliminating excess string concatenations", fig.height = 7, echo = F}

stats_csv('data/stats/corpus_1_stringconcat_8963f64.csv', caption = "Corpus 1 Stats after reducing string concatenations")
stats_csv('data/stats/corpus_2_stringconcat_8963f64.csv', caption = "Corpus 2 Stats after reducing string concatenations")
```

### Dictionary Accesses
<!--
34bf4e9a6233137b388323f810791687c2a0f823
-->
 
Dictionary accesses can be expensive, especially with the standard library. We learned from `callgrind` that along with string operations, our program spent a lot of time doing dictionary accesses. 

We looked for ways to reduce the number of lookups. At the time, the way the algorithm worked was that it looked up the current string and the next character in the dictionary. If that string is in the dictionary, we keep going. If not, we output the codeword for the current string.

But, the current string on this iteration is the current string from the last iteration, plus one character. So when we were on the previous iteration of the loop, we could save that lookup and prevent a second lookup.

See the code below

```c++
    while(next_character != EOF){

		// code omitted


        // if we've already seen the sequence, keep going
        std::string string_seen_plus_new_char = current_string_seen + next_character;
		// save this iterator`
        if (dictionary.find(string_seen_plus_new_char) != not_in_dictionary ){
            current_string_seen = string_seen_plus_new_char;
        }
        else{

            // shouldn't look up again
            int code = dictionary[current_string_seen];


			// code omitted
        }
        next_character = input.get();
    }
```

We can save that lookup, like so.

```c++

    while(next_character != EOF){

		// code ommitted

        // if we've already seen the sequence, keep going
        std::string string_seen_plus_new_char = current_string_seen + next_character;
		codeword_seen_now = dictionary.find(string_seen_plus_new_char);
        if (codeword_seen_now != not_in_dictionary ){
            current_string_seen = string_seen_plus_new_char;
			codeword_seen_previously = codeword_seen_now; // save codeword here
        }
        else{

            // lookup the current block in the dictionary and output it, along with the new character

            int code = codeword_seen_previously->second; // on the next iteration, we use it here

			// code omitted

        }
        next_character = input.get();
    }
```
Here are the stats of the version of the algorithm after this change.
```{r dictaccessesstatsfig, warnings= F, messages = F, fig.cap = "Stats after reducing dictionary accesses", fig.height = 7, echo = F}
stats_csv('data/stats/corpus_1_dictaccesses_34bf4e9.csv', caption = "Corpus 1 Stats after reducing dictionary accesses")
stats_csv('data/stats/corpus_2_dictaccesses_34bf4e9.csv', caption = "Corpus 2 Stats after reducing dictionary accesses")
```
### Using Const Char *
<!--
a12a2901d87d723223b21a9c9115c1c9787a8966
-->

The algorithm works by reading through the entire file, so we know that at some point, we will need to see every byte of the entire file.

When reading a byte stream of the file, the file may not be in memory the way we want it. The `ifstream` class in C++ also has many extraneous feature that we don't need. If we map the file directly into memory using `mmap` and pass around a pointer to that data, it will simplify and speed up the scanning process. Also, using a `char*` opens the possibility to getting rid of `std::string` entirely, which means way less overhead and decreased compression time.


**TODO**: Need to talk about getting rid of string concatenation at some point.

```{r charstarstatsfig, warnings= F, messages = F, fig.cap = "Stats after using character pointer", fig.height = 7, echo = F}
stats_csv('data/stats/corpus_1_charstar_a12a290.csv', caption = "Corpus 1 Stats after using character pointers")
stats_csv('data/stats/corpus_2_charstar_a12a290.csv', caption = "Corpus 2 Stats after using character pointers")
```
```{r comparecorporaoptimizingfig, warnings= F, messages = F, fig.cap = "Comparison of the performance of the different milestones", fig.height = 7, echo = F}
compare_coprora(list(
  c('data/stats/corpus_1_stats_fixedeof_9066eef.csv', 'data/stats/corpus_2_stats_fixedeof_9066eef.csv', 'Fixed EOF'),
  c('data/stats/corpus_1_usingconstants_2c3f782.csv', 'data/stats/corpus_2_usingconstants_2c3f782.csv', 'Using constants'),
  c('data/stats/corpus_1_stringconcat_8963f64.csv', 'data/stats/corpus_2_stringconcat_8963f64.csv', 'String concatenation'),
  c('data/stats/corpus_1_dictaccesses_34bf4e9.csv', 'data/stats/corpus_2_dictaccesses_34bf4e9.csv', 'Excess dictionary calls'),
  c('data/stats/corpus_1_charstar_a12a290.csv', 'data/stats/corpus_2_charstar_a12a290.csv', 'Character pointer')))
```

## Trying Different Dictionaries

A lot of the stress of the LZW algorithm is on the dictionary. We are constantly looking strings up and placing others. Because of the reliance on this data structure, we know that the dictionary accesses and lookups are a bottleneck, so improvements in those areas could greatly increase the efficiency of our program. 

So the another step towards an efficient LZW seemed to be to abstract out the `std_map` and have multiple different dictionary implementations to try and experiment with in our attempt to optimize LZW for DNA compression.

### Direct Map

In our analysis of the two corpora, we found some interesting statistics in the redundancy of the data.

Below is a table which shows stats on the runs lengths of the "runs" of data, where a run is a string added to the dictionary during a run of the LZW algorithm.

```{r runstatsfig, warnings= F, messages = F, fig.cap = "The counts for runs of each length", fig.height = 7, echo = F}
stats_csv('data/corpus_1_run_stats.csv', caption = "Run Lengths for Corpus 1")
stats_csv('data/corpus_2_run_stats.csv', caption = "Run Lengths for Corpus 2")
```
And here is the combined stats of both copora, along with a histogram of all the runs.

```{r bothrunsfig, warnings= F, messages = F, fig.cap =  "Run counts for both corpora", fig.height = 7, echo= F}
stats_csv('data/corpus_both_run_stats.csv', caption = "Run lengths for both corpora")
```

```{r allrunshistfig, warnings= F, messages = F, fig.cap = "A histogram showing the lengths of runs for both copora.", fig.height = 7, echo = F}
all_runs <- read.csv('data/all_corpus_run_counts.csv')
all_runs %>% 
  ggplot(aes(x=Run.Length, y=count))+
  geom_bar(stat="identity")+
  labs(title="Run Distribution for Corpora", x = "Run Length")
```

Given this data, it is clear that it would be advantageous to speed up the dictionary for smaller run sizes, since most of the runs are below size 20.

To achieve this, we opted for a unique approach. Rather than use a hashmap, we can map the strings directly into memory. Since all of the strings only contain four characters ('A', 'C', 'T', and 'G'), we can represent the characters with two bits. So for a length `n` string, we can represent it with `2n` bits. 

So we can create an indexed dictionary directly in memory for all strings below a certain length. We can use the `2n` bit representation of the string
to index into an array of codewords. 

For each strings size 1 to n , we have an array with enough slots for every possible string. For example, for strings of length 3, we have an array of size $4^3$, since there are $4^3$ possible strings. In each of those $4^3$ slots, we have space for a codeword. All strings of length 3 can be represented by 6 bits, and since 6 bits can represent $2^6=4^3$ values, we can use the bit representation to index into the dictionary. If the codeword at that place in the dictionary is 0, we have never seen it before. If it is non-zero, we have found the codeword for that string. For all strings greater than `n`, we can just use a hashmap on top to handle those.

Here is the data for this version of the dictionary of length 10.

```{r directmaplength10statsfig, warnings= F, messages = F, fig.cap = "Stats for direct map with max length of 10", fig.height = 7, echo = F}
stats_csv('data/stats/directmap/corpus_1_directmap_154fed4_len10.csv', caption = "Corpus 1 Stats for Direct Map of max length 10")
stats_csv('data/stats/directmap/corpus_2_directmap_154fed4_len10.csv', caption = "Corpus 2 Stats for Direct Map of max length 10")
```

Here is a comparison for different max string lengths


```{r directmaplenthcompfig, warnings= F, messages = F, fig.cap = "Comparing different max lengths for Direct Map", fig.height = 7, echo = F}
compare_max_string_lengths(list(
	c('data/stats/directmap/corpus_1_directmap_154fed4_len10.csv', 'data/stats/directmap/corpus_2_directmap_154fed4_len10.csv', 10),
	c('data/stats/directmap/corpus_1_directmap_266bcb7_len15.csv', 'data/stats/directmap/corpus_2_directmap_266bcb7_len15.csv', 15)
))
```

**TODO**: Have data for multiple different direct map sizes. Also talk about the fixed codeword size and what happens when we run out.

### Multiple Indexed Dictionaries

As stated before, there have been research papers about the 

Similar to the Direct Mapped approach, we use dictionaries for each string size up to a certain size `n` , and for all strings of length greater than `n`, we use a regular dictionary. As with the direct map dictionary, we need to specify a max string length. We collected metrics for different choices of max string length. Here are the results for this dictionary. 

```{r multdictlength10statsfig, warnings= F, messages = F, fig.cap = "Comparing different max lengths for Multiple Dictionaries", fig.height = 7, echo = F}
stats_csv('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', caption = "Corpus 1 Stats for Multiple Dictionaries of max length 10")
stats_csv('data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', caption = "Corpus 1 Stats for Multiple Dictionaries of max length 10")
```
\pagebreak
```{r multdictlengthcompfig, warnings= F, messages = F, fig.cap = "Comparing different max lengths for Direct Map", fig.height = 7, echo = F}
compare_max_string_lengths(list(
	c('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', 'data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', 10),
	c('data/stats/multdict/corpus_1_multdict_6d73501_len15.csv', 'data/stats/multdict/corpus_2_multdict_6d73501_len15.csv', 15),
	c('data/stats/multdict/corpus_1_multdict_80d2016_len20.csv', 'data/stats/multdict/corpus_2_multdict_80d2016_len20.csv', 20),
	c('data/stats/multdict/corpus_1_multdict_06f9a20_len25.csv', 'data/stats/multdict/corpus_2_multdict_06f9a20_len25.csv', 25)
))
```
Here is a comparison of the Multiple Dictionaries versus one standard dictionary.

```{r multvsonedictfig, warnings= F, messages = F, fig.cap = "Comparing one Dict to Mult Dict", fig.height = 7, echo = F}

compare_coprora(list(
  c('data/stats/corpus_1_stddict_68bea3d.csv', 'data/stats/corpus_2_stddict_68bea3d.csv', 'Standard Dictionary'),
  c('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', 'data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', 'Multiple Indexed Dictionaries')
  ))

```

### Comparison

Here is a comparison of all three techniques: Standard Dictionary, Multiple Standard Dictionaries, and Direct Mapped Dictionary.

```{r dictionarytechniquecompfig, warnings= F, messages = F, fig.cap = "Comparing the three types of dictionaries. The Direct Map has a max length of 15 and the Mult Dict has a max length of 10.",echo = F, fig.height=7}

compare_coprora(list(
  c('data/stats/corpus_1_stddict_68bea3d.csv', 'data/stats/corpus_2_stddict_68bea3d.csv', 'Standard Dictionary'),
  c('data/stats/directmap/corpus_1_directmap_266bcb7_len15.csv', 'data/stats/directmap/corpus_2_directmap_266bcb7_len15.csv', 'Direct Map Dictionary'),
  c('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', 'data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', 'Multiple Indexed Dictionaries')
  ))

```

## Optimizing Direct Map Even more

Since the Direct map dictionary was a standout in performance, we decided to focus on optimizing it even further.

### Get Longest
Our dictionary data structures all have a `get_longest_in_dict` function. This function does the boring work of iterating through the input from the start, checking if each substring is in the dictionary. 

Given the statistics of our corpora, we know that this process can be faster. Since most runs are above 6-7, we waste a lot of time by starting from the bottom. 

Another strategy would be to start from the maximum string length of the dictionary. We can check if the next string of the max length in the dictionary. If it is, we need to check strings longer than the max, so we can iterate up. If it isn't, we need to check strings shorter, so we can either iterate down or binary search down from the max. Here is a table showing the performance of these different strategies. 

```{r findlongestfrommaxfig, warnings= F, messages = F, fig.cap = "Comparing the different ways of finding the longest run", echo = F, fig.height = 7}
compare_coprora(list(
  c('data/stats/ddict/corpus_1_ddictbinarysearch_e196a55.csv', 'data/stats/ddict/corpus_2_ddictbinarysearch_e196a55.csv', 'Binary search'),
  c('data/stats/ddict/corpus_1_ddictbinarysearchflyindex_94305a5.csv', 'data/stats/ddict/corpus_2_ddictbinarysearchflyindex_94305a5.csv', 'Binary Fly Index'),
  c('data/stats/ddict/corpus_1_ddictloopdownflyindex_7ae114e.csv', 'data/stats/ddict/corpus_2_ddictloopdownflyindex_7ae114e.csv', 'Loop Down Fly Index'),
  c('data/stats/ddict/corpus_1_ddictloopdown_2d0c8b8.csv', 'data/stats/ddict/corpus_2_ddictloopdown_2d0c8b8.csv', 'Loop Down'),
  c('data/stats/directmap/corpus_1_directmap_266bcb7_len15.csv', 'data/stats/directmap/corpus_2_directmap_266bcb7_len15.csv', 'Direct Map Dictionary')
), TRUE)


```


```{r findlongestfromavg, warnings= F, messages = F, fig.cap = "Comparing the different ways of finding the longest run starting at the average", echo = F, fig.height = 7}
compare_coprora(list(
  c('data/stats/ddict/corpus_1_ddictbsavg_a482e6d.csv', 'data/stats/ddict/corpus_2_ddictdownavg_edafb16.csv', 'Loop Down'),
  c('data/stats/ddict/corpus_1_ddictbsavg_a482e6d.csv', 'data/stats/ddict/corpus_2_ddictbsavg_a482e6d.csv', 'Binary search'),
  c('data/stats/ddict/corpus_1_ogddict_266bcb7.csv', 'data/stats/ddict/corpus_2_ogddict_266bcb7.csv', 'Original DDict')
), TRUE)


```
