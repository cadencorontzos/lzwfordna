```{r includepackages2, include = FALSE}
# This chunk ensures that the thesisdown package is
# installed and loaded. This thesisdown package includes
# the template files for the thesis and also two functions
# used for labeling and referencing
if (!require(remotes)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("remotes", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("remotes")',
        "first in the Console."
      )
    )
  }
}
if (!require(dplyr)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("dplyr", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("dplyr")',
        "first in the Console."
      )
    )
  }
}
if (!require(ggplot2)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("ggplot2", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("ggplot2")',
        "first in the Console."
      )
    )
  }
}
if (!require(bookdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("bookdown", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("bookdown")',
        "first in the Console."
      )
    )
  }
}

if (!require(patchwork)) {
  if (params$`Install needed packages for {thesisdown}`) {
    install.packages("patchwork", repos = "https://cran.rstudio.com")
  } else {
    stop(
      paste(
        'You need to run install.packages("patchwork")',
        "first in the Console."
      )
    )
  }
}

if (!require(thesisdown)) {
  if (params$`Install needed packages for {thesisdown}`) {
    remotes::install_github("ismayc/thesisdown")
  } else {
    stop(
      paste(
        "You need to run",
        'remotes::install_github("ismayc/thesisdown")',
        "first in the Console."
      )
    )
  }
}
library(thesisdown)
library(dplyr)
library(ggplot2)
library(knitr)
library(kableExtra)
library(patchwork)
```


```{r harmonicmeanfunc, echo = F}
harmonic_mean <- function(corpus){
  num_files <-length(unique(corpus$File.Name))
  means <- corpus %>% 
    mutate(
      comp_recip = 1/max(Compression.Time,1),
      decomp_recip = 1/max(Decompression.Time,1)
    ) %>% 
    reframe(
      harmonic_mean_comp= num_files/sum(comp_recip),
      harmonic_mean_decomp= num_files/sum(decomp_recip)
    )
  return(means)
}

```

```{r csvtotablefunc, echo = F}
csv_to_table <- function(filename, caption = "f"){
df <- read.csv(filename)
names(df) <- gsub("\\.", " ", names(df))
knitr::kable(df, "markdown", align= 'cc', caption = caption)
}
```

```{r statscsvfunc, echo = F}
stats_csv <- function(filename){
  df <- read.csv(filename)
  names(df) <-   gsub("\\.", " ",gsub("\\_", " " ,names(df))) %>% toupper()
  knitr::kable(df,  linesep = c(rep("", 5), "\\addlinespace"), format="latex", booktabs=TRUE) %>% 
  kable_styling(latex_options=c("scale_down", "hold_position", "striped"))
}
```

```{r comparecorporafunc, echo = F}
compare_coprora <- function(array, per_corpus = FALSE, x_axis = "Milestone"){
	df <- data.frame()
  ord <- 1
	for(corpus_pair in array){
		cp1 <- read.csv(corpus_pair[1])
		cp2 <- read.csv(corpus_pair[2])
				cbind(corpus = 1, cp1)
		cp1["corpus"] <- "Corpus 1"
		cp2["corpus"] <- "Corpus 2"
		comb <- bind_rows(cp1, cp2)

		means <- harmonic_mean(comb)

		comb<- comb %>% reframe(
		  comp_throughput = sum(Original.File.Size)/(1000*sum(Compression.Time)),
		  hm_comp = means$harmonic_mean_comp,
		  description = corpus_pair[3],
		  order = ord,
		  corpus = corpus
		)
		ord<- ord+1
		df <- bind_rows(comb, df)
	}

  plot1<- df %>% 
    ggplot(aes(x = reorder(description, order), y = comp_throughput, fill = description))+
    geom_col(show.legend = F)+
    {if(per_corpus)facet_wrap(~corpus)}+
    labs(x=x_axis, y = "Throughput (bytes/s)", title = "Throughput")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  plot2 <- df %>% 
    ggplot(aes(x = reorder(description, order), y = hm_comp, fill = description))+
    geom_col(show.legend = F)+
    {if(per_corpus)facet_wrap(~corpus)}+
    labs(x=x_axis, y = "Mean Compression Time (ms)", title = "Average Compression Time")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  plot1+plot2
}
```

```{r comparemaxstringlengthsfunc, echo = F}
compare_max_string_lengths <- function(array){
	df <- data.frame()
	for(corpus_pair in array){
		cp1 <- read.csv(corpus_pair[1])
		cp2 <- read.csv(corpus_pair[2])
		comb <- bind_rows(cp1, cp2)
		means <- harmonic_mean(comb)

		comb<- comb %>% reframe(
		  comp_throughput = sum(Original.File.Size)/( 1000 * sum(Compression.Time)),
		  hm_comp = means$harmonic_mean_comp,
		  size = corpus_pair[3],
		)
		df <- bind_rows(comb, df)
	}
  plot1<- df %>% 
    ggplot(aes(x = size, y = comp_throughput, fill = size))+
    geom_col(show.legend = F)+
    labs(x="Max String Length", y = "Throughput (bytes/s)", title= "Throughput")
  plot2 <- df %>% 
    ggplot(aes(x = size, y = hm_comp, fill = size))+
    geom_col(show.legend = F)+
    labs(x="Max String Length", y = "Mean Compression Time (ms)", title = "Average Compression Time")	
  plot1+plot2
}

```

# Optimizing LZW: Approach

To restate the goal of this thesis, we seek to optimize LZW for use in compression of DNA. I chose to write in C++. A majority of the work in this thesis involved rewriting, refactoring, and reconfiguring code to improve performance. The various methods we used for this process are discussed throughout the chapter.

While we may not end up creating the best DNA compressor available, the objective is to explore the boundaries of LZW and to tailor it as best we can for the task of DNA compression. As you will see, the algorithm has limitations.

Our Strategy was as follows:

1. Implement a basic version of LZW in C++.
2. Optimize the algorithm. Make it as fast as possible, and specifically focus on compressing DNA
3. Once the algorithm is fast, try to entropy encode (Huffman, arithmetic encoding, etc) the compressed files to improve compression ratio


## Corpora

Most compression papers make use of a corpus, which is a collection of files to run a compression algorithm on in order to evaluate performance and to compare the performance of different algorithms to one another. 

In the world of DNA compression, there are several academic papers on the subject. One of the first and most popular of the papers was published in 1994, and the selection of DNA sequences used in the paper have become an informal corpus for the subject of DNA compression, cited by more than thirty publications [@grumbach].

```{r corpus1filesfig, warnings= F, messages = F, fig.cap = "Corpus 1", fig.height = 7, echo = FALSE} 
csv_to_table('data/corpus_1_summary.csv', "Corpus 1")
```

Another, newer paper aimed to create a corpus specifically for compressing DNA [@prataspinho]. They put together a corpus of DNA sequences for this purpose, as summarized below. Since the papers publishing, it has been cited by several DNA compression papers.

```{r corpus2filesfig, warnings= F, messages = F, fig.cap = "Corpus 2", fig.height = 7, echo = FALSE}
csv_to_table('data/corpus_2_summary.csv', "Corpus 2")
```
The dataset in Table \@ref(tab:corpus2filesfig) is publicly available at this [https://tinyurl.com/DNAcorpus](https://tinyurl.com/DNAcorpus).

## Evaluating Performance

Evaluating performance of a program is difficult. There is a notion of theoretical run time, but on an actual computer there are many processes running in the background, so it can be hard to get a consistent reading on performance.

To attempt to counteract this, we ran the function on the same file multiple times, and took the median of the compression and decompression times for all the runs. Also, for any graphs or tables in this thesis, all versions of the algorithm for a particular table were run on the same computer. We also did our best to mitigate any other programs running on the computer at the time of data collection to prevent interference.

Most graphs in this section will refer to throughput and average compression time. Throughput is defined as the number of bytes that the program processes per second. The higher the throughput, the more efficient the algorithm. The average compression time is taken as a harmonic average, where the times are weighted by the size of the file. 

## A Starting Point

As stated previously, we thought it was best to get a working implementation of LZW in C++ on regular text files, , and then optimize it for DNA. We want to try various techniques tried by researches in the field, but it is important to have a fast baseline from which we can compare and improve upon. If the initial implementation is inefficient, it makes us harder to tell if the different techniques we have are affecting performance.

### Growing Codewords and Bit Output

When reading files on the computer, most characters are stored as bytes, which are made up of 8 bits. For instance `01000001` stands for the letter 'A' in ASCII encoding. Numbers in binary are simpler to display, so `00000001` is 1, `00000010` is 2, and so on.

But if we are translating numbers to binary, we don't need all of the bits in a byte. In binary, `1` is the same as `01` is the same as `00000000000001`. So when we are outputting codewords for LZW, we don't necessarily need to output a whole byte. We can have growing codewords.

As the number of codewords grows, the number of bits needed to represent it also grows. So if we are on codeword 8, we need 4 bits since 8 is `1000`. As our dictionary grows, we can grow the number of bits needed to display a codeword and save a lot of space in our compressed document.

So we needed a method of outputting bits one by one, and reading in bits one by one. This is not something that is supported in C++ on its own. We were able to create this functionality by defining a class.

```c++
// BitInput: Read a single bit at a time from an input stream.
// Before reading any bits, ensure input stream still has valid input
class BitInput {
 public:
  // Construct with an input stream
  BitInput(const char* input);

  BitInput(const BitInput&) = default;
  BitInput(BitInput&&) = default;

  // Read a single bit (or trailing zero)
  // Allowed to crash or throw an exception if past end-of-file.
  bool input_bit();

  int read_n_bits(int n);
}

// BitOutput: Write a single bit at a time to an output stream
// Make sure all bits are written out when exiting scope
class BitOutput {
 public:
  // Construct with an input stream
  BitOutput(std::ostream& os);

  // Flushes out any remaining bits and trailing zeros, if any:
  ~BitOutput();

  BitOutput(const BitOutput&) = default;
  BitOutput(BitOutput&&) = default;

  // Output a single bit (buffered)
  void output_bit(bool bit);

  void output_n_bits(int bits, int n);
}

```
So when we are encoding and need to output a codeword, we can `output_n_bits`, where `n` is the number of bits needed to display our greatest codeword. When decoding, we can just `read_n_bits`.

### Getting EOF to work

One of the very early issues with the implementation was how to denote the end of a file. The early implementation would work for some files, but for others the very last part of the file would be lost after encoding and then decoding.

In theoretical implementations of LZW, computer scientists tend to denote the end of a message with a special character, one that isn't seen anywhere else in the file. In this initial implementation, that wasn't possible because we wanted to be able to compress any file with any characters.

The solution was to reserve a codeword to mark the end of the file. So we start with a starting dictionary containing all ASCII characters.

```c++
	std::unordered_map<std::string, int> dictionary;
   	for (int i = 0; i < 256; ++i){
		std::string str1(1, char(i));
		dictionary[str1] = i;
	}
```
As stated in Section \@ref(lempel-ziv-welch), we will need to reserve a codeword to output when we are done encoding the file so that the decoder knows where the end is. So the algorithm goes along reading a file. It builds up a current string character by character, adding the character to the string and checking if it has seen that sequence before. Once it find the end of file, we stop and output the EOF codeword. 

The problem was, what about what is left over? Suppose we are reading a file, and the file ends with "ACCT". If "A" is in the dictionary, we see if "AC" is in the dictionary, and so on. This leaves us with three possible cases when we reached the end of the file

1. "ACC" was in the dictionary but "ACCT" was not. This means we can output the codeword for "ACC", follow it by the character "T", and we are done. This is the ideal scenario, because nothing is left over when we output the EOF codeword
2. "ACCT" was in the dictionary: This means we have one more codeword to output, but since we reached the end of the file, we never got to output it.
3. "AC" was in the dictionary, but "ACC" was not: in this case, we would output the codeword for "AC" output the character "C", and then start looping again starting at "T". But we reach the end of the file, so we output EOF before outputting T.

We solved this issue by adding 2 extra bits after the EOF codeword. These bits denote the case that occurred

```c++
// after we've encoded, we either have 
// no current block (case 0)
// we have a current block that is a single character (case 1)
// otherwise we have a current block > 1 byte (default)
switch (currentBlock.length()){
case 0:
	bit_output.output_bit(false);
	bit_output.output_bit(false);
	break;
case 1:
	bit_output.output_bit(false);
	bit_output.output_bit(true);
	bit_output.output_n_bits((int) currentBlock[0], CHAR_BIT);
	break;
default:
	bit_output.output_bit(true);
	bit_output.output_bit(true);

	int code = dictionary[currentBlock];
	bit_output.output_n_bits(code, codeword_size);
	break;
}
```
So when the decoder is reading and encounters the EOF codeword, it can look at the next two bits to see if anything is left over.

At this point, there was a working implementation that was able to compress and decompress files. Here is the performance of this version on the two corpora. 
<!---
9066eef
-->

<!---
```{r fixedeofstatsfig, warnings= F, messages = F, fig.cap = "Stats for the corpora after fixing EOF.", fig.height = 7,  echo = F}

stats_csv('data/stats/corpus_2_stats_fixedeof_9066eef.csv')
```
-->
### Using Constants
<!--
2c3f782
-->
The early version of the code was not clean. There were hard coded variables, unspecified integer types, and generally messy naming conventions that made the code difficult to read and debug.

The next major step in the code was to start using constants for everything, including

- `STARTING_CODEWORD`: What codeword we should start at
- `EOF_CODEWORD`: What we should output when we reach end of file
- `STARTING_DICT_SIZE`: At this stage, we had a starting dict size of 256 to hold all possible bytes, but later we will specialize for DNA

It also made sense to start using a specific type for codewords. At this stage, we opted for a 32 bit unsigned integer.

There are several tools at a developers disposal when looking to debug and optimize code. One tool used for this thesis was `callgrind` which is a tool of `valgrind` a profiling tool. Profiling tools are used to look at how your code works, where the bottlenecks are, and what can be changed/improved for the performance of your code.

Callgrind in particular is a profiling tool which associates assembly instructions to lines of code, indicating to the programmer which lines take a lot of instructions and which take less. For those unfamiliar, assembly instructions are what code is turned into so that it can be ran on your computer's processor. In general, more instructions means that code takes longer to run.

The callgrind output drew attention to one particular part of the code. A C++ `unordered_map` uses iterators, basically pointers into the dictionary. If an entry is not present in the dictionary, the `find()` function will return a iterator to the end of the dictionary. 

The check for this in our algorithm looked like this.

```c++
// if we've already seen the sequence, keep going
if (dictionary.find(currentBlock + next_character) != dictionary.end()){
	currentBlock = currentBlock + next_character;
}
```

Here is the callgrind output for that line.

```
105,030,135 ( 0.18%)          if (dictionary.find(currentBlock + next_character) != dictionary.end()){
13,537,450,317 (22.83%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char) (3,890,005x)
11,653,779,430 (19.65%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::find(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (3,890,005x)
2,108,383,120 ( 3.56%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string() (3,890,005x)
956,941,242 ( 1.61%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::end() (3,890,005x)
241,180,314 ( 0.41%)  => /usr/include/c++/9/bits/hashtable_policy.h:bool std::__detail::operator!=<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true>(std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&, std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&) (3,890,005x)
```
As shown, this line is taking a significant amount of instructions, and it needs to pull the end() of the dictionary each time it is ran. If we use cend() instead and save that iterator in a variable called end, we can save a significant amount of instructions.

```
89,470,115 ( 0.61%)          if (dictionary.find(currentBlock + next_character) != end ){
3,353,009,053 (22.78%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > std::operator+<char, std::char_traits<char>, std::allocator<char> >(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, char) (3,890,005x)
2,833,786,025 (19.26%)  => /usr/include/c++/9/bits/unordered_map.h:std::unordered_map<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, unsigned long, std::hash<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::equal_to<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > >, std::allocator<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long> > >::find(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) (3,890,005x)
420,120,704 ( 2.85%)  => /usr/include/c++/9/bits/basic_string.h:std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >::~basic_string() (3,890,005x)
50,570,065 ( 0.34%)  => /usr/include/c++/9/bits/hashtable_policy.h:bool std::__detail::operator!=<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true>(std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&, std::__detail::_Node_iterator_base<std::pair<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const, unsigned long>, true> const&) (3,890,005x)
```

Of course, there are several issues with this method. It is difficult to associate instructions with a single line of code. Some lines are interdependent, and assembly often behaves differently than the code that produces it. Another thing is that compliers are very advanced, and sometimes small optimizations like this are done by the compiler automatically. 

Despite these issues, this change was still worth making, if not to save time then for sake of clarity and readability of the code. Also, despite the inaccuracy of callgrind, like many profiling tools, its job is not necessarily to provide exact measurements of code performance, but to give indications to trouble spots which can be improved. 

Here are the runs after this optimization.



<!---
```{r usingconstantsstatsfig, warnings= F, messages = F, fig.cap = "Stats for the corproa after switching some constants", fig.height = 7, echo = F}
stats_csv('data/stats/corpus_1_usingconstants_2c3f782.csv')
stats_csv('data/stats/corpus_2_usingconstants_2c3f782.csv')
```
-->



### Extraneous String Concatenations

<!--
8963f64ed59983edc15ed255b894ee666f2e1176
-->

The LZW algorithm is built on iteration: we go through each character, adding it to our current block. If we've seen that current block before, we keep going. If not, we add that block to the dictionary and start over. 

Another thing that I noticed from the callgrind output was that a lot of time/instructions are being spent on string concatenation. In general, string concatenation in most language, including C++, have a lot of overhead. A lot of implementations involve creating a new string every time you concatenate two existing string, which can have a significant performance penalty.

In the version of the algorithm at the time, every time we have already seen a sequence, we have to concatenate a character. I noticed that I was doing this concatenation multiple times without needing to.

```c++
// we concatenate the strings here
if (dictionary.find(currentBlock + next_character) != end ){
	// and here
	currentBlock = currentBlock + next_character;
}
else{

	// other code here ommitted


	// and here! 
	dictionary[currentBlock + next_character] = codeword;
}
```

If I just save `currentBlock + next_character` into a new variable, that will save me from doing the concatenation 2 more times.

```c++
// save concatenation here
std::string string_seen_plus_new_char = current_string_seen + next_character;
if (dictionary.find(string_seen_plus_new_char) != end ){
	current_string_seen = string_seen_plus_new_char;
}
else{

// other code omitted here

	dictionary[string_seen_plus_new_char] = codeword;
}
```

Here are the statistics on the version of the algorithm after this change.

<!---
```{r stringconcatstatsfig, warnings= F, messages = F, fig.cap = "Stats after eliminating excess string concatenations", fig.height = 7, echo = F}

stats_csv('data/stats/corpus_1_stringconcat_8963f64.csv')
stats_csv('data/stats/corpus_2_stringconcat_8963f64.csv')
```
-->

### Dictionary Lookups
<!--
34bf4e9a6233137b388323f810791687c2a0f823
-->
 
Dictionary lookups can be expensive, especially with the standard library. We learned from `callgrind` that along with string operations, our program spent a lot of time doing these lookups.

We looked for ways to reduce the volume of lookups. At the time, the way the algorithm worked was that it looked up the current string and the next character in the dictionary. If that string is in the dictionary, we keep going. If not, we output the codeword for the current string.

But, the current string on this iteration is the current string from the last iteration, plus one character. So when we were on the previous iteration of the loop, we could save that lookup and prevent a second lookup.


```c++
while(next_character != EOF){

	// code omitted


	// if we've already seen the sequence, keep going
	std::string string_seen_plus_new_char = current_string_seen + next_character;
	// save this iterator`
	if (dictionary.find(string_seen_plus_new_char) != not_in_dictionary ){
		current_string_seen = string_seen_plus_new_char;
	}
	else{

		// shouldn't look up again
		int code = dictionary[current_string_seen];


		// code omitted
	}
	next_character = input.get();
}
```

We can save that lookup, like so.

```c++

while(next_character != EOF){

	// code ommitted

	// if we've already seen the sequence, keep going
	std::string string_seen_plus_new_char = current_string_seen + next_character;
	codeword_seen_now = dictionary.find(string_seen_plus_new_char);
	if (codeword_seen_now != not_in_dictionary ){
		current_string_seen = string_seen_plus_new_char;
		codeword_seen_previously = codeword_seen_now; // save codeword here
	}
	else{

 		// on the next iteration, we use it here
		int code = codeword_seen_previously->second;

		// code omitted

	}
	next_character = input.get();
}
```


<!---
```{r dictaccessesstatsfig, eval = F, warnings= F, messages = F, fig.cap = "Stats after reducing dictionary accesses", fig.height = 7, echo = F}
stats_csv('data/stats/corpus_1_dictaccesses_34bf4e9.csv')

```
-->

### Using Const Char *
<!--
a12a2901d87d723223b21a9c9115c1c9787a8966
-->

The algorithm works by reading through the entire file, so we know that at some point, we will need to see every byte of the entire file.

When reading a byte stream of the file, the file may not be in memory the way we want it. The `ifstream` class in C++ also has many extraneous feature that we don't need. If we map the file directly into memory using `mmap` and pass around a pointer to that data, it will simplify and speed up the scanning process. Also, using a `char*` opens the possibility to getting rid of `std::string` entirely, which means way less overhead and decreased compression time.

<!--
**TODO**: Need to talk about getting rid of string concatenation at some point.
-->

<!---
```{r charstarstatsfig, eval =F, warnings= F, messages = F, fig.cap = "Stats after using character pointer", fig.height = 7, echo = F}
stats_csv('data/stats/corpus_1_charstar_a12a290.csv', caption = "Corpus 1 Stats after using character pointers")
stats_csv('data/stats/corpus_2_charstar_a12a290.csv', caption = "Corpus 2 Stats after using character pointers")
```
-->

### Comparison

Taking metrics of the algorithm at each of the stages listed in this chapter, we can make a graph showing the improvements in performance.

```{r comparecorporaoptimizingfig, warnings= F, messages = F, fig.cap = "Comparison of the performance of the different milestones", fig.height = 4, echo = F}
compare_coprora(list(
  c('data/stats/corpus_1_stats_fixedeof_9066eef.csv', 'data/stats/corpus_2_stats_fixedeof_9066eef.csv', 'Fixed EOF'),
  c('data/stats/corpus_1_usingconstants_2c3f782.csv', 'data/stats/corpus_2_usingconstants_2c3f782.csv', 'Using constants'),
  c('data/stats/corpus_1_stringconcat_8963f64.csv', 'data/stats/corpus_2_stringconcat_8963f64.csv', 'String concatenation'),
  c('data/stats/corpus_1_dictaccesses_34bf4e9.csv', 'data/stats/corpus_2_dictaccesses_34bf4e9.csv', 'Excess dictionary calls'),
  c('data/stats/corpus_1_charstar_a12a290.csv', 'data/stats/corpus_2_charstar_a12a290.csv', 'Character pointer')))
```

Figure \@ref(fig:comparecorporaoptimizingfig) shows the performance of the algorithm at the different milestones mentioned in this section. All graphs use the files from both corpora unless stated otherwise. On the left side of the figure you see that throughput increases for the program with each optimization. On the right, observe that mean compression time decreases with each optimization.

## Trying Different Dictionaries

A lot of the stress of the LZW algorithm is on the dictionary. We are constantly looking strings up and placing others. Because of the reliance on this data structure, we know that the dictionary accesses and lookups are a bottleneck, so improvements in those areas could greatly increase the efficiency of our program. 

So another step towards an efficient LZW seemed to be to abstract out the C++ `std::unordered_map` and have multiple different dictionary implementations to try and experiment with in our attempt to optimize LZW for DNA compression.

### Direct Map

In our analysis of the two corpora, we found some interesting statistics in the redundancy of the data.


```{r runstatsfigcp1, warnings= F, messages = F, fig.cap = "The counts for runs of each length", fig.height = 7, echo = F}
stats_csv('data/corpus_1_run_stats.csv')
```

```{r runstatsfigcp2, warnings= F, messages = F, fig.cap = "The counts for runs of each length", fig.height = 7, echo = F}
stats_csv('data/corpus_2_run_stats.csv')
```

Tables \@ref(tab:runstatsfigcp1) and \@ref(tab:runstatsfigcp2) show stats on the run lengths of the "runs" of data, where a run is a string added to the dictionary during a run of the LZW algorithm, and a histogram of runs from both corpora can be seen in Figure \@ref(fig:allrunshistfig).

```{r bothrunsfig, warnings= F, messages = F, fig.cap =  "Run counts for both corpora", fig.height = 7, echo= F}
stats_csv('data/corpus_both_run_stats.csv')
```

```{r allrunshistfig, warnings= F, messages = F, fig.cap = "A histogram showing the lengths of runs for both copora.", fig.height = 3, echo = F}
all_runs <- read.csv('data/all_corpus_run_counts.csv')
all_runs %>% 
  ggplot(aes(x=Run.Length, y=count))+
  geom_bar(stat="identity")+
  labs(title="Run Distribution for Corpora", x = "Run Length")
```

Given this data, it is clear that it would be advantageous to speed up the dictionary for smaller run sizes, since most of the runs are below size 15. As stated before, there have been research papers about the possibility of using multiple indexed dictionaries for LZW, including Keerthy [@KeerthyMID]. 
To achieve a similar effect to multiple indexed dictionaries, we opted for a unique approach. Rather than use a hashmap, we can map the strings directly into memory. Since all of the strings only contain four characters ('A', 'C', 'T', and 'G'), we can represent the characters with two bits. So for a length `n` string, we can represent it with `2n` bits. 

So we can create an indexed dictionary directly in memory for all strings below a certain length. We can use the `2n` bit representation of the string
to index into an array of codewords. 

For each string size 1 to n, we have an array with enough slots for every possible string. For example, for strings of length 3, we have an array of size $4^3$, since there are $4^3$ possible strings. In each of those $4^3$ slots, we have space for a codeword. All strings of length 3 can be represented by 6 bits, and since 6 bits can represent $2^6=4^3$ values, we can use the bit representation to index into the dictionary. If the codeword at that place in the dictionary is 0, we have never seen it before. If it is non-zero, we have found the codeword for that string. For all strings greater than `n`, we can just use a hashmap on top to handle those.

As seen in Figure \@ref(fig:directmaplength10statsfig), the both the average compression time and throughput are greater for a direct map dictionary with a max string length of 15 as opposed to a max string length of 10. This makes sense, because any string over the max requires an entry in a hashmap, which takes much more time than a dictionary access. Using a string length of over 15 is difficult because the amount of memory required increases exponentially.

<!---
```{r directmaplength10statsfig, warnings= F, messages = F, fig.cap = "Stats for direct map with max length of 10", fig.height = 7, echo = F}
stats_csv('data/stats/directmap/corpus_1_directmap_154fed4_len10.csv')
stats_csv('data/stats/directmap/corpus_2_directmap_154fed4_len10.csv')
```
-->


<!--
i don't know why this caption won't show up
--->

```{r directmaplenthcompfig, warnings= F, messages = F, fig.cap = "Comparing different max lengths for Direct Map", fig.height = 3, echo = F}
compare_max_string_lengths(list(
	c('data/stats/directmap/corpus_1_directmap_154fed4_len10.csv', 'data/stats/directmap/corpus_2_directmap_154fed4_len10.csv', 10),
	c('data/stats/directmap/corpus_1_directmap_266bcb7_len15.csv', 'data/stats/directmap/corpus_2_directmap_266bcb7_len15.csv', 15)
))
```

It's also worth noting that since we are allowing strings of all lengths, the compression ratio has not changed.

### Multiple Indexed Dictionaries


Similar to the Direct Mapped approach, we use dictionaries for each string size up to a certain size `n` , and for all strings of length greater than `n`, we use a regular dictionary. As with the direct map dictionary, we need to specify a max string length. We collected metrics for different choices of max string length. As seen in Figure \@ref(fig:multdictlengthcompfig), the throughput tends to increase and the average compression time tends to increase as we increase the number of indexed dictionaries. This result was not necessarily one we expected, but it does make sense that there is a certain amount of overhead that is required for a hashmap. The hash function takes about the same amount of time no matter the number of elements in the map, and resizing is rare. So adding more dictionaries only adds more overhead, which tends to slightly decrease efficiency.

Compression ratio still remains the same as strings of all lengths are accommodated. 
This logic is supported by Figure \@ref(fig:multvsonedictfig), which shoes one `std::unordered_map` compared the multiple indexed model.

<!---
```{r multdictlength10statsfig, warnings= F, messages = F, fig.height = 3, echo = F}
stats_csv('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv')
stats_csv('data/stats/multdict/corpus_2_multdict_154fed4_len10.csv')
```
-->

```{r multdictlengthcompfig, warnings= F, messages = F, fig.cap = "Comparing different max lengths for Direct Map", fig.height = 3, echo = F}
compare_max_string_lengths(list(
	c('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', 'data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', 10),
	c('data/stats/multdict/corpus_1_multdict_6d73501_len15.csv', 'data/stats/multdict/corpus_2_multdict_6d73501_len15.csv', 15),
	c('data/stats/multdict/corpus_1_multdict_80d2016_len20.csv', 'data/stats/multdict/corpus_2_multdict_80d2016_len20.csv', 20),
	c('data/stats/multdict/corpus_1_multdict_06f9a20_len25.csv', 'data/stats/multdict/corpus_2_multdict_06f9a20_len25.csv', 25)
))
```


```{r multvsonedictfig, warnings= F, messages = F, fig.cap = "Comparing one Dict to Mult Dict", fig.height = 4, echo = F}

compare_coprora(list(
  c('data/stats/corpus_1_stddict_68bea3d.csv', 'data/stats/corpus_2_stddict_68bea3d.csv', 'Standard Dictionary'),
  c('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', 'data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', 'Multiple Indexed Dictionaries')
  ))

```


### Comparison

Figure \@ref(fig:dictionarytechniquecompfig) shows a comparison of all three techniques: Standard Dictionary, Multiple Standard Dictionaries, and Direct Mapped Dictionary. As shown, the Direct Map technique greatly increases throughput and thus decreases average compression time. Given these results, we decided to shift our focus onto the Direct Map and try to optimize this scheme as much as possible.

```{r dictionarytechniquecompfig, warnings= F, messages = F, fig.cap = "Comparing the three types of dictionaries. The Direct Map has a max length of 15 and the Mult Dict has a max length of 10.",echo = F, fig.height=3}

compare_coprora(list(
  c('data/stats/corpus_1_stddict_68bea3d.csv', 'data/stats/corpus_2_stddict_68bea3d.csv', 'Standard Dictionary'),
  c('data/stats/directmap/corpus_1_directmap_266bcb7_len15.csv', 'data/stats/directmap/corpus_2_directmap_266bcb7_len15.csv', 'Direct Map Dictionary'),
  c('data/stats/multdict/corpus_1_multdict_154fed4_len10.csv', 'data/stats/multdict/corpus_2_multdict_154fed4_len10.csv', 'Multiple Indexed Dictionaries')
  ))

```

## Optimizing Direct Map Even more

Given that the Direct Map dictionary showed great performance improvements, we decided to narrow in on the scheme to see if there were ways to improve it even further. 

### Finding the Longest Runs

Our dictionary data structures all have a `get_longest_in_dict` function. This function does the boring work of iterating through the input from the start, checking if each substring is in the dictionary. 

Given the statistics of our corpora, we know that this process can be faster. Since most runs are above 6-7, we waste a lot of time by starting from the bottom.

Another strategy would be to start from the maximum string length of the dictionary, so 15. We can check if the next string of the max length is in the dictionary. If it is, we need to check strings longer than the max, so we can iterate up. If it isn't, we need to check strings shorter, so we can either iterate down . Here is some pseudocode that mimics this proposed algorithm.

```
find_longest(input_string){

	// calculate the index of the next 15 chars
	// where index = converting each char to two bits
	index_of_next_15_chars = calculate_index(input_string[0:15]);

	// look up our string
	lookup = dictionary[index_of_next_15_chars];

	if(lookup is in dictionary){
		loop_up();
	}
	else {
		loop_down();
	}
	
}

```

Calculating the index, however, takes time. While we are looping up or down, we could just use the index of the next 15 characters as a starting point. If we are looping up, we can add on the next character's two bit representation as we loop. For instance, suppose our string has an index of `00101100`. If the next character is 'A', we can simply tack the two bit representation for 'A' to the end of our index, yielding `00101100|00`. 

Similarly, we can chop off two bits at a time while looping down. We can name this process looping "on the fly", since we are constructing our index on the fly rather than recalculating it every time. So our modified pseudocode would look like the code below:

```
find_longest(input_string){

	// calculate the index of the next 15 chars
	// where index = converting each char to two bits
	index_of_next_15_chars = calculate_index(input_string[0:15]);

	// look up our string
	lookup = dictionary[index_of_next_15_chars];

	if(lookup is in dictionary){
		loop_up_on_fly(index);
	}
	else {
		loop_down_on_fly(index);
	}
	
}

```

Of course, there are theoretically quicker ways of iterating than looping up or down. We could use binary search. 

Binary search is a searching technique for sorted lists, but we can use it in this scenario as well. Suppose we have a string of characters, and we are searching for the longest string already in the dictionary. The algorithm works by repeatedly dividing the search interval in half, comparing the middle element of the subarray to the target value, and then deciding whether to continue the search on the lower half or upper half of the subarray.

If the middle element is equal to the target value, the search ends and the position of the element is returned. If the middle element is greater than the target value, the search continues on the lower half of the subarray. Conversely, if the middle element is less than the target value, the search continues on the upper half of the subarray.

Binary search has a time complexity of O(log n), which makes it a very efficient algorithm for searching large sorted arrays. In our case, we search for where the string of length n is in the dictionary, but the string of length n+1 is not.

```
find_longest(input_string){

	// code omitted...

	if(lookup is in dictionary){
		loop_up_on_fly(index);
	}
	else {
		loop_down_binary_search(index);
	}
	
}

```

Of course, we could also calculate the indexes for binary search on the fly. So we now have 5 different schemes of finding the longest run: looping up or down, looping up or down on the fly, binary search, binary search on the fly, and looping up from 0 like we were doing before. Figure \@ref(fig:findlongestfrommaxfig) shows a comparison of these methods. 

```{r findlongestfrommaxfig, warnings= F, messages = F, fig.cap = "Comparing the different ways of finding the longest run", echo = F, fig.height = 4}
compare_coprora(list(
  c('data/stats/ddict/corpus_1_ddictbinarysearch_e196a55.csv', 'data/stats/ddict/corpus_2_ddictbinarysearch_e196a55.csv', 'Binary search'),
  c('data/stats/ddict/corpus_1_ddictbinarysearchflyindex_94305a5.csv', 'data/stats/ddict/corpus_2_ddictbinarysearchflyindex_94305a5.csv', 'Binary Fly Index'),
  c('data/stats/ddict/corpus_1_ddictloopdownflyindex_7ae114e.csv', 'data/stats/ddict/corpus_2_ddictloopdownflyindex_7ae114e.csv', 'Loop Down Fly Index'),
  c('data/stats/ddict/corpus_1_ddictloopdown_2d0c8b8.csv', 'data/stats/ddict/corpus_2_ddictloopdown_2d0c8b8.csv', 'Loop Down'),
  c('data/stats/directmap/corpus_1_directmap_266bcb7_len15.csv', 'data/stats/directmap/corpus_2_directmap_266bcb7_len15.csv', 'Looping up from zero')
), TRUE, "Strategy for Finding Longest Run")

```

We see that calculating the index on the fly does improve performance. However, we can also see that none of the other strategies are better than just looping up from zero. This could be because most of the runs are very short, which we can see in \@ref(fig:allrunshistfig).  This means that if we start looking from runs around length 10, we should see a performance improvement.

### Finding the Longest From The Average

As we can see in Figure \@ref(fig:findlongestfromavg), looping or doing binary search from the average run length, which we approximate at 7, increases throughput and decreases mean compression time relative to looping up from zero.

```{r findlongestfromavg, warnings= F, messages = F, fig.cap = "Comparing the different ways of finding the longest run", echo = F, fig.height = 3}
compare_coprora(list(
  c('data/stats/ddict/corpus_1_ddictdownavg_edafb16.csv', 'data/stats/ddict/corpus_2_ddictdownavg_edafb16.csv', 'Loop Down from Average'),
  c('data/stats/ddict/corpus_1_ddictbsavg_a482e6d.csv', 'data/stats/ddict/corpus_2_ddictbsavg_a482e6d.csv', 'Binary search from Average'),
  c('data/stats/ddict/corpus_1_ogddict_266bcb7.csv', 'data/stats/ddict/corpus_2_ogddict_266bcb7.csv', 'Looping up from zero')
), TRUE)


```

As we predicted, this greatly improves performance. We are capitalizing on the fact that most runs are short. There is the occasional run that is very long, but from the run statistics we saw that the standard deviation for both corpora was pretty small. So on the edge cases in which there are very long runs, we could be wasting a lot of time. 

### Not Allowing strings over max

One way to avoid the edge case where we encounter a very long run is to just not allow strings in the dictionary longer than the max string length, in this case, 15. This means we won't have to deal with the overhead of the hashmap on top of our dictionary, but we will take a hit in compression ratio. Figure \@ref(fig:findlongestwmax) summarizes the results of the different methods with the max length rule enforced. There isn't too much of a performance change, which makes sense because long runs are very rare. 


```{r findlongestwmax, warnings= F, messages = F, fig.cap = "Comparing the different ways of finding the longest run starting at average with strings over max not accepted", echo = F, fig.height = 5}
compare_coprora(list(
  c('data/stats/ddict/corpus_1_ddictbsavg_a482e6d.csv', 'data/stats/ddict/corpus_2_ddictdownavg_edafb16.csv', 'Loop Down'),
  c('data/stats/ddict/corpus_1_ddictloopupwmax_3868e61.csv', 'data/stats/ddict/corpus_2_ddictloopupwmax_3868e61.csv', 'Loop Down with Max Enforced'),
  c('data/stats/ddict/corpus_1_ddictbsavg_a482e6d.csv', 'data/stats/ddict/corpus_2_ddictbsavg_a482e6d.csv', 'Binary search'),
  c('data/stats/ddict/corpus_1_ddictbswmax_588ee0d.csv', 'data/stats/ddict/corpus_2_ddictbswmax_588ee0d.csv', 'Binary Search with Max Enforced')
), TRUE, "Strategy for Finding Longest Run")

```

Of course, not allowing string over max length does mean that our compression ratio will change. Up until this point, all the different versions of the Direct Mapped Dictionary had the same compression ratio. Table \@ref(tab:maxenforcedstats) shows the affect of enforcing the max string length on the total compression ratio over both corpora. As we can see, it is minimal, which makes sense because long runs are very rare.


```{r maxenforcedstats, warnings= F, messages = F, echo = F}

  before_corpus1 <- read.csv('data/stats/ddict/corpus_1_ddictbsavg_a482e6d.csv')
before_corpus2 <- read.csv('data/stats/ddict/corpus_2_ddictdownavg_edafb16.csv')
  after_corpus1 <- read.csv('data/stats/ddict/corpus_1_ddictloopupwmax_3868e61.csv')
after_corpus2<- read.csv( 'data/stats/ddict/corpus_2_ddictloopupwmax_3868e61.csv')
		before <- bind_rows(before_corpus2, before_corpus1)
		after <- bind_rows(after_corpus2, after_corpus1)
f <- tibble(
  cr_before = sum(before$Original.File.Size)/sum(before$Compressed.Size),
  cr_after = sum(after$Original.File.Size)/sum(after$Compressed.Size)
)
knitr::kable(f, format="latex", booktabs=TRUE, caption = "Compression Ratio change from disallowing long strings")

```

### Using `pext`

One potential bottleneck of finding the longest run is converting a run of characters into an index. We can try to do it on the fly as we loop up or down, but we could also use machine instructions. 

Our strategy is to use `pext`, which extracts bits in parallel, meaning at the same time. The `pext` instruction is a recent addition to the x86 istruction set, so it has not been widely used yet as optimization technique.

We give `pext` a string of characters, say 'ACTG', and a bit mask, and it will extract those bits from our string. Figure \@ref(fig:pext) details this process for a string of length 4.

It theoretically does this in one machine instruction, which could be much more efficient than looping over all the characters.

\begin{figure}[h]\centering


\usetikzlibrary{chains,decorations.pathreplacing}

 \begin{tikzpicture}[
node distance=0pt,
 start chain = A going right,
    X/.style = {rectangle, draw,% styles of nodes in string (chain)
                minimum width=2ex, minimum height=3ex,
                outer sep=0pt, on chain},
    B/.style = {decorate,
                decoration={brace, amplitude=5pt,
                pre=moveto,pre length=1pt,post=moveto,post length=1pt,
                raise=1mm,
                            #1}, % for mirroring of brace, if necessary
                thick},
    B/.default=mirror, % by default braces are mirrored
                        ]
\foreach \i in {0,1,0,0,0,0,0,1,
                0,1,0,0,0,0,1,1,
                0,1,0,0,0,1,1,1,
                0,1,0,1,0,1,0,0, }% <-- content of nodes
    \node[X] {\i};
\draw[B] ( A-6.south west) -- node[below=2mm] {Index of A} ( A-7.south east);
\draw[B] (A-14.south west) -- node[below=2mm] {Index of C} (A-15.south east);
\draw[B] (A-22.south west) -- node[below=2mm] {Index of G} (A-23.south east);
\draw[B] (A-30.south west) -- node[below=2mm] {Index of T} (A-31.south east);
\node (B1) [inner sep=1pt,above=of A-1.north west] {$\downarrow$};
\node (B2) [inner sep=1pt,above=of A-9.north west] {$\downarrow$};
\draw[B=](B1.north) -- node[above=2mm] {A in binary}(B2.north);
\node (B3) [inner sep=1pt,above=of A-9.north west] {$\downarrow$};
\node (B4) [inner sep=1pt,above=of A-17.north west] {$\downarrow$};
\draw[B=](B3.north) -- node[above=2mm] {C in binary}(B4.north);
\node (B5) [inner sep=1pt,above=of A-17.north west] {$\downarrow$};
\node (B6) [inner sep=1pt,above=of A-25.north west] {$\downarrow$};
\draw[B=](B5.north) -- node[above=2mm] {G in binary}(B6.north);
\node (B7) [inner sep=1pt,above=of A-25.north west] {$\downarrow$};
\node (B8) [inner sep=1pt,above=of A-33.north west] {$\downarrow$};
\draw[B=](B7.north) -- node[above=2mm] {T in binary}(B8.north);
    \end{tikzpicture}

\caption{How `pext` extracts bits}

\label{fig:pext}
\end{figure}



We can apply this technique to our `find_longest` function: we can extract the index of a string using pext very quickly, then use that index rather than recalculating it every time. Figure \@ref(fig:findlongestwpext) hows the results of this application for both looping and binary search. In both cases, using pext to extract the index of the string increases throughput and decreases average compression time.

```{r findlongestwpext, warnings= F, messages = F, fig.cap = "Comparing the different ways of finding the longest run with pext", echo = F, fig.height = 5}
compare_coprora(list(
  c('data/stats/ddict/corpus_1_ddictloopupwmax_3868e61.csv', 'data/stats/ddict/corpus_2_ddictloopupwmax_3868e61.csv', 'Loop Down with Max'),
  c('data/stats/ddict/corpus_1_ddictloopwpext_70ee265.csv', 'data/stats/ddict/corpus_2_ddictloopwpext_70ee265.csv', 'Loop Down with Pext'),
  c('data/stats/ddict/corpus_1_ddictbswmax_588ee0d.csv', 'data/stats/ddict/corpus_2_ddictbswmax_588ee0d.csv', 'Binary Search with Max'),
  c('data/stats/ddict/corpus_1_ddictbswpext_6737906.csv', 'data/stats/ddict/corpus_2_ddictbswpext_6737906.csv', 'Binary Search with Pext')
), TRUE, "Strategy for Finding Longest Run")


```


## Returning to Compression Ratio

After spending a lot of time debugging, we returned our attenting to the compression ratio. Most of our optimizations didn't have much of an effect on compression ratio, with the exception of disallowing strings over a certain length. It is also worth noting that the Direct Map dictionary and the Std dictionary implementations will have different compression ratios, as the Direct map has a max codeword size of 16 bits while the Std dict has a max codeword size of 32 bits. Std dict also allows longer strings.

### A point of Comparison

It's worth noting that for sequences of DNA, if the bases are encoded in ASCII, there is a simple and effecient algorithm to compress the file with a 4.0 compression ratio every time. Since there are only 4 bases, we can represent each base with 2 bits. This is a simple conversion, and it requires no dictionaries or codewords. 

So the takeaway is, if a DNA compression algorithm can't compress with a compression ratio of higher than 4.0 (2 bits per base), than this simple algorithm would be perferable every time. 

We implemented this method using pext, and the results are detailed in the next chapter.

### Entropy Encoding

Our initial idea was that we would compress the DNA with LZW, then use a entropy encoder like arithmetic encoding or Huffman to further compress the data. This was an oversight, as the algorithm as we have described it thus far does not lend itself well to entropy encoding.

Our algorithm outputs codewords and two bits representing the next character. For a codeword size of 16 bits, this means each loop of our algorithm outputs 18 bits. So any repretitiveness or reuse will not be detectable by an entropy encoder which works on data of 8, 16, or 32 bit chunks.

So maybe we break the compressed file into two seperate files; a file of codewords and a file of the two bit representations of the following characters. Now we can compress these two files, right?

The issue is that entropy encoders rely on repeating numbers of a high frequency. We can cut down the number of bits a certain entry takes to make it more compressible, and make less common entries take more bits (see Figure \@ref(fig:huffman)). DNA neucleotides show up with roughly the same frequency, so having 2 bits per character is hard to beat. So the file with all the charaters in it can't be compressed further.

What about the codewords? Well, a key realization is that any codeword will only show up 4 times in a single run of the program. Say we add the codeword `1234="ACT"` to our dictionary. When will we output this codeword ? Well, we will output it when we see "ACT" followed by another character. Since there are only 4 characters, once you see those 4 other strings ("ACTA", "ACTG", "ACTC", and "ACTT"), we will never output it again because we are looking for the longest run possible.

There are two cases in which we will output a codeword more than 4 times. If we run out of codewords and we have never encoded "ACTG", any time "ACTG" comes up, we will output `1234G`. This case is hard to predict as we have no way of controlling what strings are left when we run out of codewords. The other case is for strings that are the max length. These long runs may show up multiple times, and since we won't add any strings longer than the max to our dictionary, it will never be overwritten. However, these runs are very rare, or else the compression ratio wouldn't be an issue.

So on average, every codeword shows up a maximum of 4 times. This means that for long strands of DNA, we have output nearly all of the codewords with relatively even frequency. In other words, entropy encoding will not shorten the length of the codewords.


### A New Approach

Given that we are not able to achieve a compression ratio over 4.0 with the current LZW, we need to alter our approach. The issue with our dataset is that long runs are rare. Every once in a while, you may replace a run of 15 with a codeword, but most of the time you are replacing a run less than 8. Eight characters can be represented by 16 bits, so any run under 8 that is replaced loses bits 

The fact that we are not getting a compression ratio over 4.0 led us to think of a new twist on the algorithm specifically tailored for this situation. The scheme uses three streams of data: characters, codewords, and indicator bits. The algorithm works as follows.

Compression:

+ if the next longest run is less than 8 characters, we add it to the dictionary, but rather than output the codeword, we just output the 2 bit representation of the characters. We output a 0 to the indicator stream to indicate this choice.
+ if the next longest run is equal to or greater than 8, we output a codeword to the codeword stream and the next character to the character stream. We also output a 1 to the indicator stream to indicate a codeword was output.

Decompression: We start by reading an indicator bit

+ if the bit is 1, we read a codeword from the codeword stream and the next character from the character stream. We add this to the dictionary like the old algorithm.
+ if the bit is 0, we read the next 8 characters. We then use `find_longest` to find the longest run and add it to the dictionary. We can then put the 8 characters into the output stream and move on

This is, in essence, a greedy algorithm. We have the choice of outputting a codeword or 8 characters every time, and we choose the choice which results in the least number of bits output.

The other advantage to this algorithm is that it works well for entropy encoding. The indicator bits are mostly 0, since most runs are less than 8. The codeword stream is also compressible, since none of the codewords for strings less than length 8 are output. The performance of this new scheme is assessed in the next chapter.

