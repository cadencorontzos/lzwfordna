% This is the Reed College LaTeX thesis template. Most of the work
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See https://www.reed.edu/cis/help/LaTeX/index.html for help. There are a
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment.
% They won't show up in the document, and are useful for notes
% to yourself and explaining commands.
% Commenting also removes a line from the document;
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in
% the 2002-2003 Senior Handbook. Ask a librarian to check the
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}
% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: https://www.ctan.org/
%%
\usepackage{graphicx,latexsym}
\usepackage{amsmath}
\usepackage{amssymb,amsthm}
\usepackage{longtable,booktabs,setspace}
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
% Added by CII
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{float}
\floatplacement{figure}{H}
% Thanks, @Xyv
\usepackage{calc}
% End of CII addition
\usepackage{rotating}

% Next line commented out by CII
%%% \usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the
% bibliography is included.
%\usepackage{biblatex-chicago}
%\bibliography{thesis}


% Added by CII (Thanks, Hadley!)
% Use ref for internal links
\renewcommand{\hyperref}[2][???]{\autoref{#1}}
\def\chapterautorefname{Chapter}
\def\sectionautorefname{Section}
\def\subsectionautorefname{Subsection}
% End of CII addition

% Added by CII
\usepackage{caption}
\captionsetup{width=5in}
% End of CII addition

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

% Syntax highlighting #22
  \usepackage{color}
  \usepackage{fancyvrb}
  \newcommand{\VerbBar}{|}
  \newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
  \DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
  % Add ',fontsize=\small' for more characters per line
  \usepackage{framed}
  \definecolor{shadecolor}{RGB}{248,248,248}
  \newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
  \newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
  \newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
  \newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\BuiltInTok}[1]{#1}
  \newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
  \newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
  \newcommand{\ExtensionTok}[1]{#1}
  \newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
  \newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\ImportTok}[1]{#1}
  \newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
  \newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
  \newcommand{\NormalTok}[1]{#1}
  \newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
  \newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
  \newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
  \newcommand{\RegionMarkerTok}[1]{#1}
  \newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
  \newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
  \newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}

% To pass between YAML and LaTeX the dollar signs are added by CII
\title{Optimizing Lempel-Ziv-Welch for DNA Compression}
\author{Caden Corontzos}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2023}
\division{Mathematical and Natural Sciences}
\advisor{Eitan Frachtenberg}
\institution{Reed College}
\degree{Bachelor of Arts}
%If you have two advisors for some reason, you can use the following
% Uncommented out by CII
% End of CII addition

%%% Remember to use the correct department!
\department{Computer Science}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

% Added by CII
%%% Copied from knitr
%% maxwidth is the original width if it's less than linewidth
%% otherwise use linewidth (to make sure the graphics do not exceed the margin)
\makeatletter
\def\maxwidth{ %
  \ifdim\Gin@nat@width>\linewidth
    \linewidth
  \else
    \Gin@nat@width
  \fi
}
\makeatother

% From {rticles}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
% for Pandoc 2.8 to 2.10.1
\newenvironment{cslreferences}%
  {}%
  {\par}
% For Pandoc 2.11+
% As noted by @mirh [2] is needed instead of [3] for 2.12
\newenvironment{CSLReferences}[2] % #1 hanging-ident, #2 entry spacing
 {% don't indent paragraphs
  \setlength{\parindent}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1 \everypar{\setlength{\hangindent}{\cslhangindent}}\ignorespaces\fi
  % set entry spacing
  \ifnum #2 > 0
  \setlength{\parskip}{#2\baselineskip}
  \fi
 }%
 {}
\usepackage{calc} % for calculating minipage widths
\newcommand{\CSLBlock}[1]{#1\hfill\break}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{#1}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{#1}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}

\renewcommand{\contentsname}{Table of Contents}
% End of CII addition

\setlength{\parskip}{0pt}

% Added by CII

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}

\Acknowledgements{
Thank you to Eitan Frachtenberg, who helped me constantly throughout the whole year writing this. I never thought thesising would be so much fun, and I hope you enjoyed it as much as I did.

Thanks also to all the other staff and faculty who helped me through my four years at Reed. Thank you to B Hunter, who was enormous help to me in getting jobs and opportunites throughout my time here. Thank you also to David Ramirez, whose advice and mentorship I am always grateful for.

Thank you to my family for supporting me, and thank you to all the wonderful people I met at Reed. I have enjoyed this part of my life immensely, and I am excited to see what is next.
}

\Dedication{

}

\Preface{

}

\Abstract{
Large files containing DNA sequences can be bottlenecks for genetic research, as it can be very difficult to store and transfer gigabytes or even terabytes worth of genetic data. In this thesis, we focus on compressing DNA sequences using the Lempel-Ziv-Welch (LZW) compression algorithm. LZW relies heavily on a dictionary type data structure, hindering its speed in compression. We propose a dictionary implementation we call the Direct Map Dictionary. Using the machine instruction \texttt{pext}, we were able to make the Direct Map Dictionary very fast and specifically tailored for DNA. Using this new data structure, we implement a greedy version of LZW we call Three Stream LZW which is able to compress large DNA files with a good compression ratio and notable speed, even when compared to professional tools.
}

	\usepackage{setspace}\onehalfspacing
\usepackage[edges]{forest}
\usepackage{tikz}
	\usepackage{forest}
\usepackage{tikz}
\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}
% End of CII addition
%%
%% End Preamble
%%
%
\begin{document}

% Everything below added by CII
  \maketitle

\frontmatter % this stuff will be roman-numbered
\pagestyle{empty} % this removes page numbers from the frontmatter
  \begin{acknowledgements}
    Thank you to Eitan Frachtenberg, who helped me constantly throughout the whole year writing this. I never thought thesising would be so much fun, and I hope you enjoyed it as much as I did.

    Thanks also to all the other staff and faculty who helped me through my four years at Reed. Thank you to B Hunter, who was enormous help to me in getting jobs and opportunites throughout my time here. Thank you also to David Ramirez, whose advice and mentorship I am always grateful for.

    Thank you to my family for supporting me, and thank you to all the wonderful people I met at Reed. I have enjoyed this part of my life immensely, and I am excited to see what is next.
  \end{acknowledgements}

\chapter*{List of Abbreviations}
\begin{table}[h]
    \centering
    \begin{tabular}{ll}
                \textbf{DNA} & Deoxyribonucleic acid \\
                \textbf{EOF} & End of file \\
                \textbf{LZW} & Lempel Ziv Welch \\
                \textbf{RLE} & Run Length Encoding \\
            \end{tabular}
\end{table}
  \hypersetup{linkcolor=black}
  \setcounter{secnumdepth}{2}
  \setcounter{tocdepth}{2}
  \tableofcontents

  \listoftables

  \listoffigures
  \begin{abstract}
    Large files containing DNA sequences can be bottlenecks for genetic research, as it can be very difficult to store and transfer gigabytes or even terabytes worth of genetic data. In this thesis, we focus on compressing DNA sequences using the Lempel-Ziv-Welch (LZW) compression algorithm. LZW relies heavily on a dictionary type data structure, hindering its speed in compression. We propose a dictionary implementation we call the Direct Map Dictionary. Using the machine instruction \texttt{pext}, we were able to make the Direct Map Dictionary very fast and specifically tailored for DNA. Using this new data structure, we implement a greedy version of LZW we call Three Stream LZW which is able to compress large DNA files with a good compression ratio and notable speed, even when compared to professional tools.
  \end{abstract}

\mainmatter % here the regular arabic numbering starts
\pagestyle{fancyplain} % turns page numbering back on

\hypertarget{background-and-motivations}{%
\chapter{Background and Motivations}\label{background-and-motivations}}

This thesis deals with some high-level topics and uses language specific to compression research. This chapter gives brief summaries and examples of the relevant topics to be discussed so readers of all experience levels can put our results into context.

\hypertarget{what-is-information}{%
\section{What is information?}\label{what-is-information}}

Suppose you had an idea that you wanted to share with another person. Humans have many ways to communicate information: you could send a text message, you could use words, you could use sign language. Regardless of the medium, there is some important idea to get across. Does it matter if the other person gets your message exactly? If someone asks you ``Where library?'', despite the lack of prepositions, you still understand what they mean. So did that person convey any less information than a person who asks ``Where is the library''?
Clearly, information is fundamental to how humans interact and how they understand the world, but defining it proves difficult. For our purposes, let's assume that information is data with significance that makes it worth preserving and conveying.

Information on computers can take many forms, such as text, audio, and video. This information can travel through many channels including the internet, wires, and screens. To maximize the amount of information that can be transmitted through a channel with a limited capacity, we need to encode the information in a way which minimizes its size, while also preserving its essential features. This process is called compression.

\hypertarget{compression-metrics}{%
\section{Compression Metrics}\label{compression-metrics}}

\hypertarget{compression-ratio}{%
\subsection{Compression Ratio}\label{compression-ratio}}

Compression Ratio is the measure of size reduction achieved by a compression algorithm. It is typically expressed as a ratio of the size of the original, uncompressed size (\(OS\)) to the compressed size (\(CS\)).

\[CR = \frac{OS}{CS}\]

So a higher compression ratio means a more effective compression algorithm, and means that we were able to represent more information in less space, allowing for easier storage and transfer.

\hypertarget{compression-time}{%
\subsection{Compression Time}\label{compression-time}}

The run time is also an important part of evaluating the effectiveness of a compression algorithm. Run time is typically defined as the length of time a program takes to complete a task. In this case, we are interested in compression time. Sometimes, if time is constrained, you may care less about saving space. For example, suppose you have the option of two compression algorithms, one with a compression ratio of 2.0, and another with a compression ratio of 2.15. If the one with the higher compression ratio takes twice as long as the other, you may opt for a lower compression ratio to save time.

\hypertarget{memory-usage}{%
\subsection{Memory Usage}\label{memory-usage}}

Memory usage is closely tied with runtime when it comes to compression algorithms. Memory is generally the storage a program uses while it is running. So to reduce our run time and make a more effective compression algorithm, we want to be saving only the most important data that our algorithm needs in order to reduce our memory usage. Throughout this thesis, we will assume that most of memory usage is encapsulated in our measurement of compression time.

\hypertarget{lossy-vs.-lossless-compression}{%
\section{Lossy vs.~Lossless Compression}\label{lossy-vs.-lossless-compression}}

\hypertarget{lossy}{%
\subsection{Lossy}\label{lossy}}

Lossy compression is based on the idea that not all information is vital. For instance, when saving a picture on your computer, your computer may save it in the .jpeg format to save space. Jpegs lose some of the information in the original picture and produce an overall lower quality photo, but the general information in the picture is preserved. Another example is MP3 audio files. MP3 compression discards some of the information and sound quality in exchange for a file that takes up less space, which is often favorable for devices with limited storage like MP3 players and cellphones.

\hypertarget{lossless}{%
\subsection{Lossless}\label{lossless}}

Lossless compression is the compression of data with the goal of preserving all the information in the data so that it can be reproduced perfectly on decompression. As a result, lossless compression algorithms usually don't compress as well as their lossy counterparts. Lossless algorithms are important for use cases in which data needs to be wholly recovered, like scientific data, archiving (e.g a .zip folder), and high end audio recording. Examples of lossless compression algorithms are Huffman Encoding and Lempel-Ziv-Welch.

\hypertarget{classic-compression-algorithms}{%
\section{Classic Compression Algorithms}\label{classic-compression-algorithms}}

\hypertarget{run-length-encoding}{%
\subsection{Run Length Encoding}\label{run-length-encoding}}

Run Length Encoding (RLE) is one of the simplest and most intuitive forms of compression. We can take advantage of redundant runs of characters in a sequence by encoding the number of times each character appears.
Suppose you want to send the following message

\[AAGCTTTTTTTTGGGGGCCCT\]

We can still get this message across without repeating ourself quite as much. When writing a grocery list, you don't write ``egg egg egg egg'', you say ``4 eggs''. RLE uses this same strategy.

\[2A1G1C8T5G3C1T\]

Although not as sophisticated as other methods, RLE is effective when used on applications with large runs of repetitive data. For instance, photos that have solid backgrounds, like a logo, can be effectively compressed by RLE.

\hypertarget{huffman}{%
\subsection{Huffman}\label{huffman}}

Huffman Encoding is a lossless compression algorithm that assigns variable length code to certain symbols in the data. The goal is to assign short codes to frequently appearing symbols and longer codes to less frequent symbols.

Suppose we have a message ``ACAGGATGGC''. We can calculate the frequency of each letter by counting the number of times each letter shows up and dividing by the total number of letters

Then, we can use the frequencies to build a tree, which will assign short codes for frequent letters and longer code for less frequent letters. The more frequent characters occur higher up in the tree, giving them a shorter length. The less frequent characters occur farther down on the tree. If you follow the branches in Figure \ref{fig:huffman} down to a letter, it will tell you the code associated with that letter.
\begin{figure}[h]\centering


\tikzset{iv/.style={draw,fill=red!50,circle,minimum size=20pt,inner
sep=0pt,text=black},ev/.style={draw,fill=yellow,rectangle,minimum
size=20pt,inner sep=0pt,text=black}}
\begin{forest}
for tree={where n children={0}{ev}{iv},l+=8mm,
if n=1{edge label={node [midway, left] {0} } }{edge label={node [midway, right] {1} } },}
[
 [G]  
 [
  [A]
  [
    [C]
    [T]
  ]
 ] 
] 
\end{forest}
\caption{Example Huffman tree.}
\label{fig:huffman}
\end{figure}
So \(G=0\), \(A = 10\), \(T=111\) and \(C=110\). Notice that none of the encodings are prefixes of one another, which makes it unambiguous in decoding.

So our message would be encoded to \(1011010001011100110\).

\hypertarget{arithmetic}{%
\subsection{Arithmetic}\label{arithmetic}}

Arithmetic encoding is another lossless compression algorithm that uses probability to assign codes to symbols in the message. Unlike Huffman, arithmetic encoding assigns a single code to the whole message, rather than separate codes for each symbol.

Here is a simple example. Say we want to encode a string of characters ``ACGGT''. Arithmetic Encoding also requires the encoder and decoder know the probabilities of each of the characters that could possibly be in the message. Let's say the probabilities of each symbol in the message are:
\begin{itemize}
\tightlist
\item
  \(P(A) = a_1 = 2/10\)
\item
  \(P(C) = a_2 = 2/10\)
\item
  \(P(G) = a_3 = 4/10\)
\item
  \(P(T) = a_4 = 2/10\)
\end{itemize}
We want to represent the message as a fractional number between 0 and 1. We will divide the interval {[}0,1{]} into sub intervals using the probabilities of each character in the message. That way, each symbol is represented by the sub-interval that corresponds to its probability.
\begin{figure}

{\centering \includegraphics[width=0.9\linewidth]{figure/arithmeticencoding} 

}

\caption{Example of arithmetic encoding.}\label{fig:arithmeticencoding}
\end{figure}
Since `A' comes first, we divide {[}0,1{]} into {[}0.0,0.2) since \(P(A) = 2/10\). Since `C' is next, we divide the new fraction of the space {[}0.0, 0.2) into chunks based on the probabilities of the characters and choose the chunk corresponding to `C'. Figure \ref{fig:arithmeticencoding} shows this process through the whole string ``ACGGT''. We end up with an interval from 0.0688 to 0.06752 (Singhaniya, 2021).

So any number in the interval can be used to represent our message as long as the decoder knows the probabilities that we used to encode it.

\hypertarget{lempel-ziv-welch}{%
\subsection{Lempel-Ziv-Welch}\label{lempel-ziv-welch}}

Lempel-Ziv-Welch is another lossless compression algorithm. When compressing, LZW builds a dictionary of codewords. A dictionary is a key value system: when you give it a key, it either gives you a value or tells you that the key does not exist. In this case, the keys are strings of characters and the values are codewords, which are numbers meant to represent those strings.

Here is a simple example. Suppose we decide that `AA' = 1. In other words, any time we see the number 1, we know that it actually means `AA'. So if we wanted to send a message

\[\text{AACAAC}\]

We could just say

\[\text{1C1C}\]

As long as the person recieving this message understood that `AA' = 1, they could easily determine what I meant. LZW works in the same way. This rule that we started with, that `AA' = 1, is essentially a dictionary. It holds keys and values. To make the example more complex, suppose we decide that \{`AA' = 1, `C' = 2\}. We could encoding the message ``AACAAC'' as

\[\text{1212}\]

As you can see, our message is getting shorter. The problem is, how do we decide what elements start in our dictionary? Well, we can start with a small dictionary, and build up a larger dictionary as we go along. Let me explain. Suppose we are still trying to encode ``AACAAC''. Assume we start with the dictionary \{`AA' = 1\}. We can scan through the message from left to right, looking for instances of `AA' and replace them. We see ``AAC'' at the beginning of the string. We can encode this as ``1C''. Here is our message so far

\[\text{1CAAC}\]

But what if we see the string ``AAC'' again? Well, when we output ``1C'', we can add this new string, ``AAC'', to our dictionary. So our dictionary now looks like \{`AA' = 1, `AAC' = 2\}. Notice we just assign the new string whatever number we left off on while creating the dictionary.

Now, as we get to the end of the message, we see ``AAC'' again. Now that we have a codeword for ``AAC'', we can just output 2. So our final message looks like

\[\text{1C2}\]

The reason the final meassage is not ``22'' is because we didn't start with `AAC' in our dictionary. If we output ``22'', the decoder won't know what 2 means. We leave behind the ``1C'' to indicate to the decoder that `ACC' should be added to the dictionary.

So suppose we want to decode the message ``1C2''. The decoder, when it see ``1C'', will know that that means ``take the string that has codeword 1, add on the character `C', and add that new string to your dictionary''. So they will add `AAC' = 2 to their dictionary. When they see the following 2, they know that 2 = `AAC', so they are able to decode the message. In this way, we can build up the dicionary as we encode and decode, and as long as we start with the same starting dictionary, the message will always be preserved. You can imagine continuing this process throughout the message. We find the longest string that is in our dictionary. We output the codeword for that string, and then the character that follows that string. Then, we add that new sequence to our dictionary. This way, we can build up runs of longer and longer strings, and thus we are able to replace longer runs of characters with codewords.

Here is a more complex example. We may be sending messages with the characters `A', `C', `T', `G', so we can start by assigning those strings codewords. So our dictionary will start as \{`A' = 0, `C' = 1, `T' = 2, `G' = 3\}. So in other words, `A' is synonymous with 0, `C' is synonymous with 1, and so on. Say we compress to send the message

\[\text{AAGGAATCC}\]

When we compress, we start at the beginning of the message and scan through. We ask ourselves, ``Is `A' in our dictionary?''

\[\textbf{A} \text{AGGAATCC}\]

Well, we started with `A' in our dictionary, so we can check in the dictionary and confirm it is there. We then add on the next character in the sequence and ask ``Is''AA'' in our dictionary?''

\[\textbf{AA} \text{GGAATCC}\]

We have not seen ``AA'' before, so we should add it to our dictionary. So now our dictionary looks like this \{`A' = 0, `C' = 1, `T' = 2, `G' = 3, `AA' = 4\}. Next time we see ``AA'', we know it is associated with the codeword 4. To indicate this, in our resulting string we will output the code for ``A'', the part of the string we've seen before, and the character ``A''.

So the encoded string will look something like

\[\text{0A....}\]
\begin{table}
\begin{tabular}{ | c | c | p{.4\textwidth} | c | }
\hline
\textbf{Step} & \textbf{Input String} & \textbf{Dictionary State} & \textbf{Encoded String} \\
\hline
1 & \textbf{A}AGGAATCC & {A: 0, G: 1, T: 2, C: 3} & - \\
\hline
2 & \textbf{AA}GGAATCC & {A: 0, G: 1, T: 2, C: 3} & 0A \\
\hline
3 & AA\textbf{G}GAATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4} & 0A  \\
\hline
4 & AA\textbf{GG}AATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4} & 0A1G   \\
\hline
5 & AAGG\textbf{A}ATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5} & 0A1G \\
\hline
6 & AAGG\textbf{AA}TCC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6} & 0A1G  \\
\hline
7 & AAGG\textbf{AAT}CC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6} & 0A1G4T  \\
\hline
8 & AAGGAAT\textbf{C}C & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6, AAT: 7} & 0A1G4T \\
\hline
9 & AAGGAAT\textbf{CC} & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6, AAT: 7} & 0A1G4T3C \\
\hline
10 & AAGGAATCC & {A: 0, G: 1, T: 2, C: 3, AA: 4, GG: 5, GA: 6, AAT: 7, CC: 8} & 0A1G4T3C \\
\hline
\end{tabular}
\caption{ An example of LZW ran on the input "AAGGAATCC".}
\label{tab:lzwexample}
\end{table}
Table \ref{tab:lzwexample} illustrates this example fully.

When we are decoding, we start with the same dictionary. We see ``0A'' and know that that means ``Take the string that has codeword 0, add on the character `A' and add that new string to the dictionary''. We would add ``AA'' to the dictionary and assign it to our next available codeword, 4. Again, while decoding, we are able to build up the same dictionary as was used for encoding, as long as we use the same starting dictionary.
LZW has several convenient properties:
\begin{itemize}
\tightlist
\item
  When we send this encoding to someone else, we don't need to send a ``codebook'' (our dictionary). They are able to build it up themselves as they decode.
\item
  There only needs to be one run over the data to encode and decode. This means that the run time of the algorithm should increase linearly with the length of the input.
\item
  As runs get longer, we will start to see more and more repeating patterns, and replacing them with codewords will become more and more effective.
\end{itemize}
To decode, we can simply start with the same dictionary. We see codewords followed by one character, so we decode that codeword and add the character to the end. Since the decoder continues to look for codewords, we need some special character at the end of our encoding to let the decoder know when the message is done.

Now that we have laid out how the algorithm works, we can get more specific for our use case. For us, the input is a file on the computer, and the output is also a file (hopefully a smaller one). We read all the characters in the file, encode them, and put them into a new file. Then, when we want to decode, we read the encoded file and output the decoded characters. Again, LZW is lossless, so the original file and the decoded file should be identical.

Here is some example pseudocode
\begin{verbatim}
LZWEncode(input):

    Dictionary dictionary; // where we store our string => codeword mappings
    dictionary.inititalize; // initialzie the dictionary with single characters

    codeword; // the unique numbers we assign strings
    output; // where we output the encoded characters

    currentBlock = first character of input;
    for every nextCharacter in the input:
        
        // this function returns the longest string we've already seen
        // starting at our current place in the input
        nextLongestRun = findLongestInDict();

        if currentCharacter + nextLongestRun.length > input.length:
            break;
            
        // output the code of the next longest run and next character
        code = dictionary.lookup(nextLongestRun);
        nextCharacter = input[nextLongestRun + 1]
        output(code);
        output(nextCharacter);

        
        dictionary.add(currentBlock + nextCharacter, map it to codeword);

        codeword = codeword + 1;
        input = input + nextLongestRun + 1;


    output(special end of file character);
\end{verbatim}
The decoding is much simpler. The only real difference is that we are now mapping codewords to strings, since the encoded string contains codewords.
\begin{verbatim}
LZWDecode(input):

    Dictionary dictionary; // where we store our codeword => string mappings
    dictionary.inititalize; // initialzie the dictionary with single characters

    codeword; // the unique numbers we assign strings
    result; // where we output the encoded characters

    while we don't see the end of file character:

        codewordFound = input.readCodeword()
        nextCharacter = input.readCharacter()

        sequence = dictionary.lookup(codewordFound) + nextCharacter
        result.output(sequence)

        dictionary.add(sequence = codeword)
        codeword = codeword + 1;
\end{verbatim}
This is the basic strategy we will start with for our LZW algorithm. In the next chapter, we will go over parts of the algorithm in depth in C++. Here is a quick summary of terms repeated throughout the next two chapters.
\begin{itemize}
\tightlist
\item
  \texttt{dictionary}: a key-value system. Like a real dictionary holds words and their corresponding definitions, our dictionary holds codewords and their corresponding strings of characters. In C++, this is called a \texttt{std::unordered\_map} and uses a hash table, but the concept is the same. During this thesis, we will use \texttt{std::unordered\_map} and Standard Dictionary interchangeably.
\item
  \texttt{codeword}: a number that is assigned to a string in our encoding and decoding. The it is called a \texttt{codeword} because it is a code for the string it represents.
\item
  \texttt{run}: the next run of characters in our input that are already in our dictionary. So if we are encoding ``ACTG'', and ``A'', ``AC'', and ``ACT'' are in the dictionary but ``ACTG'' is not, we have a run of 3.
\item
  \texttt{EOF}: end of file, the special character that we need to output at the end of the encoding.
\end{itemize}
\hypertarget{related-work}{%
\section{Related Work}\label{related-work}}

Deoxyribonucleic acid (DNA) is the building block of life. In four letters (A, C, G, and T) called nucleotides, DNA holds the genetic information required for reproduction in all living organisms. DNA can be billions of characters long, which can mean gigabytes or terabytes of storage on a computer.
The idea of compressing DNA is not novel, nor is the idea of using LZW for this purpose. DNA compression is a significant research area in the intersection of bioinformatics, computer science, and mathematics.

There have been several attempts to optimize LZW by computer science researchers. One paper made use of multiple indexed dictionaries in order to speed up the compression process (Keerthy, 2019). The concept is simple: rather than a single large dictionary, have multiple dictionaries, one for each possible string length. This allows each of the dictionaries to grow more slowly, allowing accesses to be faster. This paper also used genomic data to gather their metrics and compared their algorithm to other popular DNA compression techniques, which makes it particularly relevant for this thesis.

Another paper used simple parallelization techniques to improve compression speed (Pani, Mishra, \& Mishra, 2012). Rather than compressing the whole file linearly, the researches broke the file into portions and compressed them with LZW in parallel, which greatly increased the compression speed at the cost of a reduced compression ratio.

Other researchers made use of Chinese Remainder Theorem (CRT) to augment Lempel-Ziv-Welch (Ibrahim \& Gbolagade, 2020). They saw great reduction in compression time without compromising compression ratio, although these results could not be verified. The details of their implementation were not clear from the paper. We tried multiple different methods of utilizing CRT given the pseudocode in their paper, but we were not able to achieve similar results. We reached out to the authors, but we were not able to further our progress on this method and thus it is not used in this thesis.

DNA-specific compression algorithms have also been a growing subsection of computer science for decades. These papers do not focus on LZW, but they do consider some similar methods.

One of the first papers exploring this was published in 1994 (Grumbach \& Tahi, 1994). It proposes an algorithm called \texttt{biocompress2}, expanding on a previous paper by the same author. They focus on encoding palindromes in DNA sequences, which allows them to achieve an above average compression ratio, though performance is not evaluated. This paper has been cited by many following papers sparking interest in DNA compression, and the collection of sequences that it uses for algorithm comparison is used in this thesis.

Chen et al.~proposed an algorithm called \texttt{GenCompress}, which uses approximate matching (Chen, Kwong, \& Li, 2001). It matches sequences of nucleotides to sequences already seen in the file, and maps those sequences using various edits to turn one sequence into another. They are able to achieve a great compression ratio with this relative matching method, although their technique is computationally expensive.

In 2007, Cao et al.~published a paper detailing another algorithm, \texttt{XM}, which uses statistical methods to try and predict the next character while encoding and decoding (Cao, Dix, Allison, \& Mears, 2007). This method was found to outperform both \texttt{biocompress2} and \texttt{GenCompress} in terms of compression ratio.

A paper in 2021 uses segmenting and partitioning to create a parallel compression algorithm called \texttt{genozip} (Lan, Tobler, Souilmi, \& Llamas, 2021). This paper also published their code online in a convenient format which allows others to compare and test their implementation.

As a whole, these papers give us some guidance in terms of where to aim our research. Most of them boast great compression ratios, but their methods can be very computationally intensive in some cases, and thus, slow. We will aim to use previous research on LZW to make a very fast implementation for DNA sequences, then try and use characteristics of the sequences to improve compression ratio. Our hope isn't necessarily to create the best compression ratio out of all these methods, but to make a fast LZW implmentation with a respectable compression ratio. If we are able to make the algorithm very fast, it may be preferable to these other algorithms if the file is very large.

\hypertarget{optimizing-lzw-approach}{%
\chapter{Optimizing LZW: Approach}\label{optimizing-lzw-approach}}

To restate the goal of this thesis, we seek to optimize LZW for use in compression of DNA. I chose to write code C++. A majority of the work in this thesis involved creating, refactoring, and reconfiguring code to improve performance. The various methods we used for this process are discussed throughout the chapter.

While we may not end up creating the best DNA compression ratio available, the objective is to explore the boundaries of LZW and to tailor it as best we can for the task of DNA compression. As you will see, the algorithm has limitations.

Our strategy was as follows:
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Implement a basic version of LZW in C++.
\item
  Optimize the algorithm. Make it as fast as possible, and specifically focus on compressing DNA.
\item
  Once the algorithm is fast, try to entropy encode (Huffman, arithmetic encoding, etc) the compressed files to improve compression ratio.
\end{enumerate}
\hypertarget{corpora}{%
\section{Corpora}\label{corpora}}

Most compression papers make use of a corpus, which is a collection of files to run a compression algorithm on in order to evaluate performance and to compare the algorithm to others available.
\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:corpus1filesfig}Corpus 1}\tabularnewline
\toprule()
Name & bytes \\
\midrule()
\endfirsthead
\toprule()
Name & bytes \\
\midrule()
\endhead
chmpxx & 121024 \\
chntxx & 155844 \\
hehcmv & 229354 \\
humdyst & 38770 \\
humghcs & 66495 \\
humhbb & 73308 \\
humhdab & 58864 \\
humprtb & 56737 \\
mpomtcg & 186609 \\
mtpacga & 100314 \\
vaccg & 191737 \\
\bottomrule()
\end{longtable}
In the world of DNA compression, there are several academic papers on the subject. As mentioned in Chapter 1, one of the first and most popular of the papers was published in 1994. The selection of DNA sequences used in the paper have become an informal corpus for the subject of DNA compression, cited by more than thirty publications (Grumbach \& Tahi, 1994). We call this Corpus 1, summarized in Table \ref{tab:corpus1filesfig}.

Another, newer paper aimed to create a corpus specifically for compressing DNA (Pratas \& Pinho, 2018). They put together a corpus of DNA sequences for this purpose. Since the papers publishing, it has been cited by several DNA compression studies.
\begin{longtable}[]{@{}cc@{}}
\caption{\label{tab:corpus2filesfig}Corpus 2}\tabularnewline
\toprule()
Name & bytes \\
\midrule()
\endfirsthead
\toprule()
Name & bytes \\
\midrule()
\endhead
AeCa & 1591049 \\
AgPh & 43970 \\
BuEb & 18940 \\
DaRe & 62565020 \\
DrMe & 32181429 \\
EnIn & 26403087 \\
EsCo & 4641652 \\
GaGa & 148532294 \\
HaHi & 3890005 \\
HePy & 1667825 \\
HoSa & 189752667 \\
OrSa & 43262523 \\
PlFa & 8986712 \\
ScPo & 10652155 \\
YeMi & 73689 \\
\bottomrule()
\end{longtable}
The dataset in Table \ref{tab:corpus2filesfig}, which we call Corpus 2, is publicly available at \url{https://tinyurl.com/DNAcorpus}.

\hypertarget{evaluating-performance}{%
\section{Evaluating Performance}\label{evaluating-performance}}

Evaluating performance of a program is difficult. There is a notion of theoretical run time, but on an actual computer there are many processes running in the background, so it can be hard to get a consistent reading on performance.

To attempt to counteract this, we ran the function on the same file multiple times, and took the median of the compression and decompression times for all the runs. Also, for any graphs or tables in this thesis, the stats were taken on the same computer. Our test machine was a PC with an AMD Ryzen 5 5500 processor 32 gigabytes of RAM running Ubuntu version 20.04. We also did our best to mitigate any other programs running on the computer at the time of data collection to prevent interference.

Most graphs in this section will refer to throughput and average compression time. Throughput is defined as the number of megabytes that the program processes per second. The higher the throughput, the more efficient the algorithm. The average compression time is taken as the time of compression divided by the total number of files. All graphs use all the files from both corpora unless stated otherwise.

\hypertarget{a-starting-point}{%
\section{A Starting Point}\label{a-starting-point}}

As stated previously, we thought it was best to get a working implementation of LZW in C++ on regular text files, and then optimize it for DNA. We want to try various techniques tried by researches in the field, but it is important to have a fast baseline from which we can compare and improve upon. If the initial implementation is inefficient, it makes it harder to tell if the different techniques we have are affecting performance.

This section chronologically covers some milestones reached when implementing an efficient initial version of LZW. These milestones are compared to one another at the end of the section.

\hypertarget{growing-codewords-and-bit-output}{%
\subsection{Growing Codewords and Bit Output}\label{growing-codewords-and-bit-output}}

When reading files on the computer, most characters are stored as bytes, which are made up of 8 bits. For instance, \texttt{01000001} stands for the letter `A' in ASCII encoding, the typical encoding scheme for plain English text. Numbers in binary are simpler to display, so \texttt{00000001} is 1, \texttt{00000010} is 2, and so on.

But if we are translating numbers to binary, we don't need all of the bits in a byte. In binary, \texttt{1} is the same as \texttt{01} is the same as \texttt{00000000000001}. So when we are outputting codewords for LZW, we don't necessarily need to output a whole byte. We can have growing codewords.

As the number of codewords grows, the number of bits needed to represent them also grows. We need to always have enough bits to represent our current largest codeword, otherwise the decoder won't know how many bits each codeword is. So if we are on codeword 8, we need 4 bits since 8 is \texttt{1000}. We will need 4 bits until our codeword reaches 16, which is \texttt{10000} in binary. From then on, we will need to use 5 bits for every codeword. As our dictionary grows, we can grow the number of bits needed to display a codeword and save a lot of space in our compressed document.

So we needed a method of outputting bits one by one, and reading in bits one by one. This is not something that is supported in C++ on its own. We were able to create this functionality by defining a class.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// BitInput: Read a single bit at a time from an input stream.}
\CommentTok{// Before reading any bits, ensure input stream still has valid input}
\KeywordTok{class}\NormalTok{ BitInput }\OperatorTok{\{}
 \KeywordTok{public}\OperatorTok{:}
  \CommentTok{// Construct with an input stream}
\NormalTok{  BitInput}\OperatorTok{(}\AttributeTok{const} \DataTypeTok{char}\OperatorTok{*}\NormalTok{ input}\OperatorTok{);}

\NormalTok{  BitInput}\OperatorTok{(}\AttributeTok{const}\NormalTok{ BitInput}\OperatorTok{\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}
\NormalTok{  BitInput}\OperatorTok{(}\NormalTok{BitInput}\OperatorTok{\&\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}

  \CommentTok{// Read a single bit (or trailing zero)}
  \CommentTok{// Allowed to crash or throw an exception if past end{-}of{-}file.}
  \DataTypeTok{bool}\NormalTok{ input\_bit}\OperatorTok{();}

  \DataTypeTok{int}\NormalTok{ read\_n\_bits}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ n}\OperatorTok{);}
\OperatorTok{\}}

\CommentTok{// BitOutput: Write a single bit at a time to an output stream}
\CommentTok{// Make sure all bits are written out when exiting scope}
\KeywordTok{class}\NormalTok{ BitOutput }\OperatorTok{\{}
 \KeywordTok{public}\OperatorTok{:}
  \CommentTok{// Construct with an input stream}
\NormalTok{  BitOutput}\OperatorTok{(}\BuiltInTok{std::}\NormalTok{ostream}\OperatorTok{\&}\NormalTok{ os}\OperatorTok{);}

  \CommentTok{// Flushes out any remaining bits and trailing zeros, if any:}
  \OperatorTok{\textasciitilde{}}\NormalTok{BitOutput}\OperatorTok{();}

\NormalTok{  BitOutput}\OperatorTok{(}\AttributeTok{const}\NormalTok{ BitOutput}\OperatorTok{\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}
\NormalTok{  BitOutput}\OperatorTok{(}\NormalTok{BitOutput}\OperatorTok{\&\&)} \OperatorTok{=} \ControlFlowTok{default}\OperatorTok{;}

  \CommentTok{// Output a single bit (buffered)}
  \DataTypeTok{void}\NormalTok{ output\_bit}\OperatorTok{(}\DataTypeTok{bool}\NormalTok{ bit}\OperatorTok{);}

  \DataTypeTok{void}\NormalTok{ output\_n\_bits}\OperatorTok{(}\DataTypeTok{int}\NormalTok{ bits}\OperatorTok{,} \DataTypeTok{int}\NormalTok{ n}\OperatorTok{);}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
So when we are encoding and need to output a codeword, we can \texttt{output\_n\_bits}, where \texttt{n} is the number of bits needed to display our greatest codeword. When decoding, we can just \texttt{read\_n\_bits}.

\hypertarget{getting-eof-to-work}{%
\subsection{Getting EOF to work}\label{getting-eof-to-work}}

One of the very early issues with the implementation was how to denote the end of a file. The early implementation would work for some files, but for others the very last part of the file would be lost after encoding and then decoding.

The solution was to reserve a special codeword to mark the end of the file.
As discussed in Section \ref{lempel-ziv-welch}, if we don't mark the end of the file, the decoder won't be able to know when to stop decoding. So the decoding algorithm goes along reading a file. It builds up a current string character by character, adding the character to the string and checking if it has seen that sequence before. Once it finds the end of file, we stop and output the EOF codeword.

The problem was, what about what is left over? When compressing, the algorithm could be part of the way through a search when it reaches the end of the file. Suppose we are reading a file, and the file ends with ``ACCT''. If ``A'' is in the dictionary, we see if ``AC'' is in the dictionary, and so on. This leaves us with three possible cases when we reached the end of the file
\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  ``ACC'' was in the dictionary but ``ACCT'' was not. This means we can output the codeword for ``ACC'', follow it by the character ``T'', and we are done. This is the ideal scenario, because nothing is left over when we output the EOF codeword.
\item
  ``ACCT'' was in the dictionary: This means we have one more codeword to output, but since we reached the end of the file, we never got to output it.
\item
  ``AC'' was in the dictionary, but ``ACC'' was not: in this case, we would output the codeword for ``AC'' output the character ``C'', and then start looping again starting at ``T''. But we reach the end of the file, so we output EOF before outputting T.
\end{enumerate}
We solved this issue by adding 2 extra bits after the EOF codeword. These bits denote the case that occurred
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// after we\textquotesingle{}ve encoded, we either have }
\CommentTok{// no current block (case 0)}
\CommentTok{// we have a current block that is a single character (case 1)}
\CommentTok{// otherwise we have a current block \textgreater{} 1 byte (default)}
\ControlFlowTok{switch} \OperatorTok{(}\NormalTok{currentBlock}\OperatorTok{.}\NormalTok{length}\OperatorTok{())\{}
\ControlFlowTok{case} \DecValTok{0}\OperatorTok{:}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{false}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{false}\OperatorTok{);}
    \ControlFlowTok{break}\OperatorTok{;}
\ControlFlowTok{case} \DecValTok{1}\OperatorTok{:}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{false}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{true}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_n\_bits}\OperatorTok{((}\DataTypeTok{int}\OperatorTok{)}\NormalTok{ currentBlock}\OperatorTok{[}\DecValTok{0}\OperatorTok{],}\NormalTok{ CHAR\_BIT}\OperatorTok{);}
    \ControlFlowTok{break}\OperatorTok{;}
\ControlFlowTok{default}\OperatorTok{:}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{true}\OperatorTok{);}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_bit}\OperatorTok{(}\KeywordTok{true}\OperatorTok{);}

    \DataTypeTok{int}\NormalTok{ code }\OperatorTok{=}\NormalTok{ dictionary}\OperatorTok{[}\NormalTok{currentBlock}\OperatorTok{];}
\NormalTok{    bit\_output}\OperatorTok{.}\NormalTok{output\_n\_bits}\OperatorTok{(}\NormalTok{code}\OperatorTok{,}\NormalTok{ codeword\_size}\OperatorTok{);}
    \ControlFlowTok{break}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
So when the decoder is reading and encounters the EOF codeword, it can look at the next two bits to see if anything is left over.

At this point, there was a working implementation that was able to compress and decompress files.

\hypertarget{using-constants}{%
\subsection{Using Constants}\label{using-constants}}

The early version of the code was not clean. There were hard coded variables, unspecified integer types, and generally messy naming conventions that made the code difficult to read and debug.

The next major step in the code was to start using constants for everything, including
\begin{itemize}
\tightlist
\item
  \texttt{STARTING\_CODEWORD}: What codeword we should start at
\item
  \texttt{EOF\_CODEWORD}: What we should output when we reach end of file
\item
  \texttt{STARTING\_DICT\_SIZE}: At this stage, we had a starting dict size of 256 to hold all possible bytes, but later we will specialize for DNA
\end{itemize}
It also made sense to start using a specific type for codewords. At this stage, we opted for a 32 bit unsigned integer.

There are several tools at a developer's disposal when looking to debug and optimize code. One tool used for this thesis was \texttt{callgrind} which is a profiling tool. Profiling tools are used to look at how your code works, where the bottlenecks are, and what can be changed/improved for the performance of your code.

\texttt{Callgrind} in particular is a tool which associates assembly instructions to lines of code, indicating which lines take a lot of instructions and which take less. For those unfamiliar, assembly instructions are what code is converted into so that it can be ran on your computer's processor. In general, more instructions means that code takes longer to run.

The callgrind output drew attention to one particular part of the code. A C++ \texttt{unordered\_map} uses iterators, basically pointers into the dictionary. If an entry is not present in the dictionary, the \texttt{find()} function will return a iterator to the end of the dictionary.

The check for this in our algorithm looked like this.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// if we\textquotesingle{}ve already seen the sequence, keep going}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ dictionary}\OperatorTok{.}\NormalTok{end}\OperatorTok{())\{}
\NormalTok{    currentBlock }\OperatorTok{=}\NormalTok{ currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
Here is the \texttt{callgrind} output for that line. The numbers on the left indicate the number of instructions, and the arrows indicate other functions that this line relies on.
\begin{verbatim}
105,030,135 ( 0.18%)          if (dictionary.find(currentBlock + next_character) != dictionary.end()){
13,537,450,317 (22.83%)  => /usr/include/c++/9/bits/basic_string.h
11,653,779,430 (19.65%)  => /usr/include/c++/9/bits/unordered_map.h
2,108,383,120 ( 3.56%)  => /usr/include/c++/9/bits/basic_string.h
956,941,242 ( 1.61%)  => /usr/include/c++/9/bits/unordered_map.h
241,180,314 ( 0.41%)  => /usr/include/c++/9/bits/hashtable_policy.h
\end{verbatim}
So the if statement requires a lot of calls to outside functions, which results in billions of assembly instructions. This line is taking a significant amount of instructions, and it needs to look up the \texttt{end()} of the dictionary each time it is ran. If save that iterator in a variable called \texttt{end}, we can save a significant amount of instructions.
\begin{verbatim}
89,470,115 ( 0.61%)          if (dictionary.find(currentBlock + next_character) != end ){
3,353,009,053 (22.78%)  => /usr/include/c++/9/bits/basic_string.h
2,833,786,025 (19.26%)  => /usr/include/c++/9/bits/unordered_map.h
420,120,704 ( 2.85%)  => /usr/include/c++/9/bits/basic_string.h
50,570,065 ( 0.34%)  => /usr/include/c++/9/bits/hashtable_policy.h
\end{verbatim}
There are several potential issues with what \texttt{callgrind} attempts to do by associating machine instructions with individual lines. It is difficult to associate instructions with a single line of code. Some lines are interdependent, and assembly often behaves differently than the code that produces it. Modern compilers are also very advanced, and sometimes small optimizations like this are done by the compiler automatically.

Despite these issues, this change was still worth making, if not to save time then for sake of clarity and readability of the code. Also, despite the inaccuracy of \texttt{callgrind}, like many profiling tools, its job is not necessarily to provide exact measurements of code performance, but to indicate trouble spots in the code which can be improved.

\hypertarget{extraneous-string-concatenations}{%
\subsection{Extraneous String Concatenations}\label{extraneous-string-concatenations}}

The LZW algorithm is built on iteration: we go through each character, adding it to our current block. If we've seen that current block before, we keep going. If not, we add that block to the dictionary and start over.

I noticed from the \texttt{callgrind} output was that a lot of time/instructions are being spent on string concatenation. In general, string concatenations in most languages, including C++, have a lot of overhead. A lot of implementations of string concatenation involve creating a new string every time you concatenate two existing strings, which can have a significant performance penalty.

In this version of the algorithm, every time we have already seen a sequence, we have to concatenate a character. I noticed that I was doing this concatenation multiple times without needing to. You may have noticed this extraneous concatenation in the if statement in Section \ref{using-constants}.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// we concatenate the strings here}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ end}\OperatorTok{)\{}
    \CommentTok{// and here}
\NormalTok{    currentBlock }\OperatorTok{=}\NormalTok{ currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\OperatorTok{\}}
\ControlFlowTok{else}\OperatorTok{\{}

    \CommentTok{// other code here ommitted}


    \CommentTok{// and here! }
\NormalTok{    dictionary}\OperatorTok{[}\NormalTok{currentBlock }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{]} \OperatorTok{=}\NormalTok{ codeword}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
If we just save \texttt{currentBlock\ +\ next\_character} into a new variable, that will prevent doing the concatenation 2 extra times.
\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{// save concatenation here}
\BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{string\_seen\_plus\_new\_char }\OperatorTok{=}\NormalTok{ current\_string\_seen }\OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{)} \OperatorTok{!=}\NormalTok{ end}\OperatorTok{)\{}
\NormalTok{    current\_string\_seen }\OperatorTok{=}\NormalTok{ string\_seen\_plus\_new\_char}\OperatorTok{;}
\OperatorTok{\}}
\ControlFlowTok{else}\OperatorTok{\{}

\CommentTok{// other code omitted here}

\NormalTok{    dictionary}\OperatorTok{[}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{]} \OperatorTok{=}\NormalTok{ codeword}\OperatorTok{;}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
\hypertarget{dictionary-lookups}{%
\subsection{Dictionary Lookups}\label{dictionary-lookups}}

Dictionary lookups can be expensive, especially with the Standard Dictionary. We learned from \texttt{callgrind} that along with string operations, our program spent a large fraction of the total run time doing these lookups.

We looked for ways to reduce the volume of lookups. At the time, the algorithm looked up the current string and the next character in the dictionary. If that string was in the dictionary, it keeps adding characters. If the string was not in the dictionary, then the algorithm outputs the codeword for the current string.

But, the current string on this iteration is just the current string from the last iteration, plus one character. So when we were on the previous iteration of the loop, we could save that lookup and prevent a second lookup.

Here is some of the code to further explain this point.
\begin{Shaded}
\begin{Highlighting}[]
\ControlFlowTok{while}\OperatorTok{(}\NormalTok{next\_character }\OperatorTok{!=}\NormalTok{ EOF}\OperatorTok{)\{}

    \CommentTok{// code omitted}


    \CommentTok{// if we\textquotesingle{}ve already seen the sequence, keep going}
    \BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{string\_seen\_plus\_new\_char }\OperatorTok{=}\NormalTok{ current\_string\_seen }
                                                \OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
    \CommentTok{// save this iterator\textasciigrave{}}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{)} \OperatorTok{!=} 
\NormalTok{                                            not\_in\_dictionary}\OperatorTok{)\{}
\NormalTok{        current\_string\_seen }\OperatorTok{=}\NormalTok{ string\_seen\_plus\_new\_char}\OperatorTok{;}
    \OperatorTok{\}}
    \ControlFlowTok{else}\OperatorTok{\{}

        \CommentTok{// shouldn\textquotesingle{}t look up again}
        \DataTypeTok{int}\NormalTok{ code }\OperatorTok{=}\NormalTok{ dictionary}\OperatorTok{[}\NormalTok{current\_string\_seen}\OperatorTok{];}


        \CommentTok{// code omitted}
    \OperatorTok{\}}
\NormalTok{    next\_character }\OperatorTok{=}\NormalTok{ input}\OperatorTok{.}\NormalTok{get}\OperatorTok{();}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
We can save that lookup, like so.
\begin{Shaded}
\begin{Highlighting}[]

\ControlFlowTok{while}\OperatorTok{(}\NormalTok{next\_character }\OperatorTok{!=}\NormalTok{ EOF}\OperatorTok{)\{}

    \CommentTok{// code ommitted}

    \CommentTok{// if we\textquotesingle{}ve already seen the sequence, keep going}
    \BuiltInTok{std::}\NormalTok{string}\OperatorTok{ }\NormalTok{string\_seen\_plus\_new\_char }\OperatorTok{=}\NormalTok{ current\_string\_seen }
                                                \OperatorTok{+}\NormalTok{ next\_character}\OperatorTok{;}
\NormalTok{    codeword\_seen\_now }\OperatorTok{=}\NormalTok{ dictionary}\OperatorTok{.}\NormalTok{find}\OperatorTok{(}\NormalTok{string\_seen\_plus\_new\_char}\OperatorTok{);}
    \ControlFlowTok{if} \OperatorTok{(}\NormalTok{codeword\_seen\_now }\OperatorTok{!=}\NormalTok{ not\_in\_dictionary }\OperatorTok{)\{}
\NormalTok{        current\_string\_seen }\OperatorTok{=}\NormalTok{ string\_seen\_plus\_new\_char}\OperatorTok{;}
\NormalTok{        codeword\_seen\_previously }\OperatorTok{=}\NormalTok{ codeword\_seen\_now}\OperatorTok{;} \CommentTok{// save codeword }
    \OperatorTok{\}}
    \ControlFlowTok{else}\OperatorTok{\{}

        \CommentTok{// on the next iteration, we use it here}
        \DataTypeTok{int}\NormalTok{ code }\OperatorTok{=}\NormalTok{ codeword\_seen\_previously}\OperatorTok{{-}\textgreater{}}\NormalTok{second}\OperatorTok{;}

        \CommentTok{// code omitted}

    \OperatorTok{\}}
\NormalTok{    next\_character }\OperatorTok{=}\NormalTok{ input}\OperatorTok{.}\NormalTok{get}\OperatorTok{();}
\OperatorTok{\}}
\end{Highlighting}
\end{Shaded}
This should reduce the amount of time the program spends on dictionary lookups.

\hypertarget{using-const-char}{%
\subsection{Using Const Char *}\label{using-const-char}}

The algorithm works by reading through the entire file, so we know that at some point, we will need to see every byte of the entire file.

When reading a byte stream of the file, the file may not always be in memory, which is the fastest part of a computer's storage. The \texttt{ifstream} class in C++, the typical method of reading a file, has many extraneous features that we don't need. If we map the file directly into memory using \texttt{mmap} and pass around a pointer to that data, it will simplify and speed up the scanning process. Also, using a \texttt{char*} opens the possibility to getting rid of \texttt{std::string} entirely, which means way less overhead and decreased compression time. Rather than concatenate strings at all, we can pass around a pointer to the beginning of the data and a number which is an offset into the data.

\hypertarget{comparison}{%
\subsection{Comparison}\label{comparison}}

Taking metrics of the algorithm at each of the stages listed in this chapter, we can make a graph showing the improvements in performance.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/comparecorporaoptimizingfig-1.pdf}
\caption{\label{fig:comparecorporaoptimizingfig}Comparison of the performance of the different milestones.}
\end{figure}
Figure \ref{fig:comparecorporaoptimizingfig} shows the performance of the algorithm at the different milestones mentioned in this section. The bars represent distinct versions of the algorithm, and they are listed chronologically from left to right. On the left side of the figure you see that throughput increases for the program with each optimization. On the right, observe that mean compression time decreases with each optimization.

\hypertarget{trying-different-dictionaries}{%
\section{Trying Different Dictionaries}\label{trying-different-dictionaries}}

A lot of the stress of the LZW algorithm is on the dictionary. We are constantly looking strings up and inserting others. Because of the reliance on this data structure, we know that the dictionary accesses and lookups are a bottleneck, so improvements in those areas could greatly increase the efficiency of our program.

So another step towards an efficient LZW was to be to abstract out the C++ \texttt{std::unordered\_map} and have multiple different dictionary implementations to try and experiment with in our attempt to optimize LZW for DNA compression.

\hypertarget{direct-map}{%
\subsection{Direct Map}\label{direct-map}}

In our analysis of the two corpora, we found some interesting statistics in the redundancy of the data.
\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2703}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2703}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2568}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2027}}@{}}
\caption{\label{tab:runstatsfigcp1}Run statistics in Corpus 1}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Average Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Maximum Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Median Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Sd Run Length
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Average Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Maximum Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Median Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Sd Run Length
\end{minipage} \\
\midrule()
\endhead
6.135398 & 17 & 6 & 1.237217 \\
\bottomrule()
\end{longtable}
\begin{longtable}[]{@{}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2703}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2703}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2568}}
  >{\centering\arraybackslash}p{(\columnwidth - 6\tabcolsep) * \real{0.2027}}@{}}
\caption{\label{tab:runstatsfigcp2}Run statistics in Corpus 2}\tabularnewline
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Average Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Maximum Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Median Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Sd Run Length
\end{minipage} \\
\midrule()
\endfirsthead
\toprule()
\begin{minipage}[b]{\linewidth}\centering
Average Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Maximum Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Median Run Length
\end{minipage} & \begin{minipage}[b]{\linewidth}\centering
Sd Run Length
\end{minipage} \\
\midrule()
\endhead
10.96825 & 190 & 11 & 2.494305 \\
\bottomrule()
\end{longtable}
Tables \ref{tab:runstatsfigcp1} and \ref{tab:runstatsfigcp2} show stats on the run lengths of the ``runs'' of data, where a run is a string added to the dictionary during the execution of LZW. So if there is a run of length 8, that means we are replacing 8 characters in the original text with a codeword. A histogram of runs from both corpora can be seen in Figure \ref{fig:allrunshistfig}.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/allrunshistfig-1.pdf}
\caption{\label{fig:allrunshistfig}A histogram showing the lengths of runs for both copora.}
\end{figure}
Given this data, it is clear that it would be advantageous to speed up the dictionary for smaller run sizes, since most of the runs are below size 15. As stated before, there have been research papers about the possibility of using multiple indexed dictionaries for LZW, including Keerthy (Keerthy, 2019).
To achieve a similar effect to multiple indexed dictionaries, we opted for a unique approach. Rather than use a slow dictionary implementation like the \texttt{std::unordered\_map} in C++, we can try to map the strings directly into memory.

Our first realization was that since all of the strings only contain four characters (`A', `C', `T', and `G'), we can represent the characters with two bits. We can assign `A' to be \texttt{00}, `C' to be \texttt{01}, and so on. So for any string of length \texttt{n}, we can uniquely represent that string with \texttt{2n} bits.

Assume for each string size 1 to n, we have an array with enough slots for every possible string of that length. For example, for strings of length 3, we have an array of size \(4^3\), since there are \(4^3\) possible strings. In each of those \(4^3\) slots, we have space for a codeword. All strings of length 3 can be represented by 6 bits, and since 6 bits can represent \(2^6=4^3\) values, we can use the bit representation to index into the dictionary. If the codeword at that place in the dictionary is 0, we have never seen it before. If it is non-zero, we have found the codeword for that string. For all strings greater than \texttt{n}, we can just use a single \texttt{std::unordered\_map} on top to store those longer strings. Since we are storing all these codewords in memory, it also makes sense to use shorter codewords, such as 16 bit unsigned integers rather than the 32 bit codewords we were using in previous implementations.

We call this data structure the Direct Map Dictionary. Different numbers \texttt{n} may provide different performance results.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/directmaplenthcompfig-1.pdf}
\caption{\label{fig:directmaplenthcompfig}Comparing different max string lengths for the Direct Map Dictionary.}
\end{figure}
As seen in Figure \ref{fig:directmaplenthcompfig}, having a max string length of 15 rather than 10 leads to an increase in throughput and a decrease in mean compression time. This makes sense, because any string over the max requires an entry into the \texttt{std::unordered\_map}, which takes much more time than a Direct Map Dictionary access. Ideally, we would have this Direct Map data structure support strings of longer lengths than 15. However, this is difficult because the amount of memory required increases exponentially. The number of possible strings is quadrupled as the length of the string increases by 1, and our computers are limited in the amount of data that can be stored in memory.

Since we are limiting the size of codewords, we will run out of codewords faster than before. Thus, the compression ratio may be different.
\begin{table}[!h]

\caption{\label{tab:directtostdcomprat}Comparison of Compression Ratios between Direct Map and Standard Dictionaries.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r|r}
\hline
\multicolumn{2}{c|}{ } & \multicolumn{2}{c}{Compression Ratio } \\
\cline{3-4}
File Name & Original File Size & Direct Map & Standard Dictionary\\
\hline
\cellcolor{gray!6}{DNACorpus1/chmpxx} & \cellcolor{gray!6}{121024} & \cellcolor{gray!6}{2.449} & \cellcolor{gray!6}{2.797}\\
\hline
DNACorpus1/chntxx & 155844 & 2.402 & 2.683\\
\hline
\cellcolor{gray!6}{DNACorpus1/hehcmv} & \cellcolor{gray!6}{229354} & \cellcolor{gray!6}{2.460} & \cellcolor{gray!6}{2.689}\\
\hline
DNACorpus1/humdyst & 38770 & 2.103 & 2.565\\
\hline
\cellcolor{gray!6}{DNACorpus1/humghcs} & \cellcolor{gray!6}{66495} & \cellcolor{gray!6}{2.227} & \cellcolor{gray!6}{2.625}\\
\hline
DNACorpus1/humhbb & 73308 & 2.243 & 2.626\\
\hline
\cellcolor{gray!6}{DNACorpus1/humhdab} & \cellcolor{gray!6}{58864} & \cellcolor{gray!6}{2.200} & \cellcolor{gray!6}{2.618}\\
\hline
DNACorpus1/humprtb & 56737 & 2.192 & 2.616\\
\hline
\cellcolor{gray!6}{DNACorpus1/mpomtcg} & \cellcolor{gray!6}{186609} & \cellcolor{gray!6}{2.414} & \cellcolor{gray!6}{2.666}\\
\hline
DNACorpus1/mtpacga & 100314 & 2.377 & 2.737\\
\hline
\cellcolor{gray!6}{DNACorpus1/vaccg} & \cellcolor{gray!6}{191737} & \cellcolor{gray!6}{2.486} & \cellcolor{gray!6}{2.746}\\
\hline
DNACorpus2/AeCa & 1591049 & 2.833 & 2.861\\
\hline
\cellcolor{gray!6}{DNACorpus2/AgPh} & \cellcolor{gray!6}{43970} & \cellcolor{gray!6}{2.105} & \cellcolor{gray!6}{2.548}\\
\hline
DNACorpus2/BuEb & 18940 & 1.914 & 2.447\\
\hline
\cellcolor{gray!6}{DNACorpus2/DaRe} & \cellcolor{gray!6}{62565020} & \cellcolor{gray!6}{2.954} & \cellcolor{gray!6}{3.194}\\
\hline
DNACorpus2/DrMe & 32181429 & 2.886 & 3.031\\
\hline
\cellcolor{gray!6}{DNACorpus2/EnIn} & \cellcolor{gray!6}{26403087} & \cellcolor{gray!6}{2.938} & \cellcolor{gray!6}{3.067}\\
\hline
DNACorpus2/EsCo & 4641652 & 2.863 & 2.914\\
\hline
\cellcolor{gray!6}{DNACorpus2/GaGa} & \cellcolor{gray!6}{148532294} & \cellcolor{gray!6}{2.832} & \cellcolor{gray!6}{3.170}\\
\hline
DNACorpus2/HaHi & 3890005 & 2.937 & 2.978\\
\hline
\cellcolor{gray!6}{DNACorpus2/HePy} & \cellcolor{gray!6}{1667825} & \cellcolor{gray!6}{2.913} & \cellcolor{gray!6}{2.943}\\
\hline
DNACorpus2/HoSa & 189752667 & 2.966 & 3.317\\
\hline
\cellcolor{gray!6}{DNACorpus2/OrSa} & \cellcolor{gray!6}{43262523} & \cellcolor{gray!6}{2.882} & \cellcolor{gray!6}{3.058}\\
\hline
DNACorpus2/PlFa & 8986712 & 3.024 & 3.104\\
\hline
\cellcolor{gray!6}{DNACorpus2/ScPo} & \cellcolor{gray!6}{10652155} & \cellcolor{gray!6}{2.885} & \cellcolor{gray!6}{2.967}\\
\hline
DNACorpus2/YeMi & 73689 & 2.325 & 2.727\\
\hline
\end{tabular}}
\end{table}
As seen in Table \ref{tab:directtostdcomprat}, using the Direct Map Dictionary decreases the compression ratio on every input file relative to the Standard Dictionary Implementation. This is a trade off between compression time and compression ratio. Our hope is that the Direct Map version will be much faster, and we will be able to make up some of the compression ratio lost by using entropy encoding.

It's also worth noting that at this point, we are outputting codewords followed by characters. If we output codewords followed by two bits representing the next character, we will save 6 bits per iteration. However, the relative differences in compression ratio between the Direct Map and Std Dictionaries would not change.

\hypertarget{multiple-standard-dictionaries}{%
\subsection{Multiple Standard Dictionaries}\label{multiple-standard-dictionaries}}

Similar to the Direct Mapped approach, we can use an individual \texttt{std::unordered\_map} for each string size up to a certain size \texttt{n} , and for all strings of length greater than \texttt{n}, we use a single \texttt{std::unordered\_map}. As with the direct map dictionary, we need to specify a max string length. We collected metrics for different choices of max string length. As seen in Figure \ref{fig:multdictlengthcompfig}, the throughput tends to decrease and the average compression time tends to increase as we increase the number of indexed dictionaries. This result was not necessarily one we expected, but it does make sense that there is a certain amount of overhead that is required for a \texttt{std::unordered\_map}. The hash function, which is what maps keys to their values (i.e.~strings to their codewords), takes about the same amount of time no matter the number of elements in the map, and resizing is rare. So adding more dictionaries only adds more overhead, which tends to slightly decrease efficiency.

Compression ratio remains the same as a single Standard Dictionary, since strings of all lengths are accommodated and we are using 32 bit codewords.
This logic is supported by Figure \ref{fig:multvsonedictfig}, which shows one \texttt{std::unordered\_map} compared to Multiple Standard Dictionaries.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/multdictlengthcompfig-1.pdf}
\caption{\label{fig:multdictlengthcompfig}Comparing different max string lengths for Multiple Standard Dictionaries.}
\end{figure}
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/multvsonedictfig-1.pdf}
\caption{\label{fig:multvsonedictfig}Comparing one Standard Dictionary to Multiple Standard Dictionaries (with a max string length of 10).}
\end{figure}
\hypertarget{comparison-1}{%
\subsection{Comparison}\label{comparison-1}}

Figure \ref{fig:dictionarytechniquecompfig} shows a comparison of all three techniques: Standard Dictionary, Multiple Standard Dictionaries, and Direct Mapped Dictionary. As shown, the Direct Map technique greatly increases throughput and thus decreases average compression time. Given these results, we decided to shift our focus onto the Direct Map and try to optimize this scheme as much as possible.

Again, taking the Direct Map approach means decreasing compression ratio overall. The hope is that we will be able to use entropy encoding on the output of the Direct Map version of LZW to make up for that loss in compression ratio.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/dictionarytechniquecompfig-1.pdf}
\caption{\label{fig:dictionarytechniquecompfig}Comparing the three types of dictionaries. The Direct Map has a max length of 15 and the Mult Dict has a max length of 10.}
\end{figure}
\hypertarget{optimizing-direct-map-even-more}{%
\section{Optimizing Direct Map Even more}\label{optimizing-direct-map-even-more}}

Given that the Direct Map dictionary showed great performance improvements, we decided to narrow in on the scheme to see if there were ways to improve it even further.

\hypertarget{finding-the-longest-runs}{%
\subsection{Finding the Longest Runs}\label{finding-the-longest-runs}}

Our dictionary data structures all have a \texttt{get\_longest\_in\_dict} function. This function does the boring work of iterating through the input from the start, checking if each substring is in the dictionary.

Given the statistics of our corpora, we know that this process can be faster. Since most runs are above 6-7, we waste a lot of time by looping up from zero.

Another strategy would be to start from the maximum string length of the dictionary, so 15. We can check if the next string of the max length is in the dictionary. If it is, we need to check strings longer than the max, so we can iterate up. If it isn't, we need to check strings shorter, so we can either iterate down. Keep in mind that in order to look up a string in a Direct Map Dictionary, you need to convert the string to its two bit representation, which we call an index. Here is some pseudocode that mimics this proposed algorithm.
\begin{verbatim}
find_longest(input_string){

    // calculate the index of the next 15 chars
    // where index = converting each char to two bits
    index_of_next_15_chars = calculate_index(input_string[0:15]);

    // look up our string
    lookup = dictionary[index_of_next_15_chars];

    if(lookup is in dictionary){
        loop_up();
    }
    else {
        loop_down();
    }
    
}
\end{verbatim}
Calculating the index takes time. While we are looping up or down, we could just use the index of the next 15 characters as a starting point. If we are looping up, we can add on the next character's two bit representation as we loop. For instance, suppose our string has an index of \texttt{00101100}. If the next character is `A', we can simply tack the two bit representation for `A' to the end of our index, yielding \texttt{00101100\textbar{}00}.

Similarly, we can chop off two bits at a time while looping down. We can name this process looping ``on the fly'', since we are constructing our index on the fly rather than recalculating it every time. So our modified pseudocode would look like the code below:
\begin{verbatim}
find_longest(input_string){

    // calculate the index of the next 15 chars
    // where index = converting each char to two bits
    index_of_next_15_chars = calculate_index(input_string[0:15]);

    // look up our string
    lookup = dictionary[index_of_next_15_chars];

    if(lookup is in dictionary){
        loop_up_on_fly(index);
    }
    else {
        loop_down_on_fly(index);
    }
    
}
\end{verbatim}
Of course, there are theoretically quicker ways of iterating than looping up or down. We could use binary search.

Binary search is a searching technique for sorted lists, but we can use it in this scenario as well. We are looking for where a string of length n is in the dictionary, but the string of length n+1 is not. Suppose we are searching for the longest run in a string of length 7. We can see if the middle element, the string of length 4, is the longest run. If it is, we return it. If it is not the longest run but it is in the dictionary, we can continue searching between lengths 5 and 7. If the string of length 4 is not in the dictionary, we know the longest run must lie between lengths 1 and 3. We can continue searching in this manner until we find the longest run.
\begin{verbatim}
find_longest(input_string){

    // code omitted...

    if(lookup is in dictionary){
        loop_up_on_fly(index);
    }
    else {
        loop_down_binary_search(index);
    }
    
}
\end{verbatim}
Of course, we could also calculate the indexes for binary search on the fly. So we now have 5 different schemes of finding the longest run: looping up or down, looping up or down on the fly, binary search, binary search on the fly, and looping up from zero, and looping up from zero on the fly. Figure \ref{fig:findlongestfrommaxfig} shows a comparison of these methods.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestfrommaxfig-1.pdf}
\caption{\label{fig:findlongestfrommaxfig}Comparing the different ways of finding the longest run.}
\end{figure}
We see that calculating the index on the fly does tend to improve performance, so from now on we will calculate indexes on the fly. However, we can also see that none of the other strategies are better than just looping up from zero. This could be because most of the runs are very short, which we can see in Figure \ref{fig:allrunshistfig}. This means that if we start looking from runs around length 10, we should see a performance improvement.

\hypertarget{finding-the-longest-from-the-average}{%
\subsection{Finding the Longest From The Average}\label{finding-the-longest-from-the-average}}

As we can see in Figure \ref{fig:findlongestfromavg}, looping from the average run length, which we approximate at 7, increases throughput and decreases mean compression time relative to looping up from zero. Binary search from the average performs about the same as looping up from zero.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestfromavg-1.pdf}
\caption{\label{fig:findlongestfromavg}Comparing the different ways of finding the longest run starting from the average.}
\end{figure}
We are capitalizing on the fact that most runs are short. There is the occasional run that is very long, but from the run statistics we saw that the standard deviation for both corpora was pretty small. So on the edge cases in which there are very long runs, we could be wasting a lot of time.

\hypertarget{not-allowing-strings-over-max}{%
\subsection{Not Allowing strings over max}\label{not-allowing-strings-over-max}}

One way to avoid the edge case where we encounter a very long run is to not allow strings in the dictionary longer than the max string length, in this case, 15. This means we won't have to deal with the overhead of the \texttt{std::unordered\_map} on top of our Direct Map Dictionary, but we will take a hit in compression ratio. Figure \ref{fig:findlongestwmax} summarizes the results of the different methods with the max length rule enforced. There is a slight performance improvement for looping, which makes sense because long runs are very rare. Binary search actually takes a performance drop. When the max is enforced, we can do binary search across the strings of lengths 1-15, while before we were either binary searching down from 7 or looping up because the maximum string length was unbounded. This allows us to avoid ever having to loop, which should be faster, but it isn't. The reason for this is that because we are starting at the average, most of the time we are searching for a value that is right next to our start value. So in a lot of cases, binary search will actually require more search time relative to looping. There are also a lot of if statements involved in binary search, which can increase the run time if the computer has a hard time predicting the outcome of those if statements.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestwmax-1.pdf}
\caption{\label{fig:findlongestwmax}Comparing the different ways of finding the longest run starting at average with strings over max (15) not accepted.}
\end{figure}
Of course, not allowing string over max length does mean that our compression ratio will change. Up until this point, all the different versions of the Direct Mapped Dictionary had the same compression ratio represented in Table \ref{tab:directtostdcomprat}. Table \ref{tab:maxenforcedstats} shows the affect of enforcing the max string length on the total compression ratio over both corpora. As we can see, it is minimal, which makes sense because long runs are very rare.
\begin{table}

\caption{\label{tab:maxenforcedstats}Compression Ratio change from disallowing long strings}
\centering
\begin{tabular}[t]{rr}
\toprule
CR Before & CR After\\
\midrule
2.909175 & 2.907686\\
\bottomrule
\end{tabular}
\end{table}
This difference in compression ratio is very slight, but remember that we already took a decrease in compression ratio by selecting the Direct Map Dictionary over the Standard Dictionary. Given the improvement in performance, we will now be not allowing strings over the max length in the Direct Map Dictionary unless stated otherwise.

\hypertarget{using-pext}{%
\subsection{\texorpdfstring{Using \texttt{pext}}{Using pext}}\label{using-pext}}

One potential bottleneck of finding the longest run is converting a run of characters into an index for the Direct Map Dictionary. We can try to do it on the fly as we loop up or down, but we could also use machine instructions.

We could use \texttt{pext}, which is a command that extracts bits in parallel, meaning at the same time. The \texttt{pext} instruction is a recent addition to the x86 instruction set, so it has not yet been widely used yet as optimization technique.

We give \texttt{pext} a string of characters, say `ACTG', and a bit mask, and it will extract those bits from our string. Figure \ref{fig:pext} details this process for a string of length 4.

It theoretically does this in one machine instruction, which could be much more efficient than looping over all the characters.
\begin{figure}[h]\centering


\usetikzlibrary{chains,decorations.pathreplacing}
 \begin{tikzpicture}[
node distance=0pt,
 start chain = A going right,
    X/.style = {rectangle, draw,% styles of nodes in string (chain)
                minimum width=2ex, minimum height=3ex,
                outer sep=0pt, on chain},
    B/.style = {decorate,
                decoration={brace, amplitude=5pt,
                pre=moveto,pre length=1pt,post=moveto,post length=1pt,
                raise=1mm,
                            #1}, % for mirroring of brace, if necessary
                thick},
    B/.default=mirror, % by default braces are mirrored
                        ]
\foreach \i in {0,1,0,0,0,0,0,1,
                0,1,0,0,0,0,1,1,
                0,1,0,0,0,1,1,1,
                0,1,0,1,0,1,0,0, }% <-- content of nodes
    \node[X] {\i};
\draw[B] ( A-6.south west) -- node[below=2mm] {Index of A} ( A-7.south east);
\draw[B] (A-14.south west) -- node[below=2mm] {Index of C} (A-15.south east);
\draw[B] (A-22.south west) -- node[below=2mm] {Index of G} (A-23.south east);
\draw[B] (A-30.south west) -- node[below=2mm] {Index of T} (A-31.south east);
\node (B1) [inner sep=1pt,above=of A-1.north west] {$\downarrow$};
\node (B2) [inner sep=1pt,above=of A-9.north west] {$\downarrow$};
\draw[B=](B1.north) -- node[above=2mm] {A in binary}(B2.north);
\node (B3) [inner sep=1pt,above=of A-9.north west] {$\downarrow$};
\node (B4) [inner sep=1pt,above=of A-17.north west] {$\downarrow$};
\draw[B=](B3.north) -- node[above=2mm] {C in binary}(B4.north);
\node (B5) [inner sep=1pt,above=of A-17.north west] {$\downarrow$};
\node (B6) [inner sep=1pt,above=of A-25.north west] {$\downarrow$};
\draw[B=](B5.north) -- node[above=2mm] {G in binary}(B6.north);
\node (B7) [inner sep=1pt,above=of A-25.north west] {$\downarrow$};
\node (B8) [inner sep=1pt,above=of A-33.north west] {$\downarrow$};
\draw[B=](B7.north) -- node[above=2mm] {T in binary}(B8.north);
    \end{tikzpicture}
\caption{How pext extracts bits}

\label{fig:pext}
\end{figure}
We did get lucky in that the 6th and 7th bit (from left to right) on A, C, T, and G give a unique 2 bit values. This means we can just extract the 6th and 7th bit from every character without knowing what it is.

We can apply this technique to our \texttt{find\_longest} function: we can extract the index of a string using \texttt{pext} very quickly, then use that index rather than recalculating it every time. This means that if we precompute the index with \texttt{pext}, a lookup in the Direct Map Dictionary can theoretically happen in constant time. In other words, the actual dictionary queries are almost instantaneous. Figure \ref{fig:findlongestwpext} hows the results of this application for both looping and binary search. In both cases, using \texttt{pext} to extract the index of the string increases throughput and decreases average compression time. Overall, looping from the average using \texttt{pext} is the fastest strategy, so this is the method used in the final implementation of the Direct Map Dictionary.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/findlongestwpext-1.pdf}
\caption{\label{fig:findlongestwpext}Comparing the different ways of finding the longest run with pext.}
\end{figure}
\hypertarget{returning-to-compression-ratio}{%
\section{Returning to Compression Ratio}\label{returning-to-compression-ratio}}

After spending a majority of the time optimizing, we returned our attention to the compression ratio. Most of our optimizations for the Direct Map Dictionary didn't have much of an effect on compression ratio, with the exception of disallowing strings over a certain length. We did note that the Direct Map already has a lower compression ratio than the Standard Dictionary implementation due to a smaller codeword size.
\begin{table}[!h]

\caption{\label{tab:directostdfinal}Comparing compression ratios of the two Dictionary versions.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r|r}
\hline
\multicolumn{2}{c|}{ } & \multicolumn{2}{c}{Compression Ratio } \\
\cline{3-4}
File Name & Original File Size & Direct Map Dictionary & Std Dictionary\\
\hline
\cellcolor{gray!6}{DNACorpus1/chmpxx} & \cellcolor{gray!6}{121024} & \cellcolor{gray!6}{3.266} & \cellcolor{gray!6}{3.915}\\
\hline
DNACorpus1/chntxx & 155844 & 3.203 & 3.722\\
\hline
\cellcolor{gray!6}{DNACorpus1/hehcmv} & \cellcolor{gray!6}{229354} & \cellcolor{gray!6}{3.279} & \cellcolor{gray!6}{3.701}\\
\hline
DNACorpus1/humdyst & 38770 & 2.804 & 3.690\\
\hline
\cellcolor{gray!6}{DNACorpus1/humghcs} & \cellcolor{gray!6}{66495} & \cellcolor{gray!6}{2.969} & \cellcolor{gray!6}{3.721}\\
\hline
DNACorpus1/humhbb & 73308 & 2.991 & 3.712\\
\hline
\cellcolor{gray!6}{DNACorpus1/humhdab} & \cellcolor{gray!6}{58864} & \cellcolor{gray!6}{2.934} & \cellcolor{gray!6}{3.727}\\
\hline
DNACorpus1/humprtb & 56737 & 2.923 & 3.729\\
\hline
\cellcolor{gray!6}{DNACorpus1/mpomtcg} & \cellcolor{gray!6}{186609} & \cellcolor{gray!6}{3.218} & \cellcolor{gray!6}{3.682}\\
\hline
DNACorpus1/mtpacga & 100314 & 3.169 & 3.844\\
\hline
\cellcolor{gray!6}{DNACorpus1/vaccg} & \cellcolor{gray!6}{191737} & \cellcolor{gray!6}{3.315} & \cellcolor{gray!6}{3.794}\\
\hline
DNACorpus2/AeCa & 1591049 & 3.778 & 3.786\\
\hline
\cellcolor{gray!6}{DNACorpus2/AgPh} & \cellcolor{gray!6}{43970} & \cellcolor{gray!6}{2.807} & \cellcolor{gray!6}{3.653}\\
\hline
DNACorpus2/BuEb & 18940 & 2.552 & 3.595\\
\hline
\cellcolor{gray!6}{DNACorpus2/DaRe} & \cellcolor{gray!6}{62565020} & \cellcolor{gray!6}{3.928} & \cellcolor{gray!6}{4.013}\\
\hline
DNACorpus2/DrMe & 32181429 & 3.847 & 3.836\\
\hline
\cellcolor{gray!6}{DNACorpus2/EnIn} & \cellcolor{gray!6}{26403087} & \cellcolor{gray!6}{3.918} & \cellcolor{gray!6}{3.893}\\
\hline
DNACorpus2/EsCo & 4641652 & 3.817 & 3.791\\
\hline
\cellcolor{gray!6}{DNACorpus2/GaGa} & \cellcolor{gray!6}{148532294} & \cellcolor{gray!6}{3.776} & \cellcolor{gray!6}{3.943}\\
\hline
DNACorpus2/HaHi & 3890005 & 3.916 & 3.884\\
\hline
\cellcolor{gray!6}{DNACorpus2/HePy} & \cellcolor{gray!6}{1667825} & \cellcolor{gray!6}{3.884} & \cellcolor{gray!6}{3.894}\\
\hline
DNACorpus2/HoSa & 189752667 & 3.953 & 4.118\\
\hline
\cellcolor{gray!6}{DNACorpus2/OrSa} & \cellcolor{gray!6}{43262523} & \cellcolor{gray!6}{3.843} & \cellcolor{gray!6}{3.858}\\
\hline
DNACorpus2/PlFa & 8986712 & 4.031 & 4.000\\
\hline
\cellcolor{gray!6}{DNACorpus2/ScPo} & \cellcolor{gray!6}{10652155} & \cellcolor{gray!6}{3.847} & \cellcolor{gray!6}{3.813}\\
\hline
DNACorpus2/YeMi & 73689 & 3.099 & 3.859\\
\hline
\end{tabular}}
\end{table}
In Table \ref{tab:directostdfinal}, we can see a comparison in compression ratios of the final implementations of the Direct Map and Standard Dictionaries. Note that the Multiple Standard Dictionaries version has the same compression ratios as a single Standard Dictionary.

\hypertarget{a-point-of-comparison}{%
\subsection{A point of Comparison}\label{a-point-of-comparison}}

It's worth noting that for sequences of DNA, if the bases are encoded in ASCII, there is a simple and efficient algorithm to compress the file with a 4.0 compression ratio every time. Since there are only 4 bases, we can represent each base with 2 bits. Since each ASCII character takes up 1 bytes, or 8 bits, the compression ratio is always 8/2 = 4.0. This is a simple conversion, and it requires no dictionaries or codewords.

If a DNA compression algorithm can't compress with a compression ratio of higher than 4.0 (2 bits per base/nucleotide), than this simple algorithm would be preferable every time.

We call this method Four to One encoding. We implemented this in C++ using \texttt{pext}, and the results are detailed in the next chapter.

\hypertarget{entropy-encoding}{%
\subsection{Entropy Encoding}\label{entropy-encoding}}

Our initial idea was that we would compress the DNA with LZW, then use a entropy encoding method like Arithmetic encoding or Huffman to further compress the data. This was an oversight, as the algorithm as we have described it thus far does not lend itself well to entropy encoding.

Our algorithm outputs codewords and two bits representing the next character. For a codeword size of 16 bits, this means each loop of our algorithm outputs 18 bits. So any repetitiveness or reuse will not be detectable by an entropy encoder, because they tend to work on data in 8, 16, or 32 bit chunks.

So maybe we break the compressed file into two separate files; a file of codewords and a file of the two bit representations of the following characters. Now we can compress these two files, right?

The issue is that entropy encoders rely on repeating numbers of a high frequency. We can cut down the number of bits a certain entry takes to make it more compressible, and make less common entries take more bits (see Figure \ref{fig:huffman}). DNA nucleotides show up with roughly the same frequency, so having 2 bits per character is hard to beat. So the file with all the characters in it can't be compressed further.

What about the codewords? Well, a key realization is that any codeword will only show up an average of 4 times in a single run of the program. Say we add the codeword \texttt{120="ACT"} to our dictionary. When will we output this codeword ? Well, we will output it when we see ``ACT'' followed by another character. Since there are only 4 characters, once you see those 4 other strings (``ACTA'', ``ACTG'', ``ACTC'', and ``ACTT''), we will never output it again because we are looking for the longest run possible.

There are two cases in which we will output a codeword more than 4 times. If we run out of codewords and we have never encoded ``ACTG'', any time ``ACTG'' comes up, we will output \texttt{120G}. This case is hard to predict as we have no way of controlling what strings are left when we run out of codewords. The other case is for strings that are the max length. These long runs may show up multiple times, and since we won't add any strings longer than the max to our dictionary, it will never be overwritten. However, these runs are very rare, or else the compression ratio wouldn't be an issue.

So on average, every codeword shows up a maximum of 4 times. This means that for long strands of DNA, we have output nearly all of the codewords with relatively even frequency. In other words, entropy encoding will not shorten the length of the codewords.

\hypertarget{a-new-approach}{%
\subsection{A New Approach}\label{a-new-approach}}

Given that we are not able to achieve a compression ratio over 4.0 with the Direct Map LZW, we need to alter our approach. The issue with our dataset is that long runs are rare. Every once in a while, there may be an opportunity to replace a run of 15 with a codeword, but most of the time the runs being replaced are of length 8 or less. Eight characters can be represented by 16 bits via the 2 bit encodings, so any run under 8 that is replaced actually increases the total number of bits in the output file.

The fact that we are not getting a compression ratio over 4.0 led us to think of a new twist on the algorithm specifically tailored for this situation. The scheme uses three streams of data: characters, codewords, and indicator bits. The algorithm works as follows.

Compression:
\begin{itemize}
\tightlist
\item
  if the next longest run is less than 8 characters, we add it to the dictionary, but rather than output the codeword, we just output the 2 bit representation of the next 8 characters. We output a 0 to the indicator stream to indicate this choice.
\item
  if the next longest run is equal to or greater than 8, we output a codeword to the codeword stream and the next character to the character stream. We also output a 1 to the indicator stream to indicate a codeword was output.
\end{itemize}
Decompression: We start by reading an indicator bit
\begin{itemize}
\tightlist
\item
  if the bit is 1, we read a codeword from the codeword stream and the next character from the character stream. We add this to the dictionary like the old algorithm.
\item
  if the bit is 0, we read the next 8 characters. We then use \texttt{find\_longest} to find the longest run and add it to the dictionary. We can then put the 8 characters into the output stream and move on.
\end{itemize}
This is, in essence, a greedy algorithm. A greedy algorithm is one that, when given a choice, it chooses the path that is most advantageous to the overall goal. This new algorithm have the choice of outputting a codeword or 8 characters every time, and we choose the choice which results in the least number of bits in the output file.

The other advantage to this algorithm is that it works well for entropy encoding. The indicator bits are mostly 0, since most runs are less than 8. The codeword stream is also compressible, since none of the codewords for strings less than length 8 are output. We call this new scheme Three Stream LZW, and its performance is evaluated in the next chapter.

\hypertarget{comparison-to-other-tools}{%
\chapter{Comparison to other tools}\label{comparison-to-other-tools}}

Now that we have implemented several versions of LZW, we will compare them to each other to see which would be preferable in different situations. We also want to compare the performance of these different implementations to the Four to One implementation and other professional compression tools.

\hypertarget{comparison-of-our-implementations}{%
\section{Comparison of our Implementations}\label{comparison-of-our-implementations}}

We have five different implementations at this point: LZW with Direct Map Dictionary, LZW with the Standard Dictionary, LZW with Multiple Standard Dictionaries, Three Stream LZW, and the Four To One translation. Figure \ref{fig:comparisonofourmethodsfig} shows the performance of these methods on the corpora.

We can also see the raw values in Table \ref{tab:comparisonofourmethodstab}. While the Four to One implementation completely dominates in terms of throughput, the Direct Map version is substantially faster than the Standard and Multiple Dictionary versions. The Three Stream LZW comes in between the Direct Map and Four to One implementations in terms of compression time, but it actually has the best compression ratio out of all LZW versions on most of the input files. This means that by switching from the Standard Dictionary to the Direct Map and adding on entropy encoding, we were able to regain the lost ground in terms of compression ratio while not totally compromising our performance improvements.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/comparisonofourmethodsfig-1.pdf}
\caption{\label{fig:comparisonofourmethodsfig}Performance comparison of our final implementations.}
\end{figure}
\begin{table}[!h]

\caption{\label{tab:comparisonofourmethodstab}Compression Ratios of Final Implementations.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r|r|r|r|r}
\hline
\multicolumn{2}{c|}{ } & \multicolumn{5}{c}{Compression Ratio } \\
\cline{3-7}
File Name & Original File Size & Direct Map Dictionary & Four to One & Mult Std Dictionaries & Std Dictionary & Three Stream LZW\\
\hline
\cellcolor{gray!6}{DNACorpus1/chmpxx} & \cellcolor{gray!6}{121024} & \cellcolor{gray!6}{3.266} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.915} & \cellcolor{gray!6}{3.915} & \cellcolor{gray!6}{3.885}\\
\hline
DNACorpus1/chntxx & 155844 & 3.203 & 3.999 & 3.722 & 3.722 & 3.888\\
\hline
\cellcolor{gray!6}{DNACorpus1/hehcmv} & \cellcolor{gray!6}{229354} & \cellcolor{gray!6}{3.279} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.701} & \cellcolor{gray!6}{3.701} & \cellcolor{gray!6}{3.885}\\
\hline
DNACorpus1/humdyst & 38770 & 2.804 & 3.997 & 3.690 & 3.690 & 3.954\\
\hline
\cellcolor{gray!6}{DNACorpus1/humghcs} & \cellcolor{gray!6}{66495} & \cellcolor{gray!6}{2.969} & \cellcolor{gray!6}{3.998} & \cellcolor{gray!6}{3.721} & \cellcolor{gray!6}{3.721} & \cellcolor{gray!6}{3.933}\\
\hline
DNACorpus1/humhbb & 73308 & 2.991 & 3.998 & 3.712 & 3.712 & 3.935\\
\hline
\cellcolor{gray!6}{DNACorpus1/humhdab} & \cellcolor{gray!6}{58864} & \cellcolor{gray!6}{2.934} & \cellcolor{gray!6}{3.998} & \cellcolor{gray!6}{3.727} & \cellcolor{gray!6}{3.727} & \cellcolor{gray!6}{3.938}\\
\hline
DNACorpus1/humprtb & 56737 & 2.923 & 3.998 & 3.729 & 3.729 & 3.927\\
\hline
\cellcolor{gray!6}{DNACorpus1/mpomtcg} & \cellcolor{gray!6}{186609} & \cellcolor{gray!6}{3.218} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.682} & \cellcolor{gray!6}{3.682} & \cellcolor{gray!6}{3.896}\\
\hline
DNACorpus1/mtpacga & 100314 & 3.169 & 3.999 & 3.844 & 3.844 & 3.875\\
\hline
\cellcolor{gray!6}{DNACorpus1/vaccg} & \cellcolor{gray!6}{191737} & \cellcolor{gray!6}{3.315} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.794} & \cellcolor{gray!6}{3.794} & \cellcolor{gray!6}{3.874}\\
\hline
DNACorpus2/AeCa & 1591049 & 3.778 & 4.000 & 3.786 & 3.786 & 3.874\\
\hline
\cellcolor{gray!6}{DNACorpus2/AgPh} & \cellcolor{gray!6}{43970} & \cellcolor{gray!6}{2.807} & \cellcolor{gray!6}{3.997} & \cellcolor{gray!6}{3.653} & \cellcolor{gray!6}{3.653} & \cellcolor{gray!6}{3.957}\\
\hline
DNACorpus2/BuEb & 18940 & 2.552 & 3.993 & 3.595 & 3.595 & 3.964\\
\hline
\cellcolor{gray!6}{DNACorpus2/DaRe} & \cellcolor{gray!6}{62565020} & \cellcolor{gray!6}{3.928} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.013} & \cellcolor{gray!6}{4.013} & \cellcolor{gray!6}{4.035}\\
\hline
DNACorpus2/DrMe & 32181429 & 3.847 & 4.000 & 3.836 & 3.836 & 3.865\\
\hline
\cellcolor{gray!6}{DNACorpus2/EnIn} & \cellcolor{gray!6}{26403087} & \cellcolor{gray!6}{3.918} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{3.893} & \cellcolor{gray!6}{3.893} & \cellcolor{gray!6}{3.928}\\
\hline
DNACorpus2/EsCo & 4641652 & 3.817 & 4.000 & 3.791 & 3.791 & 3.847\\
\hline
\cellcolor{gray!6}{DNACorpus2/GaGa} & \cellcolor{gray!6}{148532294} & \cellcolor{gray!6}{3.776} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{3.943} & \cellcolor{gray!6}{3.943} & \cellcolor{gray!6}{3.897}\\
\hline
DNACorpus2/HaHi & 3890005 & 3.916 & 4.000 & 3.884 & 3.884 & 3.944\\
\hline
\cellcolor{gray!6}{DNACorpus2/HePy} & \cellcolor{gray!6}{1667825} & \cellcolor{gray!6}{3.884} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{3.894} & \cellcolor{gray!6}{3.894} & \cellcolor{gray!6}{3.942}\\
\hline
DNACorpus2/HoSa & 189752667 & 3.953 & 4.000 & 4.118 & 4.118 & 4.075\\
\hline
\cellcolor{gray!6}{DNACorpus2/OrSa} & \cellcolor{gray!6}{43262523} & \cellcolor{gray!6}{3.843} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{3.858} & \cellcolor{gray!6}{3.858} & \cellcolor{gray!6}{3.908}\\
\hline
DNACorpus2/PlFa & 8986712 & 4.031 & 4.000 & 4.000 & 4.000 & 4.010\\
\hline
\cellcolor{gray!6}{DNACorpus2/ScPo} & \cellcolor{gray!6}{10652155} & \cellcolor{gray!6}{3.847} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{3.813} & \cellcolor{gray!6}{3.813} & \cellcolor{gray!6}{3.851}\\
\hline
DNACorpus2/YeMi & 73689 & 3.099 & 3.998 & 3.859 & 3.859 & 3.881\\
\hline
\end{tabular}}
\end{table}
It is also worth noting that until now, we have not had much discussion about decompression time. That is because it is usually not as interesting as compression time; for instance, with LZW implementations, we are just looking up codewords in the dictionary to their corresponding string, tacking another character onto that string, and outputting the result. This tends to be much faster than having to look for the longest run, thus decompression is almost always faster than compression when it comes to LZW. Nevertheless, we can still look at the decompression times for our implementation. As seen in Figure \ref{fig:decompcomparisonofourmethodsfig}, the ordering of average decompression time is mostly unsurprising. The Multiple Standard Dictionaries and the single Standard Dictionaries take the most time, with Three Stream LZW coming in second. The surprising part is that Direct Map LZW is slightly faster than Four to One in decompression time. Four to One is also the only implementation which had a larger average decompression time than average compression time. The reason for this is that the decompression implementation for Four to One is not the fastest possible: right now it just reads the bytes one by one and translates the 2 bit encodings back into full characters. A faster implementation would start with a pre-loaded dictionary of all possible byte values mapped to their corresponding 4 character string. This wasn't implemented due to time constraints, which is why Figure \ref{fig:decompcomparisonofourmethodsfig} doesn't look quite like you might expect.
\begin{figure}

{\centering \includegraphics{thesis_files/figure-latex/decompcomparisonofourmethodsfig-1} 

}

\caption{Average Decompression time of our final implementations.}\label{fig:decompcomparisonofourmethodsfig}
\end{figure}
\hypertarget{compression-algorithms-in-literature}{%
\section{Compression Algorithms in Literature}\label{compression-algorithms-in-literature}}

As discussed in the related work section of chapter 1, there have been several other compression algorithms proposed that are tailored for DNA. Not all of these are publicly available, thus we can only compare to the numbers that the researchers reported.

Note that I wasn't able to get compression times for these algorithms, only the compression ratios (Cao et al., 2007).
\begin{table}[!h]

\caption{\label{tab:researchmethodsresultsfig}Compression ratios of related works on DNACorpus 1, reported as bits per base}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{lrrrrrrrr}
\toprule
\multicolumn{1}{c}{ } & \multicolumn{8}{c}{Bits per Base} \\
\cmidrule(l{3pt}r{3pt}){2-9}
Name & BioCompress2 & GenCompress & XM & Standard\_Dictionary & Direct\_Map\_Dictionary & Mult\_Std\_Dictionaries & Four\_to\_One & Three\_Stream\_LZW\\
\midrule
\cellcolor{gray!6}{DNACorpus1/chmpxx} & \cellcolor{gray!6}{1.6848} & \cellcolor{gray!6}{1.6730} & \cellcolor{gray!6}{1.6577} & \cellcolor{gray!6}{2.043628} & \cellcolor{gray!6}{2.449762} & \cellcolor{gray!6}{2.043628} & \cellcolor{gray!6}{2.000529} & \cellcolor{gray!6}{2.059162}\\
DNACorpus1/chntxx & 1.6172 & 1.6146 & 1.6068 & 2.149329 & 2.497831 & 2.149329 & 2.000411 & 2.057801\\
\cellcolor{gray!6}{DNACorpus1/hehcmv} & \cellcolor{gray!6}{1.8480} & \cellcolor{gray!6}{1.8470} & \cellcolor{gray!6}{1.8426} & \cellcolor{gray!6}{2.161480} & \cellcolor{gray!6}{2.439513} & \cellcolor{gray!6}{2.161480} & \cellcolor{gray!6}{2.000297} & \cellcolor{gray!6}{2.059280}\\
DNACorpus1/humdyst & 1.9262 & 1.9231 & 1.9031 & 2.168274 & 2.853547 & 2.168274 & 2.001754 & 2.023420\\
\cellcolor{gray!6}{DNACorpus1/humghcs} & \cellcolor{gray!6}{1.3074} & \cellcolor{gray!6}{1.0969} & \cellcolor{gray!6}{0.9828} & \cellcolor{gray!6}{2.150056} & \cellcolor{gray!6}{2.694819} & \cellcolor{gray!6}{2.150056} & \cellcolor{gray!6}{2.000993} & \cellcolor{gray!6}{2.034318}\\
\addlinespace
DNACorpus1/humhbb & 1.8800 & 1.8204 & 1.7513 & 2.155181 & 2.674960 & 2.155181 & 2.000873 & 2.032848\\
\cellcolor{gray!6}{DNACorpus1/humhdab} & \cellcolor{gray!6}{1.8770} & \cellcolor{gray!6}{1.8192} & \cellcolor{gray!6}{1.6671} & \cellcolor{gray!6}{2.146779} & \cellcolor{gray!6}{2.726828} & \cellcolor{gray!6}{2.146779} & \cellcolor{gray!6}{2.001087} & \cellcolor{gray!6}{2.031394}\\
DNACorpus1/humprtb & 1.9066 & 1.8466 & 1.7361 & 2.145619 & 2.737261 & 2.145619 & 2.001234 & 2.037048\\
\cellcolor{gray!6}{DNACorpus1/mpomtcg} & \cellcolor{gray!6}{1.9378} & \cellcolor{gray!6}{1.9058} & \cellcolor{gray!6}{1.8768} & \cellcolor{gray!6}{2.172671} & \cellcolor{gray!6}{2.485925} & \cellcolor{gray!6}{2.172671} & \cellcolor{gray!6}{2.000375} & \cellcolor{gray!6}{2.053449}\\
DNACorpus1/mtpacga & 1.8752 & 1.8624 & 1.8447 & 2.081225 & 2.524314 & 2.081225 & 2.000678 & 2.064557\\
\addlinespace
\cellcolor{gray!6}{DNACorpus1/vaccg} & \cellcolor{gray!6}{1.7614} & \cellcolor{gray!6}{1.7614} & \cellcolor{gray!6}{1.7649} & \cellcolor{gray!6}{2.108805} & \cellcolor{gray!6}{2.413347} & \cellcolor{gray!6}{2.108805} & \cellcolor{gray!6}{2.000365} & \cellcolor{gray!6}{2.065162}\\
\bottomrule
\end{tabular}}
\end{table}
As you can see in Table \ref{tab:researchmethodsresultsfig}, the compression ratio for these other algorithms are much better. This makes sense, because these algorithms were specifically created with DNA in mind, while we started from an algorithm, LZW, which was created for the redundancies present in human text. Our algorithms also tend to do better on longer texts, which puts them at a disadvantage because Corpus 1 has mostly smaller files. In fact, the average size of the files in Corpus 1 is surprisingly small for the number of papers that have used them. If you have files that are only about 100 thousand bytes, you don't really need to compress them at all. For smaller files, the Four to One version or even Direct Map LZW version may be preferable because of how fast they are. These other papers do not provide performance metrics like compression time, but we believe that our implementations would be much faster on longer files.

\hypertarget{comparison-to-other-professional-tools}{%
\section{Comparison to Other Professional Tools}\label{comparison-to-other-professional-tools}}

The other tools that we choose to use are \texttt{gzip}, \texttt{bzip}, \texttt{xz}, and \texttt{genozip}. All three of \texttt{xz}, \texttt{bzip}, and \texttt{gzip} are open source, general compression tools. As mentioned earlier \texttt{genozip} was created specifically for DNA compression and the compression of other common filetypes in biological research (Lan et al., 2021).
\begin{table}[!h]

\caption{\label{tab:othertoolstab}Performance metrics for other professional tools. Xz, bzip, and gzip were ran with option -9, and genozip was ran with --input=generic.}
\centering
\resizebox{\linewidth}{!}{
\begin{tabular}[t]{l|r|r|r|r|r|r|r|r|r|r}
\hline
\multicolumn{2}{c|}{ } & \multicolumn{9}{c}{Compression Ratio } \\
\cline{3-11}
File Name & Original File Size & bzip & Direct Map Dictionary & Four to One & genozip & gzip & Mult Std Dictionaries & Std Dictionary & Three Stream LZW & xz\\
\hline
\cellcolor{gray!6}{DNACorpus1/chmpxx} & \cellcolor{gray!6}{121024} & \cellcolor{gray!6}{3.77} & \cellcolor{gray!6}{3.266} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.95} & \cellcolor{gray!6}{3.60} & \cellcolor{gray!6}{3.915} & \cellcolor{gray!6}{3.915} & \cellcolor{gray!6}{3.885} & \cellcolor{gray!6}{3.84}\\
\hline
DNACorpus1/chntxx & 155844 & 3.66 & 3.203 & 3.999 & 3.85 & 3.49 & 3.722 & 3.722 & 3.888 & 3.70\\
\hline
\cellcolor{gray!6}{DNACorpus1/hehcmv} & \cellcolor{gray!6}{229354} & \cellcolor{gray!6}{3.68} & \cellcolor{gray!6}{3.279} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.86} & \cellcolor{gray!6}{3.50} & \cellcolor{gray!6}{3.701} & \cellcolor{gray!6}{3.701} & \cellcolor{gray!6}{3.885} & \cellcolor{gray!6}{3.70}\\
\hline
DNACorpus1/humdyst & 38770 & 3.66 & 2.804 & 3.997 & 3.24 & 3.36 & 3.690 & 3.690 & 3.954 & 3.60\\
\hline
\cellcolor{gray!6}{DNACorpus1/humghcs} & \cellcolor{gray!6}{66495} & \cellcolor{gray!6}{4.62} & \cellcolor{gray!6}{2.969} & \cellcolor{gray!6}{3.998} & \cellcolor{gray!6}{5.56} & \cellcolor{gray!6}{5.15} & \cellcolor{gray!6}{3.721} & \cellcolor{gray!6}{3.721} & \cellcolor{gray!6}{3.933} & \cellcolor{gray!6}{7.04}\\
\hline
DNACorpus1/humhbb & 73308 & 3.72 & 2.991 & 3.998 & 3.61 & 3.58 & 3.712 & 3.712 & 3.935 & 3.90\\
\hline
\cellcolor{gray!6}{DNACorpus1/humhdab} & \cellcolor{gray!6}{58864} & \cellcolor{gray!6}{3.86} & \cellcolor{gray!6}{2.934} & \cellcolor{gray!6}{3.998} & \cellcolor{gray!6}{3.56} & \cellcolor{gray!6}{3.61} & \cellcolor{gray!6}{3.727} & \cellcolor{gray!6}{3.727} & \cellcolor{gray!6}{3.938} & \cellcolor{gray!6}{3.97}\\
\hline
DNACorpus1/humprtb & 56737 & 3.81 & 2.923 & 3.998 & 3.52 & 3.58 & 3.729 & 3.729 & 3.927 & 3.87\\
\hline
\cellcolor{gray!6}{DNACorpus1/mpomtcg} & \cellcolor{gray!6}{186609} & \cellcolor{gray!6}{3.68} & \cellcolor{gray!6}{3.218} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.84} & \cellcolor{gray!6}{3.50} & \cellcolor{gray!6}{3.682} & \cellcolor{gray!6}{3.682} & \cellcolor{gray!6}{3.896} & \cellcolor{gray!6}{3.76}\\
\hline
DNACorpus1/mtpacga & 100314 & 3.76 & 3.169 & 3.999 & 3.84 & 3.58 & 3.844 & 3.844 & 3.875 & 3.82\\
\hline
\cellcolor{gray!6}{DNACorpus1/vaccg} & \cellcolor{gray!6}{191737} & \cellcolor{gray!6}{3.81} & \cellcolor{gray!6}{3.315} & \cellcolor{gray!6}{3.999} & \cellcolor{gray!6}{3.94} & \cellcolor{gray!6}{3.65} & \cellcolor{gray!6}{3.794} & \cellcolor{gray!6}{3.794} & \cellcolor{gray!6}{3.874} & \cellcolor{gray!6}{3.88}\\
\hline
DNACorpus2/AeCa & 1591049 & 3.71 & 3.778 & 4.000 & 4.00 & 3.57 & 3.786 & 3.786 & 3.874 & 3.84\\
\hline
\cellcolor{gray!6}{DNACorpus2/AgPh} & \cellcolor{gray!6}{43970} & \cellcolor{gray!6}{3.64} & \cellcolor{gray!6}{2.807} & \cellcolor{gray!6}{3.997} & \cellcolor{gray!6}{3.28} & \cellcolor{gray!6}{3.36} & \cellcolor{gray!6}{3.653} & \cellcolor{gray!6}{3.653} & \cellcolor{gray!6}{3.957} & \cellcolor{gray!6}{3.59}\\
\hline
DNACorpus2/BuEb & 18940 & 3.62 & 2.552 & 3.993 & 2.59 & 3.25 & 3.595 & 3.595 & 3.964 & 3.45\\
\hline
\cellcolor{gray!6}{DNACorpus2/DaRe} & \cellcolor{gray!6}{62565020} & \cellcolor{gray!6}{3.88} & \cellcolor{gray!6}{3.928} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.39} & \cellcolor{gray!6}{3.76} & \cellcolor{gray!6}{4.013} & \cellcolor{gray!6}{4.013} & \cellcolor{gray!6}{4.035} & \cellcolor{gray!6}{4.99}\\
\hline
DNACorpus2/DrMe & 32181429 & 3.70 & 3.847 & 4.000 & 4.03 & 3.60 & 3.836 & 3.836 & 3.865 & 4.01\\
\hline
\cellcolor{gray!6}{DNACorpus2/EnIn} & \cellcolor{gray!6}{26403087} & \cellcolor{gray!6}{3.72} & \cellcolor{gray!6}{3.918} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.14} & \cellcolor{gray!6}{3.61} & \cellcolor{gray!6}{3.893} & \cellcolor{gray!6}{3.893} & \cellcolor{gray!6}{3.928} & \cellcolor{gray!6}{4.56}\\
\hline
DNACorpus2/EsCo & 4641652 & 3.70 & 3.817 & 4.000 & 4.02 & 3.57 & 3.791 & 3.791 & 3.847 & 3.91\\
\hline
\cellcolor{gray!6}{DNACorpus2/GaGa} & \cellcolor{gray!6}{148532294} & \cellcolor{gray!6}{3.74} & \cellcolor{gray!6}{3.776} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.15} & \cellcolor{gray!6}{3.65} & \cellcolor{gray!6}{3.943} & \cellcolor{gray!6}{3.943} & \cellcolor{gray!6}{3.897} & \cellcolor{gray!6}{4.10}\\
\hline
DNACorpus2/HaHi & 3890005 & 3.77 & 3.916 & 4.000 & 4.14 & 3.63 & 3.884 & 3.884 & 3.944 & 3.94\\
\hline
\cellcolor{gray!6}{DNACorpus2/HePy} & \cellcolor{gray!6}{1667825} & \cellcolor{gray!6}{3.77} & \cellcolor{gray!6}{3.884} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.14} & \cellcolor{gray!6}{3.67} & \cellcolor{gray!6}{3.894} & \cellcolor{gray!6}{3.894} & \cellcolor{gray!6}{3.942} & \cellcolor{gray!6}{4.01}\\
\hline
DNACorpus2/HoSa & 189752667 & 3.89 & 3.953 & 4.000 & 4.22 & 3.73 & 4.118 & 4.118 & 4.075 & 4.48\\
\hline
\cellcolor{gray!6}{DNACorpus2/OrSa} & \cellcolor{gray!6}{43262523} & \cellcolor{gray!6}{3.73} & \cellcolor{gray!6}{3.843} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.21} & \cellcolor{gray!6}{3.65} & \cellcolor{gray!6}{3.858} & \cellcolor{gray!6}{3.858} & \cellcolor{gray!6}{3.908} & \cellcolor{gray!6}{4.62}\\
\hline
DNACorpus2/PlFa & 8986712 & 3.82 & 4.031 & 4.000 & 4.28 & 3.77 & 4.000 & 4.000 & 4.010 & 4.28\\
\hline
\cellcolor{gray!6}{DNACorpus2/ScPo} & \cellcolor{gray!6}{10652155} & \cellcolor{gray!6}{3.68} & \cellcolor{gray!6}{3.847} & \cellcolor{gray!6}{4.000} & \cellcolor{gray!6}{4.07} & \cellcolor{gray!6}{3.57} & \cellcolor{gray!6}{3.813} & \cellcolor{gray!6}{3.813} & \cellcolor{gray!6}{3.851} & \cellcolor{gray!6}{3.91}\\
\hline
DNACorpus2/YeMi & 73689 & 3.78 & 3.099 & 3.998 & 3.72 & 3.58 & 3.859 & 3.859 & 3.881 & 3.81\\
\hline
\end{tabular}}
\end{table}
Table \ref{tab:othertoolstab} summarizes the compression ratios of both the other tools and our 5 implementations. Not surprisingly, all of our implementations do better than general compressors \texttt{bzip} and \texttt{gzip} in terms of compression ratio on the larger files. Both of these tools were created to work on a broad array of files, and thus they do not take into account the specific characteristics of genetic sequences that we saw such as there only being four characters. On the other hand, \texttt{genozip} does outperform our implementation in terms of compression ratio on most files, which also makes sense as it was also created with DNA in mind. It is also worth noting that \texttt{xz} does surprisingly well, beating even \texttt{genozip} on some of the larger files. However, if you look at the performance data in Figure \ref{fig:othertoolsgraphfig}, you can see that both our Three Stream LZW and Direct Map LZW are significantly faster than all these other tools, including \texttt{genozip} and \texttt{xz}. In fact, \texttt{xz} is over 10 times slower than Three Stream LZW in terms of average compression time. So for large files, you if you want a balance between compression ratio and compression time, Three Streams may be a viable option. Of course, if you are looking for speed, you should use the Four to One implementation, which still blows all others out of the water.
\begin{figure}
\centering
\includegraphics{thesis_files/figure-latex/othertoolsgraphfig-1.pdf}
\caption{\label{fig:othertoolsgraphfig}Time comparison of professional tools and our final implementations.}
\end{figure}
\hypertarget{conclusion}{%
\chapter*{Conclusion}\label{conclusion}}
\addcontentsline{toc}{chapter}{Conclusion}

We started with a naive implementation of Lempel Ziv Welch, and our goal was to optimize it for DNA. Although we were not able to achieve as high of a compression ratio as other algorithms in literature and professional DNA compresion tools, we were able to make a very fast algorithm that can do better than a 4.0 compression ratio on large files. There is a trade off when it comes to choosing a compression algorithm, with speed and space. If you needed to compress DNA files as small as possible, you should use the professional tools or one of the algorithms proposed in other research papers. If you want to compress DNA very quickly and you are okay with a moderate compression ratio, than you should use the Four to One implementation which we discussed previously. If you want something that is in between, with a good compression ratio and fast compression time, the results in Chapter 3 tell us that the Three Stream implementation of LZW would work very well for large files.

The concept of an indexed, Direct Map dictionary is one that could prove useful for other applications. Further research could be used to apply a similar idea to a different algorithm, one more suited to the redundancies present in DNA. In other words, could we take one of the research papers discussed in Section \ref{related-work} and apply the idea of the Direct Map to the algorithm presented in it? This could also be useful in other areas of compression in which the workload has a limited number of characters like with DNA. The usage of \texttt{pext} is also worth noting. Because of the 2 bit encoding possible for DNA sequences, we can create an index for a string of DNA very quickly. This, combined with the Direct Map Dictionary, allows for the Direct Map Dictionary to have dictionary accesses that are almost as fast as array accesses.

The Four to One implementation, another use of \texttt{pext}, is also notable. The Four to One implementation is extremely fast, and could also be useful for other compression applications where there are fewer characters than the encoding scheme accounts for (bytes can be used to store 256 different values, but only 4 are used for DNA).

Overall, this thesis has produced a few notable implementations of LZW, created a Direct Map Dictionary data structure which has high potential for other compression applications, and shown the relevance of \texttt{pext} in optimizing compression algorithms.

\appendix

\hypertarget{appendix-the-code}{%
\chapter{Appendix: The Code}\label{appendix-the-code}}

All code for this thesis, including the Rmarkdown used to generate this document, can be found at \url{https://github.com/cadencorontzos/lzwfordna}.

\backmatter

\hypertarget{references}{%
\chapter*{References}\label{references}}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\noindent

\setlength{\parindent}{-0.20in}

\hypertarget{refs}{}
\begin{CSLReferences}{1}{0}
\leavevmode\vadjust pre{\hypertarget{ref-CaoXM}{}}%
Cao, D., Dix, T., Allison, L., \& Mears, C. (2007). A simple statistical algorithm for biological sequence compression. In \emph{In Proceedings of the Conference on Data Compression} (pp. 43--52). http://doi.org/\href{https://doi.org/10.1109/DCC.2007.7}{10.1109/DCC.2007.7}

\leavevmode\vadjust pre{\hypertarget{ref-Chen2001ACA}{}}%
Chen, X., Kwong, S. T. W., \& Li, M. (2001). A compression algorithm for DNA sequences. \emph{IEEE Engineering in Medicine and Biology Magazine}, \emph{20}, 61--66.

\leavevmode\vadjust pre{\hypertarget{ref-grumbach}{}}%
Grumbach, S., \& Tahi, F. (1994). {A New Challenge for Compression Algorithms: Genetic Sequences}. \emph{{Information Processing and Management}}, \emph{30}. Retrieved from \url{https://hal.inria.fr/inria-00180949}

\leavevmode\vadjust pre{\hypertarget{ref-huffman1952method}{}}%
Huffman, D. A. (1952). A method for the construction of minimum-redundancy codes. \emph{Proceedings of the IRE}, \emph{40}(9), 1098--1101.

\leavevmode\vadjust pre{\hypertarget{ref-ibrahimgbolagade}{}}%
Ibrahim, M., \& Gbolagade, K. (2020). Enhancing computational time of lempel-ziv-welch-based text compression with chinese remainder theorem. \emph{Journal of Computer Science and Its Application}, \emph{27}. http://doi.org/\href{https://doi.org/10.4314/jcsia.v27i1.9}{10.4314/jcsia.v27i1.9}

\leavevmode\vadjust pre{\hypertarget{ref-KeerthyMID}{}}%
Keerthy, P. (2019). Genomic sequence data compression using lempel-ziv-welch algorithm with indexed multiple dictionary. \emph{International Journal of Engineering and Advanced Technology}.

\leavevmode\vadjust pre{\hypertarget{ref-genozip}{}}%
Lan, D., Tobler, R., Souilmi, Y., \& Llamas, B. (2021). {Genozip: a universal extensible genomic data compressor}. \emph{Bioinformatics}, \emph{37}(16), 2225--2230. http://doi.org/\href{https://doi.org/10.1093/bioinformatics/btab102}{10.1093/bioinformatics/btab102}

\leavevmode\vadjust pre{\hypertarget{ref-panialok}{}}%
Pani, A., Mishra, M., \& Mishra, T. (2012). Parallel lempel-ziv-welch (PLZW) technique for data compression. \emph{International Journal of Computer Science and Information Technology}, \emph{3}, 4038--4040.

\leavevmode\vadjust pre{\hypertarget{ref-prataspinho}{}}%
Pratas, D., \& Pinho, A. (2018). A DNA sequence corpus for compression benchmark. In (pp. 208--215). http://doi.org/\href{https://doi.org/10.1007/978-3-319-98702-6_25}{10.1007/978-3-319-98702-6\_25}

\leavevmode\vadjust pre{\hypertarget{ref-arithmeticencodingsite}{}}%
Singhaniya, R. (2021, October). \emph{Bench Partner}. Retrieved from \url{https://benchpartner.com/q/explain-arithmetic-encoding-process-with-an-example}

\leavevmode\vadjust pre{\hypertarget{ref-welch1984technique}{}}%
Welch, T. A. (1984). A technique for high-performance data compression. \emph{Computer}, \emph{17}(6), 8--19. http://doi.org/\href{https://doi.org/10.1109/MC.1984.1659158}{10.1109/MC.1984.1659158}

\leavevmode\vadjust pre{\hypertarget{ref-ziv1977universal}{}}%
Ziv, J., \& Lempel, A. (1977). A universal algorithm for sequential data compression. In \emph{IEEE transactions on information theory} (Vol. 23, pp. 337--343). http://doi.org/\href{https://doi.org/10.1109/TIT.1977.1055714}{10.1109/TIT.1977.1055714}

\end{CSLReferences}

% Index?

\end{document}
