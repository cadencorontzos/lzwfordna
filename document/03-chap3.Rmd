# Comparison to other tools

Now that we have implemented several versions of LZW, we will compare them to each other to see which would be preferable in different situations. We also want to compare the performance of these different implementations to the Four to One implementation and other professional compression tools.

## Comparison of our Implementations

We have five different implementations at this point: LZW with Direct Map Dictionary, LZW with the Standard Dictionary, LZW with Multiple Standard Dictionaries, Three Stream LZW, and the Four To One translation. Figure \@ref(fig:comparisonofourmethodsfig) shows the performance of these methods on the corpora. 

We can also see the raw values in Table \@ref(tab:comparisonofourmethodstab). While the Four to One implementation completely dominates in terms of throughput, the Direct Map version is substantially faster than the Standard and Multiple Dictionary versions. The Three Stream LZW comes in between the Direct Map and Four to One implementations in terms of compression time, but it actually has the best compression ratio out of all LZW versions on most of the input files. This means that by switching from the Standard Dictionary to the Direct Map and adding on entropy encoding, we were able to regain the loss in compression ratio without totally compromising our compression time improvements. 

```{r comparisonofourmethodsfig, warnings= F, messages = F, fig.cap = "Performance comparison of our final implementations.", fig.height = 4, echo = F}
compare_implementations(list(
  c('data/stats/final/corpus_1_stddict_final_ccf351c.csv', 'data/stats/final/corpus_2_stddict_final_ccf351c.csv', 'Std Dictionary'),
  c('data/stats/final/corpus_1_ddict_final_ccf351c.csv', 'data/stats/final/corpus_2_ddict_final_ccf351c.csv', 'Direct Map Dictionary'),
  c('data/stats/final/corpus_1_multstddict_final_ccf351c.csv', 'data/stats/final/corpus_2_multstddict_final_ccf351c.csv', 'Mult Std Dictionaries'),
  c('data/stats/final/corpus_1_4t1_final_ccf351c.csv', 'data/stats/final/corpus_2_4t1_final_ccf351c.csv', 'Four to One'),
  c('data/stats/final/corpus_1_threestream_final_ccf351c.csv', 'data/stats/final/corpus_2_threestream_final_ccf351c.csv', 'Three Stream LZW')
), FALSE, "Final Implementation")
```

```{r comparisonofourmethodstab, warnings= F, messages = F, fig.height = 7, echo = F}

process_corpus_group <- function(array){
	wholedf = data.frame()
	for(group_list in array){
	df <- data.frame()
	cp1 <- read.csv(group_list[1])
	cp2 <- read.csv(group_list[2])
	df <- bind_rows(cp1, cp2)
	df['Implementation'] <- group_list[3]
	wholedf<- bind_rows(df, wholedf)
	}
	wholedf
}

wholedf <- process_corpus_group(list(
  c('data/stats/final/corpus_1_stddict_final_ccf351c.csv', 'data/stats/final/corpus_2_stddict_final_ccf351c.csv', 'Std Dictionary'),
  c('data/stats/final/corpus_1_ddict_final_ccf351c.csv', 'data/stats/final/corpus_2_ddict_final_ccf351c.csv', 'Direct Map Dictionary'),
  c('data/stats/final/corpus_1_multstddict_final_ccf351c.csv', 'data/stats/final/corpus_2_multstddict_final_ccf351c.csv', 'Mult Std Dictionaries'),
  c('data/stats/final/corpus_1_4t1_final_ccf351c.csv', 'data/stats/final/corpus_2_4t1_final_ccf351c.csv', 'Four to One'),
  c('data/stats/final/corpus_1_threestream_final_ccf351c.csv', 'data/stats/final/corpus_2_threestream_final_ccf351c.csv', 'Three Stream LZW')
))

df_new <- wholedf %>% 
  select(File.Name, Implementation, Compression.Ratio, Original.File.Size) %>% 
  spread(key = Implementation, value = Compression.Ratio) 
names(df_new) <-   gsub("\\.", " ", names(df_new))


kbl(df_new, caption = "Compression Ratios of Final Implementations.") %>%
  add_header_above(c(" " = 2, "Compression Ratio " = 5 )) %>% 
    kable_styling(latex_options=c("scale_down", "hold_position", "striped"))
```

It is also worth noting that until now, we have not had much discussion about decompression time. That is because it is usually not as interesting as compression time; for instance, with LZW implementations, we are just looking up codewords in the dictionary to their corresponding string, tacking another character onto that string, and outputting the result. This tends to be much faster than having to look for the longest run, thus decompression is almost always faster than compression when it comes to LZW. Nevertheless, we can still look at the decompression times for our implementation. As seen in Figure \@ref(fig:decompcomparisonofourmethodsfig), the ordering of average decompression time is mostly unsurprising. The Multiple Standard Dictionaries and the single Standard Dictionaries take the most time, with Three Stream LZW coming in second. The surprising part is that Direct Map LZW is slightly faster than Four to One in decompression time. Four to One is also the only implementation which had a larger average decompression time than average compression time. The reason for this is that the decompression implementation for Four to One is not the fastest possible: right now it just reads the bytes one by one and translates the 2 bit encodings back into full characters. A faster implementation would start with a pre-loaded dictionary of all possible byte values mapped to their corresponding 4 character string. This wasn't implemented due to time constraints, which is why Figure \@ref(fig:decompcomparisonofourmethodsfig) doesn't look quite like you might expect. 

```{r decompcomparisonofourmethodsfig, warnings= F, messages = F, fig.cap = "Average Decompression time of our final implementations.", fig.height = 4, fig.width = 4, fig.align="center", echo = F}
# Would be nice to get compare_implementations to do this automatically
# but I'm too lazy
num_files <-length(unique(wholedf$File.Name))

# Calculate avg decomp time
dcomp_summary <- wholedf %>% 
  group_by(Implementation) %>% 
  summarize(
    decomp_time = sum(Decompression.Time)*.000000001/num_files
  )

# output graph
dcomp_summary %>% 
  ggplot(aes(x = Implementation, y = decomp_time, fill = Implementation))+
  geom_col(show.legend = F)+
  labs(x="Implementation", y = "Average Decompression Time (s)", title = "Average Decompression Time")+
   theme(axis.text.x = element_text(angle = 45, hjust = 1))
  
```


## Compression Algorithms in Literature

As discussed in the related work section of chapter 1, there have been several other compression algorithms proposed that are tailored for DNA. Not all of these are publicly available, thus we can only compare to the numbers that the researchers reported.

Note that I wasn't able to get compression times for these algorithms, only the compression ratios [@CaoXM].

```{r researchmethodsresultsfig, warnings= F, messages = F, fig.cap = "Compression ratios of related works on DNACorpus1, reported as bits per base.", fig.height = 4, echo = F}

Name <- c("DNACorpus1/chmpxx", "DNACorpus1/chntxx", "DNACorpus1/hehcmv", "DNACorpus1/humdyst", "DNACorpus1/humghcs", "DNACorpus1/humhbb", "DNACorpus1/humhdab", "DNACorpus1/humprtb", "DNACorpus1/mpomtcg", "DNACorpus1/mtpacga", "DNACorpus1/vaccg")


process_run_corpus1 <- function(filename, ms=FALSE){
  
  df <- read.csv(filename)
  vec <- numeric()
  for(name in Name){
    df_filtered <- df[df$File.Name == name, ]
    ogfs <- (first(df_filtered[1,"Compressed.Size"]) * 8)

    if(ms){
      
    }
    bitscomp <-first(df_filtered[1, "Original.File.Size"])

    bits_per_base <- ogfs / bitscomp
    vec <- append(vec, bits_per_base)

  }
  vec
  
}

Standard_Dictionary<- process_run_corpus1("data/stats/final/corpus_1_stddict_final_ccf351c.csv")
 Direct_Map_Dictionary <- process_run_corpus1('data/stats/final/corpus_1_ddict_final_ccf351c.csv')
 Mult_Std_Dictionaries <- process_run_corpus1('data/stats/final/corpus_1_multstddict_final_ccf351c.csv')
 Four_to_One <- process_run_corpus1('data/stats/final/corpus_1_4t1_final_ccf351c.csv')
 Three_Stream_LZW <- process_run_corpus1('data/stats/final/corpus_1_threestream_final_ccf351c.csv')

df <- data.frame(
  Name,
  BioCompress2 = c(1.6848, 1.6172, 1.8480, 1.9262, 1.3074, 1.8800, 1.8770, 1.9066, 1.9378, 1.8752, 1.7614),
  GenCompress = c(1.6730, 1.6146, 1.8470, 1.9231, 1.0969, 1.8204, 1.8192, 1.8466, 1.9058, 1.8624, 1.7614),
  XM = c(1.6577, 1.6068, 1.8426, 1.9031, 0.9828, 1.7513, 1.6671, 1.7361, 1.8768, 1.8447, 1.7649),
  Standard_Dictionary,
  Direct_Map_Dictionary,
  Mult_Std_Dictionaries,
  Four_to_One,
  Three_Stream_LZW

)

names(df_new) <-   gsub("\\_", " ", names(df_new))
# print table

  kbl(df, caption = "Compression ratios of related works on DNACorpus 1, reported as bits per base", booktabs=TRUE) %>%   add_header_above(c(" " = 1, "Bits per Base" = 8)) %>% 
  kable_styling(latex_options=c("scale_down", "hold_position", "striped"))
```

As you can see in Table \@ref(tab:researchmethodsresultsfig), the compression ratio for these other algorithms are much better. This makes sense, because these algorithms were specifically created with DNA in mind, while we started from an algorithm, LZW, which was created for the redundancies present in human text. Our algorithms also tend to do better on longer texts, which puts them at a disadvantage because Corpus 1 has mostly smaller files. In fact, the average size of the files in Corpus 1 is surprisingly small for the number of papers that have used them. If you have files that are only about 100 thousand bytes, you don't really need to compress them at all. For smaller files, the Four to One version or even Direct Map LZW version may be preferable because of how fast they are. These other papers do not provide performance metrics like compression time, but we believe that our implementations would be much faster on longer files.

## Comparison to Other Professional Tools

The other tools that we choose to use are `gzip`, `bzip`, `xz`, and `genozip`. All three of `xz`, `bzip`, and `gzip` are open source, general compression tools. As mentioned earlier `genozip` was created specifically for DNA compression and the compression of other common filetypes in biological research [@genozip]. 

```{r othertoolstab, echo=FALSE}
gzip1 <- read.csv("data/stats/other_tools/gzip_corpus_1_stats.csv")
gzip2 <- read.csv("data/stats/other_tools/gzip_corpus_2_stats.csv")
gzip <- rbind(gzip1, gzip2)
gzip <- gzip %>%  mutate(
  Program = "gzip"
)

bzip1 <- read.csv("data/stats/other_tools/bzip_corpus_1_stats.csv")
bzip2 <- read.csv("data/stats/other_tools/bzip_corpus_2_stats.csv")
bzip <- rbind(bzip1, bzip2)
bzip <- bzip %>%  mutate(
  Program = "bzip"
)

genozip1 <- read.csv("data/stats/other_tools/genozip_corpus_1_stats.csv")
genozip2 <- read.csv("data/stats/other_tools/genozip_corpus_2_stats.csv")
genozip <- rbind(genozip1, genozip2)
genozip <- genozip %>% mutate(
  Program = "genozip"
)

xz1 <- read.csv("data/stats/other_tools/xz_corpus_1_stats.csv")
xz2 <- read.csv("data/stats/other_tools/xz_corpus_2_stats.csv")
xz <- rbind(xz2, xz1)
xz <- xz %>%  mutate(
  Program = "xz"
)
wholedf1 <- rbind(genozip, bzip, gzip, xz)

wholedf <- process_corpus_group(list(
  c('data/stats/final/corpus_1_stddict_final_ccf351c.csv', 'data/stats/final/corpus_2_stddict_final_ccf351c.csv', 'Std Dictionary'),
  c('data/stats/final/corpus_1_ddict_final_ccf351c.csv', 'data/stats/final/corpus_2_ddict_final_ccf351c.csv', 'Direct Map Dictionary'),
  c('data/stats/final/corpus_1_multstddict_final_ccf351c.csv', 'data/stats/final/corpus_2_multstddict_final_ccf351c.csv', 'Mult Std Dictionaries'),
  c('data/stats/final/corpus_1_4t1_final_ccf351c.csv', 'data/stats/final/corpus_2_4t1_final_ccf351c.csv', 'Four to One'),
  c('data/stats/final/corpus_1_threestream_final_ccf351c.csv', 'data/stats/final/corpus_2_threestream_final_ccf351c.csv', 'Three Stream LZW')
))

wholedf <- wholedf %>% rename(
  Program = Implementation
) %>% mutate(
  Compression.Time = Compression.Time * .000000001
) %>% 
  select(
    Compression.Time,
    Compression.Ratio,
    Original.File.Size,
    File.Name,
    Compressed.Size,
    Program,
    
  )

wholedf <- rbind(wholedf, wholedf1)


df_new <- wholedf %>% 
  select(File.Name, Program, Compression.Ratio, Original.File.Size) %>% 
  spread(key = Program, value = Compression.Ratio) 
names(df_new) <-   gsub("\\.", " ", names(df_new))


kbl(df_new, caption = "Performance metrics for other professional tools. Xz, bzip, and gzip were ran with option -9, and genozip was ran with --input=generic.") %>%
  add_header_above(c(" " = 2, "Compression Ratio " = 9 )) %>% 
    kable_styling(latex_options=c("scale_down", "hold_position", "striped"))

```


Table  \@ref(tab:othertoolstab) summarizes the compression ratios of both the other tools and our 5 implementations. Not surprisingly, all of our implementations do better than general compressors `bzip` and `gzip` in terms of compression ratio on the larger files. Both of these tools were created to work on a broad array of files, and thus they do not take into account the specific characteristics of genetic sequences that we saw such as there only being four characters. On the other hand, `genozip` does outperform our implementation in terms of compression ratio on most files, which also makes sense as it was also created with DNA in mind. It is also worth noting that `xz` does surprisingly well, beating even `genozip` on some of the larger files. However, if you look at the performance data in Figure \@ref(fig:othertoolsgraphfig), you can see that both our Three Stream LZW and Direct Map LZW are significantly faster than all these other tools, including `genozip` and `xz`. In fact, `xz` is over 10 times slower than Three Stream LZW in terms of average compression time. So for large files, you if you want a balance between compression ratio and compression time, Three Streams may be a viable option. Of course, if you are looking for speed, you should use the Four to One implementation, which still blows all others out of the water.

```{r othertoolsgraphfig, warnings= F, messages = F, fig.cap = "Time comparison of professional tools and our final implementations.", fig.height = 4, echo = F}

summary <- wholedf %>% 
  group_by(Program) %>% 
  reframe(
    comp_throughput = sum(Original.File.Size)/(1024^2)/sum(Compression.Time),
    mean_comp_time = sum(Compression.Time)/ length(unique(File.Name)),
  )

  plot1<- summary %>% 
    ggplot(aes(x = Program, y = comp_throughput, fill = Program))+
    geom_col(show.legend = F)+
    labs(x="Program", y = "Throughput (MBps)", title = "Throughput")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  plot2 <- summary %>% 
    ggplot(aes(x = Program, y = mean_comp_time, fill = Program))+
    geom_col(show.legend = F)+
    labs(x="Program", y = "Mean Compression Time (s)", title = "Average Compression Time")+
    theme(axis.text.x = element_text(angle = 45, hjust = 1))
  plot1+plot2
```


